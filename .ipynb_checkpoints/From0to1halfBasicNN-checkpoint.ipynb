{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#从-0-到-1/2-之-NN\" data-toc-modified-id=\"从-0-到-1/2-之-NN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>从 0 到 1/2 之 NN</a></div><div class=\"lev2 toc-item\"><a href=\"#原理简介\" data-toc-modified-id=\"原理简介-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>原理简介</a></div><div class=\"lev3 toc-item\"><a href=\"#Neural-Network-Introduction-–-WildML-笔记\" data-toc-modified-id=\"Neural-Network-Introduction-–-WildML-笔记-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span><a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch\" target=\"_blank\">Neural Network Introduction – WildML</a> 笔记</a></div><div class=\"lev2 toc-item\"><a href=\"#代码步骤（单隐层）\" data-toc-modified-id=\"代码步骤（单隐层）-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>代码步骤（单隐层）</a></div><div class=\"lev2 toc-item\"><a href=\"#代码实现\" data-toc-modified-id=\"代码实现-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>代码实现</a></div><div class=\"lev2 toc-item\"><a href=\"#代码来源\" data-toc-modified-id=\"代码来源-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>代码来源</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从 0 到 1/2 之 NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理简介\n",
    "\n",
    "### [Neural Network Introduction – WildML](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch) 笔记\n",
    "\n",
    "- 神经元的数量选择是艺术\n",
    "- 激活函数选择：tanh, sigmoid, ReLUs 等，因为导数可以用函数本身计算……\n",
    "- 前向传播：![](http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++z_1+%26+%3D+xW_1+%2B+b_1+%5C%5C++a_1+%26+%3D+%5Ctanh%28z_1%29+%5C%5C++z_2+%26+%3D+a_1W_2+%2B+b_2+%5C%5C++a_2+%26+%3D+%5Chat%7By%7D+%3D+%5Cmathrm%7Bsoftmax%7D%28z_2%29++%5Cend%7Baligned%7D&bg=ffffff&fg=000&s=0)\n",
    "- 反向传播（Softmax）：![](http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%26+%5Cdelta_3+%3D+%5Chat%7By%7D+-+y+%5C%5C++%26+%5Cdelta_2+%3D+%281+-+%5Ctanh%5E2+z_1%29+%5Ccirc+%5Cdelta_3W_2%5ET+%5C%5C++%26+%5Cfrac%7B%5Cpartial%7BL%7D%7D%7B%5Cpartial%7BW_2%7D%7D+%3D+a_1%5ET+%5Cdelta_3+%5C%5C++%26+%5Cfrac%7B%5Cpartial%7BL%7D%7D%7B%5Cpartial%7Bb_2%7D%7D+%3D+%5Cdelta_3%5C%5C++%26+%5Cfrac%7B%5Cpartial%7BL%7D%7D%7B%5Cpartial%7BW_1%7D%7D+%3D+x%5ET+%5Cdelta2%5C%5C++%26+%5Cfrac%7B%5Cpartial%7BL%7D%7D%7B%5Cpartial%7Bb_1%7D%7D+%3D+%5Cdelta2+%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0)\n",
    "- 训练中的一些技巧\n",
    "    - **[minibatch](http://cs231n.github.io/optimization-1/#gd)**\n",
    "    - **[annealing schedule for learning rate](http://cs231n.github.io/neural-networks-3/#anneal)**\n",
    "    - 激活函数，注意激活函数调整，反向传播的导数也要相应调整\n",
    "    - Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码步骤（单隐层）  \n",
    "\n",
    "- 初始化及定义\n",
    "  - 定义交叉熵函数及其导函数\n",
    "    - $y=\\frac{1} {1+e^{-x}}$ # sigmoid 函数\n",
    "    - $y'(x) = y(1-y)$\n",
    "  - 输入和标签\n",
    "    - 假设 X 是 m 维，每次输入一个训练数据，即 1×m\n",
    "    - 标签 y 是 1×1\n",
    "  - 初始化 W 矩阵\n",
    "    - 假设隐层神经元数量为 n，则输入层 -> 隐层的 W0 矩阵：m×n\n",
    "    - 隐层 -> 输出层的 W1 矩阵：n×1\n",
    "- 前向传播（Epoch 开始）\n",
    "  - 第 0 层（输入层）：L0 = X，   维度：1×m\n",
    "  - 第 1 层（隐藏层）：L1 = X×W0，维度：（1×m）×（m×n）= 1×n\n",
    "  - 第 2 层（输出层）：L2 = L1×W1，维度：（1×n）×（n×1）= 1×1\n",
    "  - 第 2 层（输出层）的 Error：L2\\_error = L2 - y，维度：1×1\n",
    "- 反向传播\n",
    "  - 更新后的第 2 层（输出层）：L2\\_delta = L2\\_error · L2 · (1-L2)，即误差×导数（梯度），维度：1×1\n",
    "  - 第 1 层（隐藏层）的 Error：L1\\_Error = L2\\_delta × W1.T，维度：（1×1）×（1×n）= 1×n\n",
    "  - 更新后的第 1 层（隐藏层）：L1\\_delta = L1\\_error · L1 · (1-L1)，维度：（1×n）×（1×n）×（1×n）= 1×n\n",
    "  - 注意：计算更新后层的时候，乘法均为 Hadamard 乘积，就是元素对应乘积，其实就是沿梯度方向更新\n",
    "  - `.T`代表转置矩阵\n",
    "- 更新 W 矩阵（Epoch 结束）\n",
    "  - 隐藏层 -> 输出层矩阵 W1 = W1 + L1.T × L2\\_delta，维度：（n×1）+（n×1）×（1×1）= n×1\n",
    "  - 输入层 -> 隐藏层矩阵 W0 = W0 + L0.T × L1\\_delta，维度：（m×n）+（m×1）×（1×n）= m×n\n",
    "\n",
    "\n",
    "总结一下，就是想办法不断更新 W 矩阵，就要获取输出和标签之间的 Error，将 Error 沿着梯度方向更新输出，更新后的输出结合输入更新 W（输出本来是输入和 W 得来的）。结果就是 W 也不断沿梯度方向变化从而减小 Error。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN Numpy 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码来源\n",
    "\n",
    "- [A Neural Network in 11 lines of Python (Part 1) - i am trask](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
