{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#从-0-到-1/2-之-RNN\" data-toc-modified-id=\"从-0-到-1/2-之-RNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>从 0 到 1/2 之 RNN</a></div><div class=\"lev2 toc-item\"><a href=\"#原理简介\" data-toc-modified-id=\"原理简介-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>原理简介</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN\" data-toc-modified-id=\"RNN-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>RNN</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev3 toc-item\"><a href=\"#笔记\" data-toc-modified-id=\"笔记-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>笔记</a></div><div class=\"lev4 toc-item\"><a href=\"#RNN-&amp;-LSTM\" data-toc-modified-id=\"RNN-&amp;-LSTM-1131\"><span class=\"toc-item-num\">1.1.3.1&nbsp;&nbsp;</span>RNN &amp; LSTM</a></div><div class=\"lev4 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-1132\"><span class=\"toc-item-num\">1.1.3.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev2 toc-item\"><a href=\"#代码步骤\" data-toc-modified-id=\"代码步骤-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>代码步骤</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN（单-Cell）\" data-toc-modified-id=\"RNN（单-Cell）-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>RNN（单 Cell）</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev2 toc-item\"><a href=\"#代码解析\" data-toc-modified-id=\"代码解析-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>代码解析</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN\" data-toc-modified-id=\"RNN-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>RNN</a></div><div class=\"lev4 toc-item\"><a href=\"#RNN-Numpy\" data-toc-modified-id=\"RNN-Numpy-1311\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>RNN Numpy</a></div><div class=\"lev4 toc-item\"><a href=\"#RNN-Tensorflow\" data-toc-modified-id=\"RNN-Tensorflow-1312\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>RNN Tensorflow</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev4 toc-item\"><a href=\"#LSTM-Numpy\" data-toc-modified-id=\"LSTM-Numpy-1321\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>LSTM Numpy</a></div><div class=\"lev4 toc-item\"><a href=\"#LSTM-Tensorflow\" data-toc-modified-id=\"LSTM-Tensorflow-1322\"><span class=\"toc-item-num\">1.3.2.2&nbsp;&nbsp;</span>LSTM Tensorflow</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从 0 到 1/2 之 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习 RNN 有一段时间了，期间也学习了各式各样的 LSTM，但总是觉得不得要领。所以，决定搞一篇综述，边写边理解。  \n",
    "希望能帮到和我一样一脸懵逼的同学。我相信，如果我懂了，大家都能懂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理简介\n",
    "\n",
    "网上资料太多，而且介绍也非常详细完善，就不重复造轮子了。大致概括一下：  \n",
    "\n",
    "### RNN \n",
    "\n",
    "- 核心在于隐层是自循环的，其实把隐层看成是自己不停迭代自己就行了\n",
    "- 输入输出：NLP 中为一个序列\n",
    "  - 如词级别的，语料为：“我 非常 喜欢 你 ！”，输入为：“我 非常 喜欢 你”，输出为：“非常 喜欢 你 ！” \n",
    "  - 输出不仅依赖输入，还依赖中间的其他词，具体来说 “非常” 依赖于 “我”，“喜欢” 依赖于 “我 非常”，以此类推，每个输出的词都依赖于之前的输入，在 RNN 中通过递归来跟踪\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "- 在 RNN 的基础上，隐层迭代时丢掉一些信息同时新增加一些信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笔记\n",
    "\n",
    "非常棒的资料\n",
    "\n",
    "#### RNN & LSTM\n",
    "\n",
    "- [Recurrent Neural Networks Tutorial – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns)\n",
    "  - 前谷歌大脑工程师：[WildML – AI, Deep Learning, NLP](http://www.wildml.com/)\n",
    "  - 发布时间：2015 年 9 月\n",
    "  - 【系列】博客，各类资源汇总，基本可以从零开始看大牛的教程\n",
    "  - 英文写的很流畅（简单）\n",
    "\n",
    "- [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "  - 训练序列的长度就是 RNN 循环的次数\n",
    "  - RNN 最重要的就是隐层的状态，能够获取序列信息\n",
    "\n",
    "- [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)\n",
    "  - 按字或按词分开\n",
    "  - **去除低频词，设为 UNK**。生成时如果碰到 UNK，就从 词表外随机选一个词或者直到生成没有 UNK 的句子为止\n",
    "  - 句首句尾添加 开始 和 结束 TOKE\n",
    "  - 构建训练集矩阵，注意不同的目的构建方法不同。如果是生成模型，则：输入[0, 179, 341, 416]，输出[179, 341, 416, 1]，0 和 1 分别表示开始和结束的 TOKEN，其他数字表示一个词或字\n",
    "  - **预处理思路可以学习*\n",
    "  - $s_t = tanh(Ux_t + Ws_{t-1})$ \n",
    "  - $o_t = softmax(Vs_t)$\n",
    "  - 维度理解：\n",
    "    - $x_t, o_t$, (vocab_size, 1)\n",
    "    - $U$, (hidden_size, vocab_size)\n",
    "    - $V$, (vocab_size, hidden_size)\n",
    "    - $W$, (hidden_size, hidden_size)\n",
    "    - (hidden_size, vocab_size) \\* (vocab_size, 1) = (hidden_size, 1)\n",
    "    - (vocab_size, hidden_size) \\* (hidden_size, 1) = (vocab_size, 1)\n",
    "  - 参数规模计算\n",
    "    - **$2HC + H^2$**, C: vocab_size, H: hidden_size\n",
    "    - $x_t$ 是 One-hot，所以 $Ux_t$ 相当与选择 U 的一列，最大的运算在 $Vs_t$\n",
    "    - **词表大小影响计算速度和资源消耗，尽可能保持小的词表**\n",
    "  - 初始化参数\n",
    "    - **初始化 W 矩阵：$[\\frac{-1}{\\sqrt{n}}, \\frac {1}{\\sqrt{n}}]$，n 表示与下一层相连的矩阵的大小**\n",
    "    - 如：`np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))`\n",
    "  - 代码思路清晰，同时有不少取巧，如 correct_word_predictions\n",
    "  - 注意 loss 的计算，每个 求和\n",
    "  - SGD\n",
    "  - Gradient Checking\n",
    "\n",
    "- [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients – WildML](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)\n",
    "\n",
    "\n",
    "\n",
    "- [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "  - 作者（大牛）：[karpathy’s gists · GitHub](https://gist.github.com/karpathy)\n",
    "  - 发布时间：2015 年 5 月\n",
    "  - 中文翻译：[递归神经网络不可思议的有效性-CSDN.NET](http://www.csdn.net/article/2015-08-28/2825569)\n",
    "  - 有非常简单易懂的 RNN 原理分析\n",
    "  - 涉及到：[RMSProb（自适应学习率）](https://arxiv.org/abs/1502.04390)\n",
    "  - [一个 RNN Demo](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "    - **生成 sample 的代码很好用**\n",
    "  - [一个 LSTM Demo: An efficient, batched LSTM. · GitHub](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n",
    "  - 可视化解释 LSTM 结果\n",
    "  - 一些非常好的综述\n",
    "    - CNN 形成的原始感知器 + RNN glance 策略\n",
    "    - 归纳推理，存储和关注 “模块”\n",
    "      \n",
    "\n",
    "#### LSTM\n",
    "\n",
    "- [Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "  - 不解释，Tensorflow 官方文档推荐\n",
    "  - 两个不错的翻译版本：\n",
    "    - [[译] 理解 LSTM 网络 - 简书](http://www.jianshu.com/p/9dc9f41f0b29)\n",
    "    - [（译）理解 LSTM 网络 （Understanding LSTM Networks by colah） - 大学之道，在明明德 - 博客频道 - CSDN.NET](http://blog.csdn.net/jerr__y/article/details/58598296)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码步骤  \n",
    "\n",
    "详细整理一下代码实现需要的步骤（可以当伪代码看），主要目的是梳理思路和便于记忆。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN（单 Cell）\n",
    "\n",
    "重点是理解隐层的循环，假设只有一个 Cell，循环“序列长度”次，与普通 NN 不同的地方加粗标记。\n",
    "\n",
    "- 初始化及定义\n",
    "  - 定义交叉熵函数及其导函数\n",
    "    - $y=\\frac{1} {1+e^{-x}}$\n",
    "    - $y'(x) = y(1-y)$\n",
    "  - 输入和输出\n",
    "    - 假设 X 是 m 维，每次输入一个训练数据，即 1×m\n",
    "    - 输出 y 是 1×1\n",
    "  - 初始化 W 矩阵\n",
    "    - 假设隐层神经元数量为 n，则输入层 -> 隐层的 W0 矩阵：m×n\n",
    "    - 隐层 -> 输出层的 W1 矩阵：n×1\n",
    "    - **隐层 -> 隐层的 Wh 矩阵：n×n（这个就是 RNN，隐层自迭代一次）**\n",
    "  - **声明 W 矩阵的更新**\n",
    "    - W0_update：m×n\n",
    "    - W1_updata：n×1\n",
    "    - Wh_update：n×n\n",
    "    \n",
    "- Epoch 开始，定义各种变量存储位置\n",
    "  - 保存全局误差 overallError = 0\n",
    "  - 保存更新后的输出层：layer_2_deltas = list()\n",
    "  - 保存隐层的输出：layer_1_values = list()\n",
    "  - **隐层上一个时间点的输出：layer_1_values.append(np.zeros(n))** 这个很重要，相当于第一次循环时隐层的值（这时候还没有正向传播，没有值，所以赋值 0）\n",
    "  - **隐层下一个时间点更新后的输出：future_layer_1_delta = np.zeros(n)**，相当于第一次反向传播时更新后隐层的值（这时候还没有反向传播，没有指，所以赋值 0）\n",
    "\n",
    "- 序列前向传播\n",
    "  - 隐层输出：layer_1 = sigmoid(np.dot(X, W0) + **np.dot(layer_1_values[-1], Wh))**，其实就是普通 NN 的基础上加了隐层 -> 隐层对输出的影响，这个也是 RNN 的核心。sigmoid 是加了激活函数，维度是 1×n\n",
    "  - 输出层：layer_2 = sigmoid(np.dot(layer_1, W1))，维度是 1×1\n",
    "  - 输出层 Error：layer_2_error = y - layer_2\n",
    "  - 总误差：overallError += np.abs(layer_2_error[0])\n",
    "  - 保存隐层输出：layer_1_values.append(copy.deepcopy(layer_1))\n",
    "\n",
    "- 序列反向传播\n",
    "  - 更新后的输出层：layer_2_deltas.append((layer_2_error)·(layer_2)·(1-layer_2)) # 注意，后面两个点表示矩阵对应元素相乘\n",
    "  - 隐层对应的更新后的输出：layer_2_delta = layer_2_deltas[-1]\n",
    "  - **隐层更新后的输出**：  \n",
    "    layer_1_delta = (future_layer_1_delta.dot(Wh.T) +layer_2_delta.dot(W1.T))·(layer_1)·(1-layer_1))\n",
    "\n",
    "- 更新序列 W 矩阵\n",
    "  - 隐层的输出：layer_1 = layer_1_values[-1]\n",
    "  - 隐层上一时点的输出：prev_layer_1 = layer_1_values[-2]，循环一次就有两个输出了，相当于隐层自己迭代了一次\n",
    "  - 隐层 -> 输出层：W1_update += layer_1.T × layer_2_delta\n",
    "  - 隐层 -> 隐层：Wh_update += prev_layer_1.T × layer_1_delta\n",
    "  - 输入层 -> 隐层：W0_update += X.T × layer_1_delta\n",
    "  - 记录下本时序的隐藏层的 delta 用来在下一时序使用：future_layer_1_delta = layer_1_delta\n",
    "\n",
    "- 更新整体 W 矩阵\n",
    "  - W0 += W0_update * alpha\n",
    "  - W1 += W1_update * alpha\n",
    "  - Wh += Wh_update * alpha\n",
    "\n",
    "- Epoch 结束，更新变量归零\n",
    "  - W0_update *= 0\n",
    "  - W1_update *= 0\n",
    "  - Wh_update *= 0\n",
    "\n",
    "总结一下，与普通 NN 最大的区别就是多了隐层：上一时间点的输出，下一时间点更新后的输出，其他其实差不多。  \n",
    "需要注意的是，上面的步骤根据下面的代码总结而成，结合代码看会比较好。当然，重在理解原理，形式可以不那么看重。  \n",
    "如果换做自然语言，则输入为每句话（一组训练数据）的每个字，输出为下一个字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 代码解析\n",
    "\n",
    "具体代码实现，详细解读每一行代码的含义。代码内容如下：  \n",
    "\n",
    "- RNN Numpy 实现\n",
    "- LSTM Numpy 实现\n",
    "- RNN Tensorflow 实现\n",
    "- LSTM Tensorflow 实现\n",
    "- Bi-LSTM Tensorflow 实现\n",
    "\n",
    "### RNN\n",
    "\n",
    "#### RNN Numpy\n",
    "\n",
    "- [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) - i am trask](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "  - sigmoid 激活\n",
    "  - 存储下一个时刻的 delta（输出）\n",
    "- [Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy · GitHub](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "  - softmax 激活\n",
    "  - 存储下一个时刻的 delta×W 矩阵（Error）\n",
    "  - 防止梯度爆炸\n",
    "  - 字母模型\n",
    "  - for 循环方式\n",
    "- [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)\n",
    "  - softmax 激活\n",
    "  - 所有时刻的初始化为一个大矩阵\n",
    "  - 矩阵乘以向量直接写成一个矩阵，代码精简但不好理解\n",
    "  - 反向传播时，没有考虑下一个时刻的 delta（其实本来就不存在）\n",
    "  - 词模型\n",
    "  - 矩阵形式，没有 for 循环\n",
    "\n",
    "\n",
    "#### RNN Tensorflow\n",
    "\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "#### LSTM Numpy\n",
    "\n",
    "- [Simple LSTM](http://nicodjimenez.github.io/2014/08/08/lstm.html)\n",
    "- [An efficient, batched LSTM. · GitHub](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n",
    "- [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "\n",
    "#### LSTM Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "194px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "308px",
    "left": "3px",
    "right": "20px",
    "top": "37px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
