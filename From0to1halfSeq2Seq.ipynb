{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#理解-Seq2Seq\" data-toc-modified-id=\"理解-Seq2Seq-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>理解 Seq2Seq</a></div><div class=\"lev2 toc-item\"><a href=\"#笔记：RNN-的改进\" data-toc-modified-id=\"笔记：RNN-的改进-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>笔记：RNN 的改进</a></div><div class=\"lev3 toc-item\"><a href=\"#Seq2Seq-模型\" data-toc-modified-id=\"Seq2Seq-模型-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Seq2Seq 模型</a></div><div class=\"lev3 toc-item\"><a href=\"#Reinforcement-Learning\" data-toc-modified-id=\"Reinforcement-Learning-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Reinforcement Learning</a></div><div class=\"lev3 toc-item\"><a href=\"#Attention-机制\" data-toc-modified-id=\"Attention-机制-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Attention 机制</a></div><div class=\"lev3 toc-item\"><a href=\"#Memory-network-机制\" data-toc-modified-id=\"Memory-network-机制-114\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Memory network 机制</a></div><div class=\"lev3 toc-item\"><a href=\"#参考\" data-toc-modified-id=\"参考-115\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>参考</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解 Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 笔记：RNN 的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq 模型\n",
    "  - Encoder-Decoder 模型，表示知识的学习和应用。Encoder 每个时间片的学习，丰富自己的大脑（Cell），大脑容量固定（Cell 维度固定），过滤之前的知识（遗忘机制），同时增加新的认识（更新），获得知识（ Encoder 的输出向量）\n",
    "  - Encoder 对输入的句子进行编码，通过非线性变换转化为中间语义 C，Decoder 根据 C 和已经生成的历史信息（输出部分要生成的单词之前的词）来生成要生成的单词。每个词都这么出现，整体看起来就是根据输入的句子生成了目标句子。  \n",
    "    - $y_1 = f(C)$\n",
    "    - $y_2 = f(C,y_1)$\n",
    "    - $y_3 = f(C,y_1,y_2)$\n",
    "\n",
    "### Reinforcement Learning\n",
    "  - 融入学习技巧\n",
    "  - [[1606.01541] Deep Reinforcement Learning for Dialogue Generation](https://arxiv.org/abs/1606.01541)，在 Encoder 中加入强化学习（增强学习思想），类似加入了学习技巧和目的。这需要一定知识基础才能有效果，所以需要在一般 Seq2Seq 模型上进行预训练，再融入强化学习的新模型。\n",
    "\n",
    "### Attention 机制\n",
    "  - 资源分配机制，某个特定时刻，注意力总是集中在画面中的某个焦点部分，而对其他部分视而不见\n",
    "  - 通用思想，不依赖于 Seq2Seq\n",
    "  - 因为会遗忘，所以应用时有些需要重新学习。但如果每学到一个知识点都记录下来，应用时就可以查笔记，这就是在 Attention 机制。\n",
    "  - [Survey on Attention-based Models Applied in NLP – Yanran's Attic](http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html)\n",
    "  - 因为 C 一样（RNN 的 Encoder 后面输入的词影响较大），没有体现出注意力。如果句子较长，用中间语义向量表示所有语义会丢失很多细节。\n",
    "  - 注意力模型分配给每个输入单词概率大小不一样，不断发生变化的 C，每个 C 代表不同的输入单词注意力分配的概率分布。\n",
    "    - $y_1 = f(C_1)$\n",
    "    - $y_2 = f(C_2,y_1)$\n",
    "    - $y_3 = f(C_3,y_1,y_2)$\n",
    "\n",
    "### Memory network 机制\n",
    "  - 学会利用外界资源\n",
    "  - 一个应用：[[1702.01932] A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/abs/1702.01932)\n",
    "\n",
    "### 参考\n",
    "  - 整理汇总自以下博文，都是非常 Nice 的文章\n",
    "  - [关于RNN（Seq2Seq）的一点个人理解与感悟 - liuyuemaicha的专栏 - 博客频道 - CSDN.NET](http://blog.csdn.net/liuyuemaicha/article/details/56684516)\n",
    "  - [自然语言处理中的Attention Model：是什么及为什么 - 张俊林的博客 - 博客频道 - CSDN.NET](http://blog.csdn.net/malefactor/article/details/50550211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "139px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
