{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/notebooks/w1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 使用 jieba 分词并统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "limit = 10000\n",
    "lm = Counter()\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = jieba.cut(line.strip()) # strip把句子前后的空格去掉\n",
    "    for word in words:\n",
    "        # 转为 utf-8\n",
    "        lm[word.encode('utf-8')] += 1\n",
    "#        lm[word] += 1\n",
    "# 如果用 Counter project，则不需要写下面的 if-else\n",
    "#        if word in lm:\n",
    "#            lm[word] += 1\n",
    "#        else:\n",
    "#            lm[word] = 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\\uff0c', 19065),\n",
       " (u'\\u3002', 7170),\n",
       " (u'\\uff1a', 3934),\n",
       " (u'\\u201d', 3859),\n",
       " (u'\\u201c', 3858),\n",
       " (u'\\u9053', 2994),\n",
       " (u'\\u4e86', 2775),\n",
       " (u'\\u6211', 2313),\n",
       " (u'\\u4ed6', 2277),\n",
       " (u'\\u4f60', 2257)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示出现最多的10个词\n",
    "lm.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\xef\\xbc\\x8c', 19065),\n",
       " ('\\xe3\\x80\\x82', 7170),\n",
       " ('\\xef\\xbc\\x9a', 3934),\n",
       " ('\\xe2\\x80\\x9d', 3859),\n",
       " ('\\xe2\\x80\\x9c', 3858),\n",
       " ('\\xe9\\x81\\x93', 2994),\n",
       " ('\\xe4\\xba\\x86', 2775),\n",
       " ('\\xe6\\x88\\x91', 2313),\n",
       " ('\\xe4\\xbb\\x96', 2277),\n",
       " ('\\xe4\\xbd\\xa0', 2257)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转为 utf-8 \n",
    "lm.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 19065\n",
      "。 7170\n",
      "： 3934\n",
      "” 3859\n",
      "“ 3858\n",
      "道 2994\n",
      "了 2775\n",
      "我 2313\n",
      "他 2277\n",
      "你 2257\n"
     ]
    }
   ],
   "source": [
    "# 可以将结果打印出来\n",
    "for (word, cnt) in lm.most_common(10):\n",
    "    print('%s %d' % (word,cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用 pseg 分词，标注词性并统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "limit = 10000\n",
    "lm = Counter()\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    for word,flag in words:\n",
    "        # 如果是标点符号，就不要打印\n",
    "        if flag == 'x':\n",
    "            continue\n",
    "        # 转为 utf-8\n",
    "        lm[(word.encode('utf-8'), flag.encode('utf-8'))] += 1           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 不打印标点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道 q 3120\n",
      "了 ul 2795\n",
      "我 r 2454\n",
      "他 r 2387\n",
      "你 r 2266\n",
      "那 r 2096\n",
      "的 uj 1986\n",
      "是 v 1598\n",
      "行者 n 1562\n",
      "来 v 1028\n"
     ]
    }
   ],
   "source": [
    "# 不打印标点符号\n",
    "for (word,flag), cnt in lm.most_common(10):\n",
    "    print('%s %s %d' % (word, flag, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132268"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 所有词的个数\n",
    "sum(lm.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算各词百分比\n",
    "s = float(sum(lm.values()))\n",
    "for key, cnt in lm.items():\n",
    "    lm[key] /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道 q 0.023588\n",
      "了 ul 0.021131\n",
      "我 r 0.018553\n",
      "他 r 0.018047\n",
      "你 r 0.017132\n",
      "那 r 0.015847\n",
      "的 uj 0.015015\n",
      "是 v 0.012082\n",
      "行者 n 0.011809\n",
      "来 v 0.007772\n",
      "有 v 0.007583\n",
      "这 r 0.007266\n",
      "在 p 0.007266\n",
      "也 d 0.007069\n",
      "去 v 0.006751\n",
      "又 d 0.005980\n",
      "不 d 0.005806\n",
      "得 ud 0.005776\n",
      "与 p 0.005708\n",
      "师父 n 0.005217\n"
     ]
    }
   ],
   "source": [
    "for (word,flag), cnt in lm.most_common(20):# 取 20 个\n",
    "    print('%s %s %f' % (word, flag, cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 只显示词和百分比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "limit = 10000\n",
    "lm = Counter()\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    for word,flag in words:\n",
    "        # 如果是标点符号，就不要打印\n",
    "        if flag == 'x':\n",
    "            continue\n",
    "        # 转为 utf-8\n",
    "        lm[word.encode('utf-8')] += 1           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算各词百分比\n",
    "s = float(sum(lm.values()))\n",
    "for key, cnt in lm.items():\n",
    "    lm[key] /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道 0.023672\n",
      "了 0.021131\n",
      "我 0.018553\n",
      "他 0.018047\n",
      "你 0.017132\n",
      "那 0.015847\n",
      "的 0.015015\n",
      "是 0.012082\n",
      "行者 0.011809\n",
      "来 0.007772\n",
      "有 0.007583\n",
      "在 0.007266\n",
      "这 0.007266\n",
      "也 0.007069\n",
      "去 0.006751\n",
      "又 0.005980\n",
      "不 0.005829\n",
      "得 0.005776\n",
      "与 0.005708\n",
      "师父 0.005217\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm.most_common(20):# 取 20 个\n",
    "    print('%s %f' % (word, cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 基于语料构建词语生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(lm):\n",
    "    r = random.random() # 生成随机数\n",
    "    s_ = 0.0\n",
    "    for (word, prob) in lm.items():\n",
    "        s_ += prob\n",
    "        if s_ >= r:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "念咒\n"
     ]
    }
   ],
   "source": [
    "# 每次可以 print 出一个不同的词\n",
    "print(generate(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 生成段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "青霄 擒 招 行者 路 作 打 门首 行者 道 老 菩萨 望见 那沙僧 行者 行者 推聋妆哑 他 奴才 今日 光 行者 筑 我 按 不得 见 大 拜佛 宣 对 受苦 揭开 捉 观众 你 扑 看看 火 去 缠 坐在 先锋 八戒 云端 第一 血 个 教 把 果是 穿 下 不 老子 莫惊 之 马上 他 祸 又 都 也 直接 度牒 他 我 传家之宝 妖 跳 觌 熟 报道 高 所有 行者 偏正 闲 孙 说 一观 这 同行者 声金 是 愆 不息 地方 诀窍 将 骨 的 筒子 想 兵 他 难 了 一口 耍子 \n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "for i in range(100):\n",
    "    word = generate(lm)\n",
    "    s += word + ' ' # 用空格将生成的词语隔开\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 我 丢 翻腾 模样 判官 仍 骂 又 两个 哩 星月 迎 原来 阁 瞒 你 师父 一个多月 如何 那话儿 说 问道 既 那处 也 山根 教 是 僧家 脚尖儿 里灵霄 回东 路径 说 了 打造 去 到 一行 这 得 享 只管 之 他 生 时 若 黑鱼 慌 玉液 且 正 的 门 嚷 好 惫懒 倒 捉拿 天 我 与 进去 了 难 来 个贩 得 这 即 什么 瞒 而复 不要 那 你 什么 那 得 陪 与 腰 断乎 道 一根 上午 我 眉 曰 及 是 来 的 受用 鞭 问 家书 点点 \n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "for i in range(100):\n",
    "    word = generate(lm)\n",
    "    s += word + ' ' # 用空格将生成的词语隔开\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 将标点符号也考虑进去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "limit = 10000\n",
    "lm = Counter()\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    for word,flag in words:\n",
    "        # 转为 utf-8\n",
    "        lm[word.encode('utf-8')] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 19065\n",
      "。 7170\n",
      "： 3934\n",
      "” 3859\n",
      "“ 3858\n",
      "道 3131\n",
      "了 2795\n",
      "我 2454\n",
      "他 2387\n",
      "你 2266\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm.most_common(10):\n",
    "    print('%s %d' % (word,cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算各词百分比\n",
    "s = float(sum(lm.values()))\n",
    "for key, cnt in lm.items():\n",
    "    lm[key] /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 0.109139\n",
      "。 0.041045\n",
      "： 0.022520\n",
      "” 0.022091\n",
      "“ 0.022085\n",
      "道 0.017924\n",
      "了 0.016000\n",
      "我 0.014048\n",
      "他 0.013665\n",
      "你 0.012972\n",
      "那 0.011999\n",
      "的 0.011369\n",
      "！ 0.009835\n",
      "是 0.009148\n",
      "行者 0.008942\n",
      "？ 0.008919\n",
      "来 0.005885\n",
      "有 0.005742\n",
      "在 0.005501\n",
      "这 0.005501\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm.most_common(20):# 取 20 个\n",
    "    print('%s %f' % (word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，。你，今日本庄伶仃盖留衣服：道文殊心复来龙头着正说正当这尸首冤家大胆腰疼了叫形影这，出道猴：”看看的”自己波翻浪他师父将！若了来一堆：宝殿只祈，祖宗道整一整行筋八戒呼唤—。知了行者鞭！人。哩，提念金銮殿行者摇身一变挑担方丈，，？出，这轮才哩”：是天下三人你可以有个你山门是太子罪\n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "for i in range(100):\n",
    "    word = generate(lm)\n",
    "    s += word\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. 改进的语言模型：n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ngram = 2 时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（\t明\n",
      "）\n",
      "-----\n",
      "明\t）\n",
      "吴承恩\n",
      "-----\n",
      "）\t吴承恩\n",
      "著\n",
      "-----\n",
      "灵根\t育孕\n",
      "源流\n",
      "-----\n",
      "育孕\t源流\n",
      "出\n",
      "-----\n",
      "源流\t出\n",
      "心性\n",
      "-----\n",
      "出\t心性\n",
      "修持\n",
      "-----\n",
      "心性\t修持\n",
      "大道\n",
      "-----\n",
      "修持\t大道\n",
      "生\n",
      "-----\n",
      "诗\t曰\n",
      "：\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "line_no = 0\n",
    "limit = 5\n",
    "lm = Counter()\n",
    "ngram = 2\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    # cut 生成的是 generator，需要转换为 list 才能使用 len 函数\n",
    "    words = [i.word for i in words] # pseg.cut 返回一对值，只需要\n",
    "    # a,b,c,d\n",
    "    # bi-gram\n",
    "    # (c/a,b)\n",
    "    # (d/b,c)\n",
    "    # 2-gram，从第 3 个词开始 \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i]\n",
    "        print('\\t'.join(context))\n",
    "        print(word)\n",
    "        print('-----')\n",
    "    \n",
    "#    for word,flag in words:\n",
    "#        # 转为 utf-8\n",
    "#        lm[word.encode('utf-8')] += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 n-gram = 5 时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "灵根\t育孕\t源流\t出\t心性\n",
      "修持\n",
      "-----\n",
      "育孕\t源流\t出\t心性\t修持\n",
      "大道\n",
      "-----\n",
      "源流\t出\t心性\t修持\t大道\n",
      "生\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "line_no = 0\n",
    "limit = 5\n",
    "lm = Counter()\n",
    "ngram = 5\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    # cut 生成的是 generator，需要转换为 list 才能使用 len 函数\n",
    "    words = [i.word for i in words] # pseg.cut 返回一对值，只需要\n",
    "    # a,b,c,d\n",
    "    # bi-gram\n",
    "    # (c/a,b)\n",
    "    # (d/b,c)\n",
    "    # 2-gram，从第 3 个词开始 \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i]\n",
    "        print('\\t'.join(context))\n",
    "        print(word)\n",
    "        print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过下面代码的分析，我们知道 ngram = 5 时，很多行其实并未被使用到\n",
    "- 因为原文是按行存储的，我们也是按行读入的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "\n",
      "0\n",
      "------\n",
      "（ 明 ） 吴承恩 著\n",
      "7\n",
      "------\n",
      "第一回\n",
      "3\n",
      "------\n",
      "灵根 育孕 源流 出 心性 修持 大道 生\n",
      "14\n",
      "------\n",
      "诗 曰 ：\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "line_no = 0\n",
    "limit = 5\n",
    "lm = Counter()\n",
    "ngram = 5\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    # cut 生成的是 generator，需要转换为 list 才能使用 len 函数\n",
    "    words = [i.word for i in words] # pseg.cut 返回一对值，只需要\n",
    "    print('------')    \n",
    "    print(' '.join(words))\n",
    "    print(len(''.join(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_no = 0\n",
    "limit = 1000\n",
    "lm0 = Counter()\n",
    "#lm1 = Counter()\n",
    "lm2 = Counter()\n",
    "ngram = 2\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    # cut 生成的是 generator，需要转换为 list 才能使用 len 函数\n",
    "    words = [i.word for i in words] # pseg.cut 返回一对值，只需要\n",
    "    # a,b,c,d\n",
    "    # bi-gram\n",
    "    # (c/a,b)\n",
    "    # (d/b,c)\n",
    "    # 2-gram，从第 3 个词开始 \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i-ngram]\n",
    "        lm0[context[0].encode('utf-8'),context[1].encode('utf-8')] += 1\n",
    "        lm2[word.encode('utf-8')] += 1\n",
    "#        print('\\t'.join(context))\n",
    "#        print('-----')\n",
    "#        print(word)\n",
    "#        word1 = words[(i-ngram)]\n",
    "#        word2 = words[i-ngram]\n",
    "#        lm0[context[0].encode('utf-8'),context[1].encode('utf-8')] += 1\n",
    "#        lm2[context[0].encode('utf-8')] += 1\n",
    "#        lm1[word1.encode('utf-8')] += 1\n",
    "#        lm2[word2.encode('utf-8')] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10386"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3981"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15221"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lm0.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15221"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lm2.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：虽然 lm0 和 lm2 的长度不一样，但求和总数是一样的，因为都在一个循环内？**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名鹰 1\n",
      "没管 1\n",
      "惊张 1\n",
      "土 6\n",
      "不瞒你说 1\n",
      "一遍 3\n",
      "脑浆 1\n",
      "形 1\n",
      "夸赞 1\n"
     ]
    }
   ],
   "source": [
    "for word,cnt in lm2.items()[0:9]:\n",
    "    print('%s %d' %(word,cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "揭 不得 1\n",
      "此处 远 1\n",
      "三藏 着 1\n",
      "及 回看 1\n",
      "摸摸 ， 1\n",
      "坐 着 2\n",
      "做 如意 1\n",
      "的 人物 1\n",
      "我 亲 1\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2),cnt in lm0.items()[0:9]:\n",
    "    print('%s %s %d' %(word1,word2,cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算各词百分比\n",
    "s0 = float(sum(lm0.values()))\n",
    "for key, cnt in lm0.items():\n",
    "    lm0[key] /= s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算各词百分比\n",
    "s2 = float(sum(lm2.values()))\n",
    "for key, cnt in lm2.items():\n",
    "    lm2[key] /= s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "： “ 0.020564\n",
      "道 ： 0.017739\n",
      "。 ” 0.009986\n",
      "？ ” 0.004796\n",
      "” 三藏 0.004599\n",
      "！ ” 0.003876\n",
      "” 行者 0.003285\n",
      "， 你 0.003285\n",
      "， 我 0.003154\n",
      "三藏 道 0.002956\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2), cnt in lm0.most_common(10):\n",
    "    print('%s %s %f'  % (word1, word2, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 0.109980\n",
      "。 0.037448\n",
      "： 0.021024\n",
      "“ 0.020892\n",
      "” 0.019578\n",
      "道 0.018527\n",
      "了 0.016425\n",
      "我 0.015702\n",
      "他 0.012943\n",
      "的 0.012943\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm2.most_common(10):\n",
    "    print('%s %f' %(word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = {}\n",
    "\n",
    "for i in range(len(lm0)):\n",
    "    lm[lm0.items()[i][0]]  = lm0.items()[i][1]/lm2[lm0.items()[i][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "揭 不得 0.111111\n",
      "此处 远 0.142857\n",
      "三藏 着 0.007407\n",
      "及 回看 0.333333\n",
      "摸摸 ， 1.000000\n",
      "坐 着 0.181818\n",
      "做 如意 0.034483\n",
      "的 人物 0.005076\n",
      "我 亲 0.004184\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2), cnt in lm.items()[0:9]:\n",
    "    print('%s %s %f' %(word1, word2, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 错误代码，没有搞清楚贝叶斯\n",
    "\n",
    "lm = {}\n",
    "\n",
    "# len(lm0) = 10386\n",
    "# len(lm2) = 3855\n",
    "for i in range(10385):\n",
    "    for j in range(3854):\n",
    "        lm[(lm0.items()[i][0], lm2.items()[j][0])]  = lm0.items()[i][1]*lm2.items()[j][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 合并代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.381 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "line_no = 0\n",
    "limit = 10000\n",
    "lm = {}\n",
    "lm0 = Counter()\n",
    "lm2 = Counter()\n",
    "ngram = 2\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    words = [i.word for i in words]\n",
    "    \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i]\n",
    "        lm0[context[0].encode('utf-8'),context[1].encode('utf-8')] += 1\n",
    "        #lm2[context[0].encode('utf-8')] += 1\n",
    "        lm2[word.encode('utf-8')] += 1\n",
    "\n",
    "# 将 values 从数额换成百分比\n",
    "s0 = float(sum(lm0.values()))\n",
    "for key, cnt in lm0.items():\n",
    "    lm0[key] /= s0\n",
    "\n",
    "s2 = float(sum(lm2.values()))\n",
    "for key, cnt in lm2.items():\n",
    "    lm2[key] /= s2\n",
    "\n",
    "# P(B|A) = P(A*B)/P(A)\n",
    "# 注意，这里应该是： P(AB) = P(A|B) * P(B)，上面的小代码按上一行的公式\n",
    "for i in range(len(lm0)):\n",
    "    lm[lm0.items()[i][0]]  = lm0.items()[i][1] * lm2[lm0.items()[i][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 洞 0.000006\n",
      "今 喜寿 0.000006\n",
      "急 整衣 0.000006\n",
      "了 几卷 0.000006\n",
      "大圣 归善 0.000006\n",
      "蟮 ， 0.000006\n",
      "也 认得 0.000013\n",
      "宽限 。 0.000006\n",
      "戒 。 0.000006\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2), cnt in lm0.items()[0:9]:\n",
    "    print('%s %s %f' %(word1, word2, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赛过 0.000019\n",
      "承领 0.000006\n",
      "寻寻 0.000006\n",
      "土 0.000116\n",
      "圜 0.000019\n",
      "吉日 0.000019\n",
      "体性 0.000006\n",
      "守承 0.000006\n",
      "念咒 0.000077\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm2.items()[0:9]:\n",
    "    print('%s %f' %(word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.021669796033848606, 6.430206538234008e-06)"
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lm0.values()),min(lm0.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11456698989171532, 6.430206538234008e-06)"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lm2.values()),min(lm2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005201609390310762, 0.0)"
      ]
     },
     "execution_count": 970,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lm.values()),min(lm.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155516.0"
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155516.0"
      ]
     },
     "execution_count": 958,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74135"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17603"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 补充：About Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "line_no = 0\n",
    "limit = 10\n",
    "lm = Counter()\n",
    "ngram = 2\n",
    "dct = defaultdict(Counter)\n",
    "\n",
    "for line in open('/tensorflow/w0/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    # cut 生成的是 generator，需要转换为 list 才能使用 len 函数\n",
    "    words = [i.word for i in words] # pseg.cut 返回一对值，只需要\n",
    "    # a,b,c,d\n",
    "    # bi-gram\n",
    "    # (c/a,b)\n",
    "    # (d/b,c)\n",
    "    # 2-gram，从第 3 个词开始 \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i]\n",
    "        dct['context']['word'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('context', Counter({'word': 57}))]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'word': 57})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('context', Counter({'word': 57}))]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dct.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'word': 57})]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defaultdict 表示当 key 不在 dict 中时，会构造一个函数\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dct = defaultdict(Counter)\n",
    "\n",
    "dct['context']['word'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "d = collections.defaultdict(list)\n",
    "for k, v in s:\n",
    "    d[k].append(v)\n",
    "list(d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "# defaultdict\n",
    "d = collections.defaultdict(list)\n",
    "for k, v in s:\n",
    "    d[k].append(v)\n",
    "# Use dict and setdefault   \n",
    "g = {}\n",
    "for k, v in s:\n",
    "    g.setdefault(k, []).append(v)\n",
    "      \n",
    "# Use dict\n",
    "e = {}\n",
    "for k, v in s:\n",
    "    e[k] = v\n",
    "##list(d.items())\n",
    "##list(g.items())\n",
    "##list(e.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(g.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 4), ('red', 1), ('yellow', 3)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(e.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 构造 n-gram 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(lm):\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    r = random.uniform(min(lm.values()),max(lm.values())) # 生成在区间内随机数\n",
    "    s_ = 0.0\n",
    "    for (word1, word2), prob in lm.items():\n",
    "        s_ += prob\n",
    "        if s_ >= r:\n",
    "            return word1 + word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，特了便，学。那，忽，多多，命，配，摘，遇木。大圣，果，正，莫来，老和尚，迎，多多，手执，吹。那马，不识，妖精，吹，大势，特。那，特，忽，里面。那，冒冒！变化，径自，吹，正，学。那，忽，忽。诚他扎大仙在了奸心，四海，移，兵器到他他那，多，取我看，何来了与那，跨海，忽，忽，正，吹。那，夜授，滑冻，夜授“莫要，上头“女的吃。那的形影，怨。那，外边，顺出，穿，取，特，映着，正，庆贺，忽，吹，水，大势，贯。那，分开，忽，找，上头”这，至儿来，找，有烦。那，忽。那。言语，条条他这，穿，撞。那，虚里他。，差说？。那，乱打，敌。那，撞，大势，学，取，忽，四海，忽，吹，妖精师父渡河，喘气，望空，取，映着，不识。那在老孙，穿。那，俯仰我徒弟，乱打，不识。大圣。那，至“唐僧，跨海，吹弹歌舞，找。大圣，近朱者赤，正，水。那，手执，逢你先，正，毂辘，分开。那。那，人间，找。大圣，谁，四海，忽。那，正，倘，何”却”果，你们，忽，找，分开，鸿蒙他。，你好，不非。那。那，水，多“开园，取，撞，忽，算了，腾，往里，冒冒，找，水势，你们，吹，忽。那救你们，幸有，入林，点，吹，跌的供不强似\n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "for i in range(200):\n",
    "    word = generate(lm)\n",
    "    s += str(word)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 新的 n-gram 生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**可以写一个随机开始词，然后依据这个词找到 lm（词组构成的字典）的最大概率词，循环若干次生成若干个词构成的句子或段落。**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/notebooks/OMOOC.DeepLearning/w1_Ngram'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/OMOOC.DeepLearning\n"
     ]
    }
   ],
   "source": [
    "cd OMOOC.DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/notebooks/OMOOC.DeepLearning/w1_Ngram'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/OMOOC.DeepLearning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mAssisantEvaluate\u001b[0m/  \u001b[01;34mw0_PreStart\u001b[0m/  \u001b[01;34mw1_Ngram\u001b[0m/  \u001b[01;34mw2_NaiveBayes\u001b[0m/  \u001b[01;34mw3_Tensorflow-NN\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.384 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "line_no = 0\n",
    "limit = 200\n",
    "lm = {}\n",
    "lm0 = Counter()\n",
    "lm2 = Counter()\n",
    "ngram = 2\n",
    "\n",
    "for line in open('./AssisantEvaluate/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    words = [i.word for i in words]\n",
    "    \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i]\n",
    "        lm0[context[0].encode('utf-8'),context[1].encode('utf-8')] += 1\n",
    "        #lm2[context[0].encode('utf-8')] += 1\n",
    "        lm2[word.encode('utf-8')] += 1\n",
    "\n",
    "# 将 values 从数额换成百分比\n",
    "s0 = float(sum(lm0.values()))\n",
    "for key, cnt in lm0.items():\n",
    "    lm0[key] /= s0\n",
    "\n",
    "s2 = float(sum(lm2.values()))\n",
    "for key, cnt in lm2.items():\n",
    "    lm2[key] /= s2\n",
    "\n",
    "# P(B|A) = P(A*B)/P(A)\n",
    "# 注意，这里应该是： P(AB) = P(A|B) * P(B)，上面的小代码按上一行的公式\n",
    "for i in range(len(lm0)):\n",
    "    lm[lm0.items()[i][0]]  = lm0.items()[i][1] * lm2[lm0.items()[i][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('\\xe3\\x80\\x82', '\\xe4\\xbb\\x96'), 0.0007730962504831851)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm0.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "也 省得 0.000001\n",
      "丈 四尺 0.000000\n",
      "三等 名 0.000000\n",
      "列位 安眠 0.000000\n",
      "此处 远 0.000000\n",
      "骏马 ， 0.000000\n",
      "禽 言 0.000000\n",
      "到 此天 0.000000\n",
      "立志 潜心 0.000000\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2), cnt in lm.items()[0:9]:\n",
    "    print('%s %s %f' %(word1, word2, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 他 0.000773\n",
      "也 省得 0.000387\n",
      "丈 四尺 0.000387\n",
      "三等 名 0.000387\n",
      "列位 安眠 0.000387\n",
      "此处 远 0.000387\n",
      "骏马 ， 0.000387\n",
      "禽 言 0.000387\n",
      "到 此天 0.000387\n"
     ]
    }
   ],
   "source": [
    "for (word1,word2), cnt in lm0.items()[0:9]:\n",
    "    print('%s %s %f' %(word1, word2, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浮玉 0.000387\n",
      "排列 0.000387\n",
      "土 0.000773\n",
      "奇葩 0.000387\n",
      "运通 0.000387\n",
      "异果 0.000387\n",
      "不觉 0.000387\n",
      "罗拜 0.000387\n",
      "古云 0.000387\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in lm2.items()[0:9]:\n",
    "    print('%s %f' % (word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(lm):\n",
    "    \n",
    "    import random\n",
    "    wordtest = ''\n",
    "    r = random.uniform(min(lm.values()),max(lm.values())) # 生成在区间内随机数\n",
    "    s_ = 0.0\n",
    "    for word, prob in lm.items():\n",
    "        s_ += prob\n",
    "        if s_ >= r:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各享\n"
     ]
    }
   ],
   "source": [
    "print(generate(lm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordtest = generate(lm2)\n",
    "\n",
    "lmm ={}\n",
    "\n",
    "for (word1, word2), prob in lm.items():\n",
    "    \n",
    "    if word2 == wordtest:\n",
    "        \n",
    "        lmm[(word1, word2)] = lm[(word1, word2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('\\xe5\\xb1\\xb1\\xe7\\x9f\\xb3', '\\xe5\\x9c\\x9f'): 1.4941945312778993e-07,\n",
       " ('\\xe6\\x9c\\x89', '\\xe5\\x9c\\x9f'): 5.379100312600437e-06,\n",
       " ('\\xe9\\xab\\x98\\xe7\\xa7\\xaf', '\\xe5\\x9c\\x9f'): 1.4941945312778993e-07}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmmm = {}\n",
    "\n",
    "for (word1, word2), prob in lmm.items():\n",
    "    \n",
    "    if prob == max(lmm.values()):\n",
    "        \n",
    "        lmmm[(word1, word2)] = lmm[(word1, word2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('\\xe5\\x8f\\x88', '\\xe8\\xbf\\x9b\\xe6\\x9d\\xa5'): 1.7930334375334792e-06,\n",
       " ('\\xe6\\x88\\x91', '\\xe8\\xbf\\x9b\\xe6\\x9d\\xa5'): 1.7930334375334792e-06}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate1(lm):\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    r = random.uniform(min(lm.values()),max(lm.values())) # 生成在区间内随机数\n",
    "    s_ = 0.0\n",
    "    for word, prob in lm.items():\n",
    "        s_ += prob\n",
    "        if s_ >= r:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(wordtest,lm):\n",
    "    lmm ={}\n",
    "    lmmm = {}\n",
    "#    wordtest = generate1(lm1)\n",
    "    \n",
    "    for (word1, word2), prob in lm.items():\n",
    "        if word2 == wordtest:\n",
    "            lmm[(word1, word2)] = lm[(word1, word2)]\n",
    "    \n",
    "    for (word1, word2), prob in lmm.items():\n",
    "        if prob == max(lmm.values()):\n",
    "            lmmm[(word1, word2)] = lmm[(word1, word2)]\n",
    "    return word1 + word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe7\\x9b\\x98\\xe5\\x8f\\xa4\\xe5\\xbc\\x80\\xe8\\xbe\\x9f'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(generate1(lm2),lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，无忧无虑隐了，不觉比山猴而日落满地奇葩五点梅花其石北俱见有金字儿隐五点梅花故曰地务必访原堤草色，拉的拉仔细再他围住，大声为花果山一跳着驴骡猴拍手在仙山九宫八卦，不觉一仙石中跳出，整整齐齐，惟有奉酒，学座铁板桥是我们。此收拾些，忽赐恩座铁板桥，惟有隐了赐恩猴道行”猴王翠柏长春其石更解搬得力因见风隐了隐了故曰地享乐天真竿修是我们连日东南风一点儿远虑在仙山樽 复瞑目会元功出心性其石故曰混沌故曰地一跳精月华，而复。持篙中跳出，萦回座铁板桥世界之间搬得力马猴等。故曰满地奇葩，善哉摘异果为花果山潜息矣九窍八孔称美猴王果独自跳上岸。此有土隐了北俱樽 隐了翠柏长春称美猴王那股涧水“天气北俱见那股瓜涌溅涧壑此三者\n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "\n",
    "for i in range(100):\n",
    "    word = generate(generate1(lm2),lm)\n",
    "#    s[i+1] = generate(s[i])\n",
    "    s += word\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. 新代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "line_no = 0\n",
    "limit = 200\n",
    "dct = defaultdict(Counter)\n",
    "ngram = 2\n",
    "\n",
    "for line in open('./AssisantEvaluate/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    words = [i.word.encode('utf-8') for i in words]\n",
    "    \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = ''.join(words[(i-ngram):i])\n",
    "        word = words[i]\n",
    "        dct[context][word] += 1\n",
    "        \n",
    "    for context, wordcnt in dct.items():\n",
    "        s = float(sum(wordcnt.values()))\n",
    "        for w, cnt in wordcnt.items():\n",
    "            dct[context][word] = cnt/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(lm):\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    r = random.uniform(min(lm[context].values()),max(lm[context].values())) # 生成在区间内随机数\n",
    "    s_ = 0.0\n",
    "    for word, prob in lm[context].items():\n",
    "        s_ += prob\n",
    "        if s_ >= r:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇得遇\n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "\n",
    "for i in range(100):\n",
    "    word = generate(dct)\n",
    "    s += word\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "line_no = 0\n",
    "limit = 200\n",
    "lm = {}\n",
    "lm0 = Counter()\n",
    "lm2 = Counter()\n",
    "ngram = 2\n",
    "\n",
    "for line in open('./AssisantEvaluate/西游记.txt'):\n",
    "    line_no += 1\n",
    "    if line_no > limit:\n",
    "        break\n",
    "    words = pseg.cut(line.strip())\n",
    "    words = [i.word for i in words]\n",
    "    \n",
    "    for i in range(ngram, len(words)):\n",
    "        context = words[(i-ngram):i]\n",
    "        word = words[i-ngram]\n",
    "        lm0[context[0].encode('utf-8'),context[1].encode('utf-8')] += 1\n",
    "        lm2[word.encode('utf-8')] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lm0)):\n",
    "    lm[lm0.items()[i][0]]  = float(lm0.items()[i][1]) / lm2[lm0.items()[i][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate1(lm):\n",
    "    r = random.random() # 生成随机数\n",
    "    s_ = 0.0\n",
    "    for (word, prob) in lm.items():\n",
    "        s_ += prob/100.0\n",
    "        if s_ >= r:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(wordtest,lm):\n",
    "    lmm ={}\n",
    "    lmmm = {}\n",
    "    \n",
    "    for (word1, word2), prob in lm.items():\n",
    "        if word1 == wordtest:\n",
    "            lmm[(word1, word2)] = lm[(word1, word2)]\n",
    "    \n",
    "    for (word1, word2), prob in lmm.items():\n",
    "        if prob == max(lmm.values()):\n",
    "            lmmm[(word1, word2)] = lmm[(word1, word2)]\n",
    "    return word1 + word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(wordtest,lm):\n",
    "\n",
    "    for (word1, word2), prob in lm.items():\n",
    "        if word1 == wordtest:\n",
    "            return word1 + word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圣大开辟以来于丑神圣三者在山中须臾回金光焰了。何怕阎君些枯松桥边有者，呼兄，迟眠不金光焰午、地，难。于丑仙山之内隆，五形齿肩了。呼兄，了。滑凳板生花芋栗剖开仙山之内浮玉，了。二将果神圣三者了。跳出一个通者，异果，在山中五形迸裂，辰时食后此，仙桃，者，在山中午、桥边有呼兄，了。云。何怕阎君些枯松迟眠不些枯松仙桃，了。芋栗剖开些枯松地，仙桃，在山中了。遮闭门户海波中跳出一个通跳出一个通金光焰古云：二将果开辟以来滑凳板生花锦鸡鸣列位安眠成家之者，了。地秀，芋栗剖开古云：于丑海波中了。在山中此，了。至时来跳出一个通直至西 靠遮闭门户在山中浮玉，神圣三者神圣三者神圣三者遮闭门户猕猴、迟眠不遮闭门户芋栗剖开\n"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "\n",
    "for i in range(100):\n",
    "    word = generate(generate1(lm2),lm)\n",
    "    s += word\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\"\"\"读取语料 生成 n-gram 模型\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "#from pprint import pprint\n",
    "from random import random\n",
    "import jieba\n",
    "\n",
    "\n",
    "# N = 2  # N元模型\n",
    "START = '$$' # 句首的 token\n",
    "BREAK = '。！？'  # 作为句子结束的符号\n",
    "IGNORE = '\\n “”\"《》〈〉()*'  # 忽略不计的符号\n",
    "\n",
    "\n",
    "def process_segs(segments):\n",
    "    \"\"\"对 segments (iterator) 进行处理，返回一个 list. 处理规则： \n",
    "    - 忽略 \\n、空格、引号、书名号等\n",
    "    - 在断句符号后添加 START token\n",
    "    \"\"\"\n",
    "    results = [START for i in range(N-1)]\n",
    "    for seg in segments:\n",
    "        if seg in IGNORE:\n",
    "            continue\n",
    "        else:\n",
    "            results.append(seg)\n",
    "            if seg in BREAK:\n",
    "                results.extend([START for i in range(N-1)])\n",
    "    # 小瑕疵：segments 会以 start token 结束，但对语言模型没有影响，暂且忽略\n",
    "    return results\n",
    "\n",
    "\n",
    "def count_ngram(segments):\n",
    "    \"\"\"统计 N-gram 出现次数\"\"\"\n",
    "    dct = defaultdict(Counter)\n",
    "    for i in range(N-1, len(segments)):\n",
    "        context = tuple(segments[i-N+1:i])\n",
    "        word = segments[i]\n",
    "        dct[context][word] += 1\n",
    "    return dct\n",
    "\n",
    "\n",
    "def to_prob(dct):\n",
    "    \"\"\"将次数字典转换为概率字典\"\"\"\n",
    "    prob_dct = dct.copy()\n",
    "    for context, count in prob_dct.items():\n",
    "        total = sum(count.values())\n",
    "        for word in count:\n",
    "            count[word] /= total  # works in Python 3\n",
    "    return prob_dct\n",
    "\n",
    "\n",
    "def generate_word(context):\n",
    "    \"\"\"根据 context 及条件概率，随机生成 word\"\"\"\n",
    "    r = random()\n",
    "    psum = 0\n",
    "    for word, prob in prob_dct[context].items():\n",
    "        psum += prob\n",
    "        if psum > r:\n",
    "            return word\n",
    "    #return START\n",
    "\n",
    "\n",
    "def generate_sentences(m):\n",
    "    \"\"\"生成 m 个句子\"\"\"\n",
    "    sentences = []\n",
    "    text = ''\n",
    "    context = tuple(START for i in range(N-1))\n",
    "    i = 0\n",
    "    while (i < m):\n",
    "        word = generate_word(context)\n",
    "        text = text + word\n",
    "        context = tuple((list(context) + [word])[1:])\n",
    "        if word in BREAK:\n",
    "            sentences.append(text)\n",
    "            text = ''\n",
    "            context = tuple(START for i in range(N-1))\n",
    "            i += 1\n",
    "    return sentences\n",
    "\n",
    "\n",
    "for N in range(2, 6):\n",
    "    print('\\n*** reading corpus ***')\n",
    "    with open('ZhangAiLing.txt') as f:\n",
    "        corpus = f.read()\n",
    "    print('*** cutting corpus ***')\n",
    "    raw_segments = jieba.cut(corpus)\n",
    "    print('*** processing segments ***')\n",
    "    segments = process_segs(raw_segments)\n",
    "    print('*** generating {}-gram count dict ***'.format(N))\n",
    "    dct = count_ngram(segments)\n",
    "    print('*** generating {}-gram probability dict ***'.format(N))\n",
    "    prob_dct = to_prob(dct)\n",
    "    #pprint(prob_dct)\n",
    "    print('*** generating sentences ***')\n",
    "    with open('generated_{}gram.txt'.format(N), 'w') as f:\n",
    "        f.write('\\n'.join(generate_sentences(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from jieba import posseg as pseg\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "In [165]:\n",
    "\n",
    "def load_file(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    segs = []\n",
    "    for line in lines:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        words = pseg.cut(line)\n",
    "        words = [key for (key, flag) in words]\n",
    "        for word in words:\n",
    "            segs.append(word)            \n",
    "    return segs\n",
    "\n",
    "In [166]:\n",
    "\n",
    "def create_model(segs, n, padding):\n",
    "    Delimiter = [u'。',u'！',u'？']\n",
    "    lm = defaultdict(Counter)\n",
    "    if n < 1:\n",
    "        n = 1\n",
    "    post_segs = []\n",
    "    for i in range(n):\n",
    "        post_segs.append(padding)\n",
    "    for word in segs:\n",
    "        post_segs.append(word)\n",
    "        context = tuple(post_segs[-n-1:-1])\n",
    "        lm[context][word] += 1\n",
    "        if word in Delimiter:\n",
    "            for j in range(n):\n",
    "                post_segs.append(padding)  \n",
    "                \n",
    "    for key, cnt in lm.items():\n",
    "        s = float(sum(cnt.values()))\n",
    "        for word in cnt:\n",
    "            cnt[word] /= s\n",
    "    return lm\n",
    "\n",
    "In [167]:\n",
    "\n",
    "def generate_head(n, pad):\n",
    "    head = []\n",
    "    for i in range(n):\n",
    "       head.append(pad)\n",
    "    return head\n",
    "\n",
    "In [168]:\n",
    "\n",
    "def generate_word(lm_counter, context):\n",
    "    r = random.random()\n",
    "    s = 0.0\n",
    "    for word, value in lm_counter[context].items():\n",
    "        s += value\n",
    "        if s > r:\n",
    "            return word\n",
    "\n",
    "In [169]:\n",
    "\n",
    "def generate_sentence(lm, start):    \n",
    "    context = start\n",
    "    sentence = []\n",
    "    text = ''\n",
    "\n",
    "    while (True):\n",
    "        word = generate_word(lm, context)\n",
    "        if word == None: \n",
    "            break\n",
    "        else:\n",
    "            text += word\n",
    "            temp = list(context)[1:]\n",
    "            temp.append(word)\n",
    "            context = tuple(temp) \n",
    "            \n",
    "    sentence.append(text.encode('utf-8'))\n",
    "    return sentence\n",
    "\n",
    "In [170]:\n",
    "\n",
    "def generate_sample_text(lm, n, sentence_count, padding):\n",
    "    heads = generate_head(n, padding)\n",
    "    start = tuple(heads)\n",
    "    text = ''\n",
    "    for i in range(sentence_count):\n",
    "        sentences = generate_sentence(lm, start)\n",
    "        sentences.append('\\r\\n')\n",
    "        text += ''.join(sentences)\n",
    "    return text\n",
    "\n",
    "In [171]:\n",
    "\n",
    "def demo_model_from_file(file_name, max_n):\n",
    "    segs = Load_File(file_name)\n",
    "    padding = u'%'\n",
    "    \n",
    "    if max_n > 5:\n",
    "        max_n = 5\n",
    "    else:\n",
    "        if max_n < 1:\n",
    "            max_n = 1\n",
    "            \n",
    "    for n in range(1,max_n):\n",
    "        print '-----------ngram = %d-----------' % (n)\n",
    "        lm = create_model(segs, n, padding)\n",
    "        text = generate_sample_text(lm, n, 10, padding)\n",
    "        print text\n",
    "        print '--------------------------------'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
