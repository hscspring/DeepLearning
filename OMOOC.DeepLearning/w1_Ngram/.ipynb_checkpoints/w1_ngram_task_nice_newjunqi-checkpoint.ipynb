{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.313 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** start 2_gram ******************\n",
      "看了铺盖因犯对那猴道行隆，把帽儿戴一顶毗卢方帽，你若听见道：雕鞍彩凤双鸣；抛弹子，就变做个四四方方一块大唐所管，早已不见了他怎么不来谢那长老只得依言，一个徒弟啊，牢记心头火，似这般看着金字，径奔前来，也有一孔窍相通，我也不认得他。”老者道：“放心，若是地裂山崩。”众猴高叫道：“\n",
      "***************** end 2_gram ******************\n",
      "***************** start 3_gram ******************\n",
      "三藏依言，跨了刬马，怎生骑得？且待寻船渡过涧去，鸥鹭相忘没钓逢。师徒们正走多时，怯战而走，拜了四方。目运两道金光，而两间人物俱无矣，故曰混沌。又撺于水内，深潜涧底中间，左右倒有芝兰相衬。盖自开辟以来，每受天真地秀，日精月华，感之既久，遂称美猴王。上前来扯住唐僧道：“这股水不知\n",
      "***************** end 3_gram ******************\n",
      "***************** start 4_gram ******************\n",
      "菩萨道：“如何不是？”行者笑道：“也正合我们的宗派。你这个模样，就象那小头陀一般，我再赠你一般本事。”菩萨闻言道：“徒弟啊，却怎生寻得马着么？”悟空道：“我往东洋大海老龙王家讨茶吃吃。”三藏道：“弟子屡感菩萨圣恩，未及叩谢。今遇禅院，就如见菩萨一般，甚好拜谢。”那老者道\n",
      "***************** end 4_gram ******************\n",
      "***************** start 5_gram ******************\n",
      "水。我们今日赶闲无事，顺涧边往上溜头寻看源流，耍子去耶！”喊一声，都拖男挈女，唤弟呼兄，一齐跑来，顺涧爬山，直至源流之处，乃是一股瀑布飞泉。但见那——翠藓堆蓝，白云浮玉，光摇片片烟霞。虚窗静室，滑凳板生花。乳窟龙珠倚挂，萦回满地奇葩。锅灶傍崖存火迹，樽 靠案见肴渣。石座石床真可爱，石盆\n",
      "***************** end 5_gram ******************\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "读取语料 生成 n-gram 模型\n",
    "\n",
    "基本思路：\n",
    "\n",
    "从词的角度来说，词语存在着固定的搭配，例如输入法中，当输入了某个字，后面提示的就是这个字的常用搭配\n",
    "而搭配总是前后相关，即前面出现的词决定了后面的词出现的频率。\n",
    "因此，采用根据概率采样的方式，由前面的词决定后面的词出现的概率\n",
    "\n",
    "\"\"\"\n",
    "import jieba.posseg as pseg\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "lm = defaultdict(Counter)\n",
    "lmPrepareN = Counter()\n",
    "\n",
    "\"\"\"读取文件内容\"\"\"\n",
    "def getContent():\n",
    "    lineNo = 0 \n",
    "    lineLimit = 1000\n",
    "    wordList = []\n",
    "    ignoreList = '\\n'  # 忽略不计的符号\n",
    "    for line in open('../AssisantEvaluate/西游记.txt'):\n",
    "        lineNo += 1\n",
    "        if lineNo > lineLimit:\n",
    "            continue\n",
    "        words = pseg.cut(line.strip())\n",
    "        for word,tx in words:\n",
    "            if word.encode(\"utf-8\") in ignoreList:\n",
    "                continue;\n",
    "            wordList.append(word.encode(\"utf-8\"))\n",
    "    return wordList\n",
    "     \n",
    "        \n",
    "\"\"\"生成N-gram后面的部分\"\"\"    \n",
    "def getGram(ngram,wordList):\n",
    "    #统计gram频率\n",
    "    for i in range(ngram, len(wordList)):\n",
    "        context = ''.join(wordList[(i-ngram):i])\n",
    "        word = wordList[i]\n",
    "        lm[context][word]+=1\n",
    "    #计算gram的概率\n",
    "    for context,wordCnts in lm.items():  \n",
    "        s = float(sum(wordCnts.values()))\n",
    "        for word,cnt in wordCnts.items():\n",
    "            lm[context][word] = cnt/s     \n",
    "        \n",
    "        \n",
    "def gerenWord(context):\n",
    "    rand = random.random()\n",
    "    s = 0.0\n",
    "   \n",
    "    for key,cnt in lm[context].items():\n",
    "        s+=cnt\n",
    "        if s >= rand:\n",
    "            return key\n",
    "\n",
    "for param in range(2,6):        \n",
    "    nGram=param-1\n",
    "    wordList = getContent()\n",
    "    sentence = []\n",
    "\n",
    "    #随机生成前面的context\n",
    "    randamInt = random.randint(0, len(wordList)-nGram+1)\n",
    "    for i in range(randamInt,randamInt+nGram):\n",
    "        sentence.append(wordList[i]) \n",
    "\n",
    "    #生成N-gram后面的gram\n",
    "    getGram(nGram,wordList)\n",
    "\n",
    "    for i in range(nGram,100):\n",
    "        context = '';\n",
    "        for j in range(nGram):\n",
    "            context += sentence[i-(nGram-j)]\n",
    "        sentence.append(gerenWord(context))\n",
    "    \n",
    "    print(\"***************** start \" + str(param) + \"_gram ******************\")\n",
    "    print(\"\".join(sentence))\n",
    "    print(\"***************** end \" + str(param) + \"_gram ******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
