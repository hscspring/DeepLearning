{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程目标\n",
    "\n",
    "- 如何利用神经网络构建语言模型\n",
    "- 神经网络构建语言模型的优势\n",
    "- 对神经网络学到的模型进行可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络与梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三个偏导数加起来就构成了 三个参数 的梯度  \n",
    "\n",
    "从输出层往回传导数的过程：反向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "纠错：视频1 13:34 $w_{21}x_2$ 应为 $w_{21}x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业1\n",
    "\n",
    "基于矩阵乘法，用 Tensorflow 实现 Lecture 3 最后作业的单隐层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.constant([[1, 1], [2, 2]])\n",
    "x = tf.constant([[1], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(2, 2) dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(2, 1) dtype=int32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(W, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [6]]\n",
      "[[1 1]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))\n",
    "    print(sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的表达能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无隐层神经网络\n",
    "- 单隐层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不加激活函数的单隐层神经网络  \n",
    "  $W'(Wx + b) + b' = (W'W)x + W'b + b'$\n",
    "- [Universal approximation theorem](http://www.wikiwand.com/en/Universal_approximation_theorem)（单隐层）： $|f(x) - y| < \\epsilon$\n",
    "- Why go deep?  层数变多，参数个数变少，效果更好\n",
    "  单层，每个神经元相对独立的\n",
    "  多层，类似函数调用，第一层是非常小（底层）的函数；组件，后面的利用前面的组件；符合人类认知，从已有知识+关系学到更多知识；预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度的好处：结果复用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bengio 的电脑程序类比\n",
    "- [icml09-ConvolutionalDeepBeliefNetworks.pdf](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)\n",
    "- 深度的优势还没有理论上的完全定论。但是实际当中效果确实很好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Overfitting slides11.pdf](http://work.caltech.edu/slides/slides11.pdf)\n",
    "- 过拟合不一定是模型问题，也可能是数据量不够；数据很多时，参数很多也不容易过拟合；数据很少时，参数很少也可能过拟合\n",
    "\n",
    "\n",
    "\n",
    "- 复杂模型拟合了数据当中的噪音\n",
    "- 用什么模型？参数怎么定？\n",
    "- 测试集（Test set）和验证集（Validation set）\n",
    "  - 验证集：确定超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵符号 Revisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "纠错：\n",
    "- 视频3: 00:08 $w_{21}x_2$ 应为 $w_{21}x_1$\n",
    "- 视频3: 06:15 行向量的两个矩阵等式 $w_{12}x_2$ 应为 $w_{12}x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf 假设输入是行向量，每个样本是一行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业2\n",
    "\n",
    "- 基础代码 tf_matrix.ipynb，将列向量表示形式，改成行向量表示形式\n",
    "- 进一步，使用 `tf.layers.dense` 替换原有的矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计语言模型存在的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(w_3|w_1w_2)$\n",
    "  - 数据中有很多的 $p(一|今天，星期), p(二|今天，星期)$\n",
    "  - 数据中没有 $p(一|明天，星期), p(二|明天，星期)$\n",
    "  - 类似于查表\n",
    "  - 数据中是可以知道【今天】【明天】两个词很像\n",
    "  - 怎么判断两个词很像？看他们有没有类似的【上下文】\n",
    "  - 基于统计的 N-gram Language Model 没有办法【迁移】这种知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 要素：模型的输出是一个概率分布，有 N 个词，就要有 N 个输出，还需要加和 = 1\n",
    "- Sigmoid -> Softmax\n",
    "- 要素：输入是上下文（比如前文）\n",
    "- 最简单的做法：词表大小是 N，输入有 N 个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.constant([1., 2, 3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09003057  0.24472848  0.66524094]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.nn.softmax(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denom = math.exp(1) + math.exp(2) + math.exp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09003057317038046"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(1) /denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "纠错：视频4 13:00 和 13:45 是不是重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = tf.random_normal([30000, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 直接告诉它哪些元素是 0 ，这里指定一个元素是 1，其他都是 0\n",
    "# 存储空间很小，只存储非 0 元素对应的位置 和 值\n",
    "# 和稠密矩阵或另一个稀疏矩阵做乘法时，有一个特殊的乘法，可以优化计算，把计算量优化到很小\n",
    "x = tf.SparseTensor(indices=[[0, 0]], values=[1.0], dense_shape=[2000, 30000]) \n",
    "x_dense = tf.random_normal([2000, 30000]) # 稠密矩阵，大部分元素不是 0, Sparse tensor 大部分元素都是 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse tensor * dense tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00494313240051\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    # 假设 session 用 with，可以直接调用 eval 表示得到值，run graph 中必要的部分得到值\n",
    "    tf.sparse_tensor_dense_matmul(x, matrix).eval()\n",
    "    t1 = time.time()\n",
    "    print(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense tensor * Dense tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14146113396\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    # 假设 session 用 with，可以直接调用 eval 表示得到值，run graph 中必要的部分得到值\n",
    "    tf.matmul(x_dense, matrix).eval()\n",
    "    t1 = time.time()\n",
    "    print(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding lookup (矩阵取行)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业3\n",
    "\n",
    "为什么取这个矩阵的某一行，和这个词出现的时候做矩阵乘法，结果是一样的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix1 = tf.random_normal([30000, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50270391  0.51018757  0.24039407 -1.09312904  0.4385021  -0.75943923\n",
      "  -1.26064861 -0.77937162]\n",
      " [-0.10846059  1.3035562   1.51551282  0.67917299 -0.70449454 -0.84641021\n",
      "  -1.13484204  1.20230627]]\n",
      "--------\n",
      "0.0138111114502\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    t0 = time.time()\n",
    "    # 取矩阵的某一行或者某几行，matrix 就是 权重矩阵，第二个参数是 list，[0, 1] 表示 第 0 和 第 1 行\n",
    "    print(tf.nn.embedding_lookup(matrix1, [0, 1]).eval())\n",
    "    t1 = time.time()\n",
    "    print(\"--------\")\n",
    "    print(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么 NNLM 可以 迁移 知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 所有的样本进行全局优化\n",
    "- 【今天】【明天】在很相似的上下文里面，他们的词向量也会比较像\n",
    "- 【今天】有而【明天】没有的一些上下文，【明天】自然可以 Get 到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.818182px",
    "width": "1.818182px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
