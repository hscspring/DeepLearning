{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业1\n",
    "\n",
    "测试你的 Language Model 在 batch_size = 1 VS batch_size = 125 时，预测完一定数量样本所需要的时间比（模型算出哪个词概率最高即可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 2.7.6\n",
      "IPython 5.1.0\n",
      "\n",
      "tensorflow 1.0.1\n",
      "numpy 1.12.1\n",
      "\n",
      "compiler   : GCC 4.8.4\n",
      "system     : Linux\n",
      "release    : 4.4.0-21-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "# 示例代码运行环境\n",
    "%load_ext watermark\n",
    "%watermark -p tensorflow,numpy -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import jieba\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IGNORE = ' \\n' # 忽略的字符\n",
    "SEN_LENGTH = 20 # 句子预设长度\n",
    "#PADDING = '<PD>' # 句子长度不足时的占位符\n",
    "LIMIT = 3 # 构建词表时，选择词汇的最低频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutset(s):\n",
    "    words = []\n",
    "    for w,f in pseg.cut(s.strip()):\n",
    "        if f == 'x' and f == 'm' and (w in IGNORE):\n",
    "            continue\n",
    "        words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        dataset = []\n",
    "        rawdata = f.read().decode('utf-8')\n",
    "        data = rawdata.split('\\n')\n",
    "        for w in data:\n",
    "            dataset.append(w.split('\\t'))\n",
    "    \n",
    "        sensetcut = []\n",
    "        for i in range(len(dataset)-1):\n",
    "            tmp = [cutset(dataset[i][0]), int(dataset[i][1])]\n",
    "            sensetcut.append(tmp)\n",
    "    \n",
    "    return sensetcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(sensetcut):\n",
    "    words = []\n",
    "    for s in sensetcut:\n",
    "        for w in s[0]:\n",
    "            words.append(w)\n",
    "    word_cnt = Counter(words)\n",
    "    #vocab = [i[0] for i in word_cnt.most_common(vocab_size - 1)] # 采用固定长度 vocab_size，也可以采用 word_cnt 不低于某值\n",
    "    vocab = [i[0] for i in word_cnt.most_common() if (i[1] > LIMIT) ]\n",
    "    vocab.insert(0, 'UNK')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(sensetcut, vocab):\n",
    "\n",
    "    train_ids = []\n",
    "    label_ids = []\n",
    "    inputs = np.zeros(len(sensetcut)*SEN_LENGTH).reshape(len(sensetcut), SEN_LENGTH)\n",
    "    labels = np.zeros(len(sensetcut))\n",
    "    for i in range(len(sensetcut)):\n",
    "        if len(sensetcut[i][0]) < 1:\n",
    "            continue\n",
    "        tmp1 = np.array([[vocab.index(word) if (word in vocab) else 0 for word in sensetcut[i][0]]])\n",
    "        if tmp1.shape[1] < SEN_LENGTH:\n",
    "            # 也可以用 extand：docs[i].extend([PADDING] * (DOC_LENGTH - len(docs[i])))\n",
    "            tmp2 = np.array([np.zeros(SEN_LENGTH-tmp1.shape[1])])\n",
    "            tmp = np.hstack((tmp1, tmp2))\n",
    "        else:\n",
    "            tmp = tmp1[0, 0:SEN_LENGTH]\n",
    "        inputs[i] = tmp\n",
    "        labels[i] = sensetcut[i][1]\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.364 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "sensetcut_train = read_file('../w5_CNN/train_shuffle.txt')\n",
    "sensetcut_test = read_file('../w5_CNN/test_shuffle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_train = get_vocab(sensetcut_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_train, labels_train = get_data(sensetcut_train, vocab_train)\n",
    "inputs_test, labels_test = get_data(sensetcut_test, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24586, 20), (24586,), (10538, 20), (10538,))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train.shape, labels_train.shape, inputs_test.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels_train = labels_train.reshape(len(labels_train),1)\n",
    "#labels_test = labels_test.reshape(len(labels_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_train)\n",
    "word_embed_size = 32\n",
    "filter_num = 64\n",
    "window_size = 3\n",
    "num_fc_hidden = 10\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, word_embed_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, shape=[None, SEN_LENGTH], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeds = tf.nn.embedding_lookup(W, inputs)\n",
    "embeds_expand = tf.expand_dims(embeds, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_pool\n",
    "with tf.name_scope(\"conv-maxpool\"):\n",
    "\n",
    "    filter_shape = [window_size, word_embed_size, 1, filter_num]\n",
    "    # W 和 b 是卷积的参数\n",
    "    W = tf.Variable(tf.random_uniform(filter_shape, -1.0, 1.0), name=\"W\")\n",
    "    # bias 和 filter_num 个数是一样的\n",
    "    b = tf.Variable(tf.constant(0.0, shape=[filter_num]), name=\"b\")\n",
    "    # 步长为1，这里不做 Padding，因此句子太短的话可能要丢掉\n",
    "    # 原始语料已经做了 Padding\n",
    "    conv = tf.nn.conv2d(\n",
    "                    embeds_expand,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "    # 卷积出来的结果加上 bias\n",
    "    conv_hidden = tf.nn.tanh(tf.add(conv, b), name=\"tanh\")\n",
    "\n",
    "    # 因为没有 padding，出来的结果个数是 sequence_length - window_size + 1，如果加了 padding 这里要对应更改。\n",
    "    pool = tf.nn.max_pool(\n",
    "                    conv_hidden,\n",
    "                    ksize=[1, SEN_LENGTH - window_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = tf.layers.dense(pool, num_fc_hidden, activation=tf.nn.tanh)\n",
    "\n",
    "squeezed_pool = tf.squeeze(fc, [1, 2]) \n",
    "raw_output = tf.layers.dense(squeezed_pool, num_labels)\n",
    "\n",
    "#raw_output = tf.layers.dense(fc, num_labels, name='output')\n",
    "output = tf.nn.softmax(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(sess, inputs_, labels_, print_matrix=False):\n",
    "    \"\"\"评估模型指标，并打印输出\"\"\"\n",
    "    pred_prob = sess.run(output, feed_dict={inputs:inputs_, labels:labels_})\n",
    "    preds = np.asarray((pred_prob[:, 1] > 0.5), dtype=int)\n",
    "    mat = sess.run(tf.confusion_matrix(labels_, preds))\n",
    "    tn, fp, fn,  tp = mat.reshape(4)\n",
    "    precision = np.float(tp) / (tp + fp)\n",
    "    recall = np.float(tp) / (tp + fn)\n",
    "    if print_matrix:\n",
    "        print('confusion matrix:\\n', mat)\n",
    "    print('precision:% .3f, recall: %.3f' %(precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost: train: 0.797152 / test: 0.796957\n",
      "precision: 0.471, recall: 0.999\n",
      "Epoch 125 cost: train: 0.033436 / test: 0.588153\n",
      "precision: 0.893, recall: 0.884\n",
      "Epoch 250 cost: train: 0.0218082 / test: 0.570705\n",
      "precision: 0.905, recall: 0.880\n",
      "Epoch 375 cost: train: 0.00498927 / test: 0.740818\n",
      "precision: 0.914, recall: 0.885\n",
      "Epoch 500 cost: train: 0.00424016 / test: 0.811011\n",
      "precision: 0.907, recall: 0.888\n",
      "Epoch 625 cost: train: 0.00401673 / test: 0.857259\n",
      "precision: 0.900, recall: 0.892\n",
      "Epoch 750 cost: train: 0.00396796 / test: 0.8719\n",
      "precision: 0.903, recall: 0.892\n",
      "Epoch 875 cost: train: 0.00394944 / test: 0.891875\n",
      "precision: 0.904, recall: 0.892\n",
      "Epoch 1000 cost: train: 0.00393985 / test: 0.903336\n",
      "precision: 0.903, recall: 0.893\n",
      "Epoch 1125 cost: train: 0.00393795 / test: 0.910647\n",
      "precision: 0.902, recall: 0.894\n",
      "Epoch 1250 cost: train: 0.00393274 / test: 0.924075\n",
      "precision: 0.902, recall: 0.895\n",
      "Epoch 1375 cost: train: 0.0039272 / test: 0.937047\n",
      "precision: 0.902, recall: 0.896\n",
      "Epoch 1500 cost: train: 0.00392352 / test: 0.949012\n",
      "precision: 0.902, recall: 0.897\n",
      "Epoch 1625 cost: train: 0.0039201 / test: 0.959992\n",
      "precision: 0.901, recall: 0.896\n",
      "Epoch 1750 cost: train: 0.00391751 / test: 0.968987\n",
      "precision: 0.901, recall: 0.897\n",
      "Epoch 1875 cost: train: 0.00391417 / test: 0.975907\n",
      "precision: 0.902, recall: 0.895\n",
      "Epoch 2000 cost: train: 0.00391247 / test: 0.982344\n",
      "precision: 0.902, recall: 0.895\n",
      "Epoch 2125 cost: train: 0.00391097 / test: 0.987869\n",
      "precision: 0.901, recall: 0.895\n",
      "Epoch 2250 cost: train: 0.0039105 / test: 0.991654\n",
      "precision: 0.901, recall: 0.895\n",
      "Epoch 2375 cost: train: 0.00390953 / test: 0.994956\n",
      "precision: 0.901, recall: 0.896\n",
      "Epoch 2500 cost: train: 0.00390721 / test: 0.998184\n",
      "precision: 0.903, recall: 0.895\n",
      "Epoch 2625 cost: train: 0.00389923 / test: 0.99953\n",
      "precision: 0.903, recall: 0.894\n",
      "Epoch 2750 cost: train: 0.00389859 / test: 1.0021\n",
      "precision: 0.904, recall: 0.894\n",
      "Epoch 2875 cost: train: 0.00389854 / test: 1.00497\n",
      "precision: 0.904, recall: 0.893\n",
      "\n",
      "time: 4448.59768105\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "batch_size = 125\n",
    "epochs = 3000\n",
    "print_cost_every = 125\n",
    "\n",
    "feed_train = {inputs: inputs_train, labels: labels_train}\n",
    "feed_test = {inputs: inputs_test, labels: labels_test}\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "costs_train = []\n",
    "costs_test = []\n",
    "start_time = time.time()\n",
    "\n",
    "num_inputs = len(labels_train)\n",
    "order = np.arange(num_inputs)\n",
    "np.random.shuffle(order)\n",
    "\n",
    "try:\n",
    "    for i in range(epochs):\n",
    "        if i % print_cost_every == 0:\n",
    "            cost_train = sess.run(cost, feed_dict=feed_train)\n",
    "            cost_test = sess.run(cost, feed_dict=feed_test)\n",
    "            print('Epoch %d cost: train: %s / test: %s' %(i, cost_train, cost_test))\n",
    "            costs_train.append(cost_train)\n",
    "            costs_test.append(cost_test)\n",
    "            evaluate_model(sess, inputs_test, labels_test)\n",
    "        for j in range(0, num_inputs, batch_size):\n",
    "            batch_index = order[j: j + batch_size]\n",
    "            batch_inputs = inputs_train[batch_index]\n",
    "            batch_labels = labels_train[batch_index]\n",
    "            batch_feed = {inputs: batch_inputs, labels: batch_labels}\n",
    "            sess.run(train_step, feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print('\\ntime: %s' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost: train: 0.872736 / test: 0.872346\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 100 cost: train: 5.7949 / test: 5.79462\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 200 cost: train: 5.79602 / test: 5.79575\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 300 cost: train: 5.79667 / test: 5.79619\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 400 cost: train: 5.79682 / test: 5.79655\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 500 cost: train: 5.79685 / test: 5.79665\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 600 cost: train: 5.79685 / test: 5.79668\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 700 cost: train: 5.79685 / test: 5.79669\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 800 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 900 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1000 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1100 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1200 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1300 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1400 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1500 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1600 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1700 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1800 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 1900 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2000 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2100 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2200 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2300 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2400 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2500 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n",
      "Epoch 2600 cost: train: 5.79725 / test: 5.79719\n",
      "precision: 0.471, recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "batch_size = 1\n",
    "epochs = 3000\n",
    "print_cost_every = 100\n",
    "\n",
    "feed_train = {inputs: inputs_train, labels: labels_train}\n",
    "feed_test = {inputs: inputs_test, labels: labels_test}\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "costs_train = []\n",
    "costs_test = []\n",
    "start_time = time.time()\n",
    "\n",
    "num_inputs = len(labels_train)\n",
    "order = np.arange(num_inputs)\n",
    "np.random.shuffle(order)\n",
    "\n",
    "try:\n",
    "    for i in range(epochs):\n",
    "        if i % print_cost_every == 0:\n",
    "            cost_train = sess.run(cost, feed_dict=feed_train)\n",
    "            cost_test = sess.run(cost, feed_dict=feed_test)\n",
    "            print('Epoch %d cost: train: %s / test: %s' %(i, cost_train, cost_test))\n",
    "            costs_train.append(cost_train)\n",
    "            costs_test.append(cost_test)\n",
    "            evaluate_model(sess, inputs_test, labels_test)\n",
    "        for j in range(0, num_inputs, batch_size):\n",
    "            batch_index = order[j: j + batch_size]\n",
    "            batch_inputs = inputs_train[batch_index]\n",
    "            batch_labels = labels_train[batch_index]\n",
    "            batch_feed = {inputs: batch_inputs, labels: batch_labels}\n",
    "            sess.run(train_step, feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print('\\ntime: %s' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nce_loss() takes at least 6 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-0870d17d086d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: nce_loss() takes at least 6 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.nce_loss(raw_output, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "batch_size = 125\n",
    "epochs = 3000\n",
    "print_cost_every = 125\n",
    "\n",
    "feed_train = {inputs: inputs_train, labels: labels_train}\n",
    "feed_test = {inputs: inputs_test, labels: labels_test}\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "costs_train = []\n",
    "costs_test = []\n",
    "start_time = time.time()\n",
    "\n",
    "num_inputs = len(labels_train)\n",
    "order = np.arange(num_inputs)\n",
    "np.random.shuffle(order)\n",
    "\n",
    "try:\n",
    "    for i in range(epochs):\n",
    "        if i % print_cost_every == 0:\n",
    "            cost_train = sess.run(cost, feed_dict=feed_train)\n",
    "            cost_test = sess.run(cost, feed_dict=feed_test)\n",
    "            print('Epoch %d cost: train: %s / test: %s' %(i, cost_train, cost_test))\n",
    "            costs_train.append(cost_train)\n",
    "            costs_test.append(cost_test)\n",
    "            evaluate_model(sess, inputs_test, labels_test)\n",
    "        for j in range(0, num_inputs, batch_size):\n",
    "            batch_index = order[j: j + batch_size]\n",
    "            batch_inputs = inputs_train[batch_index]\n",
    "            batch_labels = labels_train[batch_index]\n",
    "            batch_feed = {inputs: batch_inputs, labels: batch_labels}\n",
    "            sess.run(train_step, feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print('\\ntime: %s' % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
