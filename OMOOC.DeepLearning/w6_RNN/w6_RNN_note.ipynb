{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#循环神经网络\" data-toc-modified-id=\"循环神经网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>循环神经网络 </a></div><div class=\"lev2 toc-item\"><a href=\"#Softmax-与交叉熵\" data-toc-modified-id=\"Softmax-与交叉熵-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Softmax 与交叉熵 </a></div><div class=\"lev3 toc-item\"><a href=\"#Sigmoid\" data-toc-modified-id=\"Sigmoid-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Sigmoid</a></div><div class=\"lev2 toc-item\"><a href=\"#2-class-Softmax\" data-toc-modified-id=\"2-class-Softmax-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>2-class Softmax</a></div><div class=\"lev3 toc-item\"><a href=\"#k-class-Softmax\" data-toc-modified-id=\"k-class-Softmax-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>k-class Softmax</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-Embedding-+-Softmax-计算量\" data-toc-modified-id=\"Word-Embedding-+-Softmax-计算量-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word Embedding + Softmax 计算量 </a></div><div class=\"lev3 toc-item\"><a href=\"#FLOPs\" data-toc-modified-id=\"FLOPs-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>FLOPs</a></div><div class=\"lev3 toc-item\"><a href=\"#FLOPs-Code\" data-toc-modified-id=\"FLOPs-Code-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>FLOPs Code</a></div><div class=\"lev3 toc-item\"><a href=\"#评估/小结\" data-toc-modified-id=\"评估/小结-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>评估 / 小结 </a></div><div class=\"lev3 toc-item\"><a href=\"#作业1\" data-toc-modified-id=\"作业1-134\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>作业 1</a></div><div class=\"lev3 toc-item\"><a href=\"#Softmax-优化：降低-k-和-V\" data-toc-modified-id=\"Softmax-优化：降低-k-和-V-135\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Softmax 优化：降低 k 和 V</a></div><div class=\"lev4 toc-item\"><a href=\"#纠错\" data-toc-modified-id=\"纠错-1351\"><span class=\"toc-item-num\">1.3.5.1&nbsp;&nbsp;</span>纠错 </a></div><div class=\"lev3 toc-item\"><a href=\"#模型方面的优化\" data-toc-modified-id=\"模型方面的优化-136\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>模型方面的优化 </a></div><div class=\"lev4 toc-item\"><a href=\"#Softmax-优化：Hierarchical-Softmax\" data-toc-modified-id=\"Softmax-优化：Hierarchical-Softmax-1361\"><span class=\"toc-item-num\">1.3.6.1&nbsp;&nbsp;</span>Softmax 优化：Hierarchical Softmax</a></div><div class=\"lev4 toc-item\"><a href=\"#Softmax-优化：Noise-contrastive-estimation-(NCE)\" data-toc-modified-id=\"Softmax-优化：Noise-contrastive-estimation-(NCE)-1362\"><span class=\"toc-item-num\">1.3.6.2&nbsp;&nbsp;</span>Softmax 优化：Noise-contrastive estimation (NCE)</a></div><div class=\"lev3 toc-item\"><a href=\"#作业2\" data-toc-modified-id=\"作业2-137\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>作业 2</a></div><div class=\"lev4 toc-item\"><a href=\"#纠错\" data-toc-modified-id=\"纠错-1371\"><span class=\"toc-item-num\">1.3.7.1&nbsp;&nbsp;</span>纠错 </a></div><div class=\"lev2 toc-item\"><a href=\"#循环神经网络\" data-toc-modified-id=\"循环神经网络-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>循环神经网络 </a></div><div class=\"lev3 toc-item\"><a href=\"#词向量的几种用法\" data-toc-modified-id=\"词向量的几种用法-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>词向量的几种用法 </a></div><div class=\"lev3 toc-item\"><a href=\"#如何阅读\" data-toc-modified-id=\"如何阅读-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>如何阅读 </a></div><div class=\"lev3 toc-item\"><a href=\"#建模\" data-toc-modified-id=\"建模-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>建模 </a></div><div class=\"lev3 toc-item\"><a href=\"#RNN-Demo-(Numpy)\" data-toc-modified-id=\"RNN-Demo-(Numpy)-144\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>RNN Demo (Numpy)</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN-Demo-(Tensorflow)\" data-toc-modified-id=\"RNN-Demo-(Tensorflow)-145\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>RNN Demo (Tensorflow)</a></div><div class=\"lev3 toc-item\"><a href=\"#分类-VS-Language-Model\" data-toc-modified-id=\"分类-VS-Language-Model-146\"><span class=\"toc-item-num\">1.4.6&nbsp;&nbsp;</span>分类 VS Language Model</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN-Demo（处理变长数据）\" data-toc-modified-id=\"RNN-Demo（处理变长数据）-147\"><span class=\"toc-item-num\">1.4.7&nbsp;&nbsp;</span>RNN Demo（处理变长数据）</a></div><div class=\"lev4 toc-item\"><a href=\"#损失函数的解决\" data-toc-modified-id=\"损失函数的解决-1471\"><span class=\"toc-item-num\">1.4.7.1&nbsp;&nbsp;</span>损失函数的解决 </a></div><div class=\"lev4 toc-item\"><a href=\"#纠错\" data-toc-modified-id=\"纠错-1472\"><span class=\"toc-item-num\">1.4.7.2&nbsp;&nbsp;</span>纠错 </a></div><div class=\"lev3 toc-item\"><a href=\"#作业3\" data-toc-modified-id=\"作业3-148\"><span class=\"toc-item-num\">1.4.8&nbsp;&nbsp;</span>作业 3</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 与交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络语言模型\n",
    "  - 用固定长度的上文预测下一个词\n",
    "  - CNN：处理变化长度的句子，利用卷积神经网络的卷积层 +　maxpooling 变成固定长度的向量\n",
    "  - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Sigmoid + CE(Cross Entropy): C = -\\frac {1}{n} \\sum_{x}[yloga + (1-y)log(1-a)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n 个样本\n",
    "- x 加和，x 假设每个样本用 x 表示\n",
    "- y 是 label\n",
    "- a 是经过 Sigmoid 变换后的概率值（预测值），代表样本属于 1（正例）的概率；1-a 相反\n",
    "- Sigmoid 只有一个输出 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-class Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ 2-class-Softmax + CE: C = -\\frac {1}{n} \\sum_{x}[y_1loga_1 + y_2loga_2] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a1 + a2 = 1\n",
    "- y1, y2 对应的 label，一个是 1，另一个是 0\n",
    "- 概率接近 1 时，越来越接近 0 ，取负号就是正数，但很小，cost 也很小；相反 cost 很大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-class Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每个样本只有一个 label：  \n",
    "  $ Softmax(k-class) + CE: C = -\\frac {1}{n} \\sum_{x} \\sum_{k} [y_kloga_k] $\n",
    "\n",
    "- 每个样本只有一个类别：\n",
    "  - 只有一个 yk 是 1 其他都是 0\n",
    "  - 对 cost 没有任何贡献\n",
    "  - $ Softmax(k-class) + CE: C = -\\frac {1}{n} \\sum_{x} [y_{k=true}loga_{k=true}] $\n",
    "\n",
    "- 每个样本给的是几个 label 上的概率分布：\n",
    "  - y 变成概率值\n",
    "  - `tf.nn.softmax_cross_entropy_with_logits` y 是概率分布\n",
    "  - `tf.nn.sparse_softmax_cross_entropy_with_logits` sparse 意思是只有一个 label（二分类或者多分类），计算量更小，是上一个的特殊情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding + Softmax 计算量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k：词向量维度\n",
    "- V：词表大小\n",
    "- 计算量：k × V 次浮点数乘法\n",
    "- Forward + Backprop\n",
    "- FLOPS 的概念：每秒做多少次浮点数计算  \n",
    "  - CPU Frequency: 10^9 GHz, 10^6 MHz, 10^3 kHz\n",
    "  - FLOPs per cycle \\* Cores \\* CPU Frequency: 2 \\* 4 \\* GHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPs Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "vocab_size = 80000\n",
    "word_embed = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导数第二层和最后一层\n",
    "# 1 * 128\n",
    "inputs = np.random.rand(batch_size, word_embed)\n",
    "# 128 * 80000\n",
    "weights = np.random.rand(word_embed, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00918579816818\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(100):\n",
    "    inputs.dot(weights)\n",
    "end = time.time()\n",
    "print(end - start) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 8.44 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit inputs.dot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137777777.777778"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均每秒可以做多少次浮点数乘法\n",
    "128 * 80000 / 0.009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论值：2.20GHz × 4 × 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06464646464646466, 0.5171717171717173)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实际/理论\n",
    "1137777777.777778 / (2.2*(10**9) * 4 * 2), 1137777777.777778 / (2.2*(10**9) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估/小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以估算模型时间下限（至少要多少时间）\n",
    "- batch_size 增大，矩阵乘法会让耗时较小，但是性能红利随着矩阵 size 增大会递减（边际递减）\n",
    "  - so，minibatch 是更好的选择：矩阵乘以矩阵性能红利（样本增加，需要消耗的时间不是等比例增加），而且算的更准\n",
    "  - SGD：向量乘以矩阵\n",
    "  - 集群：样本并行\n",
    "      - 整体 batch_size 1000，10 台机器，每个机器**每次** 100 个样本\n",
    "      - GPU FLOPs 非常高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业1\n",
    "\n",
    "测试你的 Language Model 在 batch_size = 1 VS batch_size = 125 时，预测完一定数量样本所需要的时间比（模型算出哪个词概率最高即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax 优化：降低 k 和 V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出节点多的 Softmax 在 NLP 当中应用广泛\n",
    "- 降低 k：深度 vs 宽度。和数据有关（问题简单降低也不会影响）\n",
    "- 降低 V：字级别模型\n",
    "  - 纯字级别：对应的 context 要拉长，k 也要增大\n",
    "  - 字词混合：高频词用词级别，低频变成字级别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 纠错\n",
    "\n",
    "- 视频2: 13:36 纯自己别 --> 纯字级别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型方面的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax 优化：Hierarchical Softmax\n",
    "\n",
    "\n",
    "\n",
    "- 改进训练，没法改进【生成】\n",
    "  - 只在最后用\n",
    "  - 二叉树里，每个分支是一个 128 维的向量\n",
    "  - 高频 label 走的路劲比较短\n",
    "- 在 Google word2vec 中使用\n",
    "  - 只需要拿到词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax 优化：Noise-contrastive estimation (NCE)\n",
    "\n",
    "- 对负例进行采样，转化成多个二分类问题\n",
    "  - k 个负样本，k+1 个二分类问题\n",
    "  - Eg  \n",
    "      今天 星期 | 天  1  \n",
    "      今天 星期 | 中国 0   \n",
    "      今天 星期 | 厦门 0  \n",
    "      今天 星期 | 哈哈 0  \n",
    "- 理论上，实质损失函数和 Softmax 不一样了\n",
    "  - k 负样本逐渐增加时，能够比较逼近 Softmax 损失函数\n",
    "  - Mnih and Teh (2012): 25 个负样本，表现和 softmax 接近，提速 45 倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业2\n",
    "\n",
    "使用一种方法改善 Lecture 4 中实现的神经网络语言模型的性能，并比较相对 Softmax 的训练性能提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 纠错\n",
    "\n",
    "视频2: 25:51 Noise-constranstive --> Noise-contranstive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量的几种用法  \n",
    "\n",
    "- 求平均：我喜欢你 vs 你喜欢我 | 北京到广州 vs 广州到北京\n",
    "  - 对语义损害比较大\n",
    "  - 因为都是用词向量处理语言模型、情感分类。求平均后无法区分，那模型相应也就没办法区分\n",
    "- 卷积 + Pooling\n",
    "  - 理论上可以区分两个相似的句子\n",
    "- 循环神经网络（Recurrent neural network）\n",
    "  - 可以处理变长的句子\n",
    "  - 也可以区分相似的句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何阅读\n",
    "\n",
    "一般来说是从左往后阅读一段文字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模\n",
    "\n",
    "- 模型的参数始终是同一组，只是 input 和 state 的变化\n",
    "- 最基本的 RNN\n",
    "- $new\\_state = activation(W * input + U * state + B)$  \n",
    "  - input 和 上一个 state 加 bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Demo (Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assume input and state are 128-length vector\n",
    "class RNNCell:\n",
    "    def __init__(self, input_size, state_size):\n",
    "        self.W = np.random.rand(input_size, state_size)\n",
    "        self.U = np.random.rand(state_size, state_size)\n",
    "        self.b = np.zeros((1, state_size))\n",
    "    def forward(self, input_, state):\n",
    "        return input_.dot(self.W) + state.dot(self.U) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNCell(128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个样本\n",
    "input0 = np.random.rand(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "state0 = np.zeros((1, 128)) # 最初始的状态，全设为 0\n",
    "state1 = rnn.forward(input0, state0)\n",
    "print(state1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "input1 = np.random.rand(1, 128)\n",
    "state2 = rnn.forward(input1, state1)\n",
    "print(state2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整个过程中，W, U, b 是没有变化的，在初始化中已经生成好了（假定是训练好的模型）\n",
    "- 处理变长的句子，如情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Demo (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "word_embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicRNNCell at 0x7f89ac433310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.asarray(list(chunks(range(30), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]\n",
      " [21 22 23]\n",
      " [24 25 26]\n",
      " [27 28 29]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0, 1, 2] 这个句子  \n",
    "P(1|0) 以 0 为上下文预测下个词的分类问题  \n",
    "P(2|(0, 1)) 0 1 作为上下文预测 2 这个词  \n",
    "\n",
    "- RNN 情感分类只有最后一个 state 后面接了 Sigmoid 或者 Softmax\n",
    "- RNN 做 Language Model，标准做法是每个 state 都会接个共享的 Softmax（当然中间还可以加其他隐层）。每个状态都有个自己的分类问题，预测这个状态下，下一个词是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variables.Variable at 0x7f89a3722710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))\n",
    "word_embedding\n",
    "# 100 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(10, 2, 128) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds = tf.nn.embedding_lookup(word_embedding, data[:, [0, 1]])\n",
    "input_embeds\n",
    "# 取第 0 1 列，前两个词的 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'rnn/transpose:0' shape=(10, 2, 128) dtype=float32>, <tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# 这里 states 就是 output\n",
    "# 也可以做更复杂的模型\n",
    "# 本例中，前进了两步，会有两个输出，就是两个 state\n",
    "# final state 和第二个 output 一样的\n",
    "output, states = tf.nn.dynamic_rnn(cell, input_embeds, dtype=tf.float32)\n",
    "print(output, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow 也可以 reshape\n",
    "output_flat = tf.reshape(output, (-1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 128)\n",
      "(10, 2, 128)\n",
      "(20, 128)\n",
      "(10, 128)\n",
      "[[ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]\n",
      " ..., \n",
      " [ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_val = output.eval()\n",
    "    states_val = states.eval()\n",
    "    \n",
    "    print(output_flat.eval().shape) # 20*128\n",
    "    print(output_val.shape) # 10*2*128\n",
    "    # -1 让它自己算维度，也可以直接指定 20\n",
    "    # 必须保证 reshape 后的形状和原来一样\n",
    "    # 128 这个维度是不变的\n",
    "    print(output_val.reshape((-1, 128)).shape) # 20*128\n",
    "    \n",
    "    print(states_val.shape) # 10*128\n",
    "    \n",
    "    print(output_val[:,1,:] == states_val)\n",
    "    # 确定是不是每一个都是 True\n",
    "    print((output_val[:,1,:] == states_val).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output_shape: 10 个样本，在 2 个 state 里各自的输出是怎样的\n",
    "  - 之所以是 128 维是因为设置：`cell = tf.contrib.rnn.BasicRNNCell(128)`\n",
    "- state_shape: evaluate 后的 value，只保存最后一个状态\n",
    "- output: 取第二个状态（0 表示第一个，1 表示第二个），其他维度不变，看和最终的 state 是否一样\n",
    "  - 根据前面所说，肯定一样，最后一个 output 和 state 是一样的\n",
    "  - 并不是所有的 cell 都满足这样的假设，有的 output 和 state 是由不同的操作来完成，结果也会不太一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类 VS Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 假设做情感分类，目前长度都是 2（假设取 0 1 列），走了两步后，只要用最后的 state（10×128）作为后面正常前馈神经网络后续输入，再把对应的 label 和 cost 函数 对应的给它处理，就可以顺利完成分类问题\n",
    "  - `labels = [[1, 2], [4, 5], [7, 8]]` 打平成 `labels_flat = [1, 2, 4, 5, 7, 8]` 就可以和 output 打平的结果对应\n",
    "- Language Model，两个 output 都需要被用到\n",
    "  - 10 个样本对应 20 个分类问题，每个样本有两个分类问题\n",
    "  - 三维的 Tensor（10×2×128）转换成二维的（20×128) 20 个分类问题，就可以处理（Softmax）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Demo（处理变长数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 2, 3], [1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 三个样本，句子长度分别是 2 3 5\n",
    "data = [[1, 2], [1, 2, 3], [1, 2, 3, 4, 5]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0, 0],\n",
       "       [1, 2, 3, 0, 0],\n",
       "       [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding，补 0\n",
    "data_padding= np.asarray([[1, 2, 0, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 4, 5]])\n",
    "data_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding 的问题：\n",
    "\n",
    "- 因为后面有 0，RNN 进化到一个【不对】的状态\n",
    "- 最长的句子可能很长，影响性能\n",
    "- 损失函数没法安排，多了很多【假】label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "word_embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "word_embedding = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1:0' shape=(3, 4, 128) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去掉了最后一个词，对网络来说，不需要最后一个词\n",
    "# 最后一个词作为 label 训练的\n",
    "input_embeds = tf.nn.embedding_lookup(word_embedding, data_padding[:, 0:4])\n",
    "input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 0]\n",
      " [1 2 3 0]\n",
      " [1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(data_padding[:, 0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 虽然给的数据是 padding 过的，但是 sequence_length 可以自动停止进化\n",
    "# 它超过长度后，state 就一直复制上一个结果，output 就置为 0\n",
    "# 只是简单复制，对性能影响也很小\n",
    "output, states = tf.nn.dynamic_rnn(cell, input_embeds, dtype=tf.float32, sequence_length=[2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加了 sequence_length 参数是一个 Tensor（可以随着 batch 的数据变化）  \n",
    "- 对每个样本，超过其对应 sequence_length 之后，dynamic_rnn 不会实际去做 state 计算，而是直接复制上一个 state，超长的对应 output 则设置为 0 向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数的解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的解释我们知道可以让 dynamic_rnn 在超长之后不做实际计算，这时候超长的部分 output 会输出 0  \n",
    "但是这些 output 仍然会和 label（非零部分才是真正的 label）进行计算，产生 cost，进而影响模型的变量  \n",
    "所以要把 0 的部分的 label 在 cost 函数中去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [2, 3, 0, 0],\n",
       "       [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding[:, 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是我们这个 RNN Language Model 对应的 labels，我们如何让 0 的部分不计入损失呢？  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(12,) dtype=int64>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 在用之前会用 reshape 打平成固定长度的行向量\n",
    "labels = tf.reshape(data_padding[:, 1:5], [-1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sign_2:0' shape=(12,) dtype=int64>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >0 返回 1，否则 0\n",
    "mask = tf.sign(labels)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = tf.sign([-2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(mask.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把原来的 cost 和 mask 函数相乘，结果做 reduce_mean，作为新的 cost 给优化器进行优化，就把不想要的 label 置为 0，即可忽略这些无用的 labels，不会影响最后的 cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 纠错\n",
    "\n",
    "视频4: 6:20 mask 大于 0 返回 1 不是大于 0 返回 0 --> mask 大于 0 返回 1，小于 0 返回 -1, 0 返回 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业3\n",
    "\n",
    "- 使用 RNN 构建语言模型，并进行生成，观察生成的效果\n",
    "- 尝试学习并使用复杂度更高的 RNNCell，如 GRUCell，LSTMCell，并观察使用之后模型生成的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.818182px",
    "width": "1.818182px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
