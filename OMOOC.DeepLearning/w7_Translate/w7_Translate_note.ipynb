{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2 toc-item\"><a href=\"#学习心得\" data-toc-modified-id=\"学习心得-01\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>学习心得</a></div><div class=\"lev1 toc-item\"><a href=\"#神经网络翻译模型\" data-toc-modified-id=\"神经网络翻译模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>神经网络翻译模型</a></div><div class=\"lev2 toc-item\"><a href=\"#梯度消散（Gradient-Vanish）问题\" data-toc-modified-id=\"梯度消散（Gradient-Vanish）问题-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>梯度消散（Gradient Vanish）问题</a></div><div class=\"lev2 toc-item\"><a href=\"#翻译问题\" data-toc-modified-id=\"翻译问题-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>翻译问题</a></div><div class=\"lev3 toc-item\"><a href=\"#传统机器翻译（IBM-Model）\" data-toc-modified-id=\"传统机器翻译（IBM-Model）-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>传统机器翻译（IBM Model）</a></div><div class=\"lev3 toc-item\"><a href=\"#神经机器翻译\" data-toc-modified-id=\"神经机器翻译-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>神经机器翻译</a></div><div class=\"lev3 toc-item\"><a href=\"#神经机器翻译理论模型\" data-toc-modified-id=\"神经机器翻译理论模型-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>神经机器翻译理论模型</a></div><div class=\"lev2 toc-item\"><a href=\"#Tensorflow-seq2seq\" data-toc-modified-id=\"Tensorflow-seq2seq-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Tensorflow seq2seq</a></div><div class=\"lev2 toc-item\"><a href=\"#代码学习\" data-toc-modified-id=\"代码学习-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>代码学习</a></div><div class=\"lev2 toc-item\"><a href=\"#作业\" data-toc-modified-id=\"作业-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>作业</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习心得\n",
    "\n",
    "\n",
    "- 理解模型的基本结构和数学原理，掌握CNN，RNN，Word Embedding， Gradient descent等基本原理，对后续的学习非常有帮助，否则会知其然不知其所以然。配合阅读一些论文效果会更好，目前我看的论文数量还不多，后面有时间要多看论文。开课前读过Michael Nielsen的书[Neural Networks and Deep Learning](Neural Networks and Deep Learning), 很有帮助。  \n",
    "\n",
    "- 掌握tensorflow对应API的使用，重点是理解各个参数的数据维度。学习方法是看官方API文档，同时结合别人写的例子代码来看，如果对参数结构不明白的，最好运行例子代码，把每个参数的shape打印出来，会加快理解。  \n",
    "\n",
    "- 准备训练数据，第一步是要原始数据做预处理，然后按照模型API的要求生成训练数据集。这部分代码工作量不小，不过慢慢可以积累一些可以复用的代码，可参考tensorflow官方教程中的一些数据处理代码。  \n",
    "\n",
    "- 训练模型，需要尝试调整超参数集，估计训练时间，尽快实现模型收敛。对训练时间长的模型，训练结果要保存下来，以后可以继续训练或用来做预测，否则下次又要重头开始训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络翻译模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消散（Gradient Vanish）问题  \n",
    "\n",
    "- 为什么早期不直接用深层神经网络？ \n",
    "  - 数据不够\n",
    "  - 计算资源不够\n",
    "  - 梯度消散问题\n",
    "- ReLU\n",
    "  - 梯度（导数）要么是 0 要么是 1\n",
    "  - 对于小于 0 的部分，置为了 0：Sparse activation；可能更符合生物神经网络特性，低于一定阈值就不传输了；后面的矩阵更稀疏也可能正好是我们需要的\n",
    "- 小于 4 层 5 层，tanh 足够了；不是绝对的\n",
    "- 如果层数加深收敛的不够好，可以使用 ReLU\n",
    "- 或者层数不多但希望得到稀疏激活的情况，也可以使用 ReLU\n",
    "- 在计算量允许的情况下，不断加深层数，直到验证集错误率不再改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source -> Target\n",
    "  - 中文 -> 英文\n",
    "  - 微博 -> 微博评论\n",
    "  - 邮件 -> 回复\n",
    "  - 命令 -> （助理的）回复\n",
    "\n",
    "- Target 会根据 Source 的内容做一些反应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统机器翻译（IBM Model）  \n",
    "\n",
    "- 平行语料，词对齐  \n",
    "\n",
    "\n",
    "### 神经机器翻译\n",
    "\n",
    "- 和人翻译有些类似\n",
    "- 过完一句英文，拿到一整句的词向量，根据词向量再翻译出每个词\n",
    "  - 英文 -> 句子向量：Encode，编码器\n",
    "  - 句子向量 -> Target：Decode，解码器\n",
    "- Sequence to Sequence (seq2seq)\n",
    "  - 假设模型已经训练好\n",
    "  - 用最后状态的输出作为 ContextVector（包含隐藏的语义）\n",
    "  - 一个 RNN Cell 接收三个输入：State，ContextVector，上一个时间的词向量（刚开始可以给个 start token）\n",
    "  - 输出一个 Output，接 Softmax，得出 y1\n",
    "  - 进入下一个 Cell，接收：State，ContextVector，y1\n",
    "  - 只看 y2 的问题时，y1 的 softmax 是没有用的，但是 y1 本身的问题也要考虑，所以有多个分类问题综合考虑，一起产生梯度更新参数的过程\n",
    "  - 直到遇到 end token（softmax 中 end 的概率最大时）\n",
    "- 参数复用，会有好几个梯度，但都算到同一个 RNN 矩阵上\n",
    "  - 每个 RNN 拿到的梯度不一样，但最后都更新到同一个矩阵上\n",
    "\n",
    "\n",
    "### 神经机器翻译理论模型\n",
    "\n",
    "- $P(y_1 | C)$\n",
    "- $P(y_2 | y_1, C)$\n",
    "- $P(y_3 | y_1, y_2, C)$\n",
    "- $P(y_1, y_2,...,y_n | C) = \\prod_i P(y_i | y_1, y_2,...,y_{i-1}, C)$\n",
    "\n",
    "Language Model + Context  \n",
    "\n",
    "[官方文档](http://www.tensorflow.org/tutorials/seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10:58 有间断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.legacy_seq2seq import basic_rnn_seq2seq, embedding_rnn_seq2seq, sequence_loss\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    '<GO>': 6,\n",
    "    '<EOS>': 7\n",
    "}\n",
    "\n",
    "reverse_vocab = dict([(v, k) for (k, v) in vocab.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: '<GO>', 7: '<EOS>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<EOS>': 7, '<GO>': 6, 'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda _:[vocab[_]], ['A','B','C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['A', 'B', 'C'])\n",
    "decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'D', 'E', 'F', '<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Const:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_1:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_2:0' shape=(1,) dtype=int32>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Const_3:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_4:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_5:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_6:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_7:0' shape=(1,) dtype=int32>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "num_encoder_symbols = 8\n",
    "num_decoder_symbols = 8\n",
    "embedding_size = 128\n",
    "\n",
    "outputs, states = embedding_rnn_seq2seq(\n",
    "    encoder_inputs, decoder_inputs, cell, \n",
    "    num_encoder_symbols, num_decoder_symbols, \n",
    "    embedding_size, output_projection=None, feed_previous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = map(lambda _: tf.constant([_], dtype=tf.float32), [1, 1, 1, 1, 1])\n",
    "loss = sequence_loss(outputs, decoder_inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Const_8:0' shape=(1,) dtype=float32>,\n",
       " <tf.Tensor 'Const_9:0' shape=(1,) dtype=float32>,\n",
       " <tf.Tensor 'Const_10:0' shape=(1,) dtype=float32>,\n",
       " <tf.Tensor 'Const_11:0' shape=(1,) dtype=float32>,\n",
       " <tf.Tensor 'Const_12:0' shape=(1,) dtype=float32>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequence_loss/truediv:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "F\n",
      "F\n",
      "<EOS>\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     for iteration in range(50):\n",
    "#         sess.run(train_step)\n",
    "#         print(sess.run(loss))\n",
    "    \n",
    "    # Decoding\n",
    "    with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "        decode_decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'A', 'A', 'A', 'A'])\n",
    "        outputs, states = embedding_rnn_seq2seq(\n",
    "            encoder_inputs, decode_decoder_inputs, cell,\n",
    "            num_encoder_symbols, num_decoder_symbols,\n",
    "            embedding_size, output_projection=None,\n",
    "            feed_previous=True)\n",
    "        \n",
    "        for o in outputs:\n",
    "            m = np.argmax(o.eval(), axis=1)\n",
    "            print(reverse_vocab[m[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Const_13:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_14:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_15:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_16:0' shape=(1,) dtype=int32>,\n",
       " <tf.Tensor 'Const_17:0' shape=(1,) dtype=int32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85723\n",
      "1.64927\n",
      "1.47348\n",
      "1.32612\n",
      "1.20205\n",
      "1.09635\n",
      "1.00502\n",
      "0.92502\n",
      "0.854147\n",
      "0.790796\n",
      "0.733791\n",
      "0.682248\n",
      "0.635484\n",
      "0.592952\n",
      "0.554203\n",
      "0.518852\n",
      "0.486567\n",
      "0.457053\n",
      "0.430047\n",
      "0.405312\n",
      "0.382632\n",
      "0.361814\n",
      "0.342682\n",
      "0.325075\n",
      "0.308851\n",
      "0.29388\n",
      "0.280044\n",
      "0.267239\n",
      "0.255369\n",
      "0.24435\n",
      "0.234104\n",
      "0.224563\n",
      "0.215664\n",
      "0.207353\n",
      "0.199579\n",
      "0.192297\n",
      "0.185466\n",
      "0.179049\n",
      "0.173013\n",
      "0.167327\n",
      "0.161966\n",
      "0.156904\n",
      "0.152118\n",
      "0.147588\n",
      "0.143296\n",
      "0.139224\n",
      "0.135358\n",
      "0.131682\n",
      "0.128185\n",
      "0.124854\n",
      "<GO>\n",
      "D\n",
      "E\n",
      "F\n",
      "<EOS>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iteration in range(50):\n",
    "        sess.run(train_step)\n",
    "        print(sess.run(loss))\n",
    "    \n",
    "    # Decoding\n",
    "    # embedding 内建，reuse 是复用之前训练好的 embedding，需要在同一个 session 里面\n",
    "    with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "        decode_decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'A', 'A', 'A', 'A'])\n",
    "        outputs, states = embedding_rnn_seq2seq(\n",
    "            encoder_inputs, decode_decoder_inputs, cell,\n",
    "            num_encoder_symbols, num_decoder_symbols,\n",
    "            embedding_size, output_projection=None,\n",
    "            feed_previous=True)\n",
    "        \n",
    "        for o in outputs:\n",
    "            m = np.argmax(o.eval(), axis=1)\n",
    "            print(reverse_vocab[m[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65117\n",
      "1.47759\n",
      "1.32882\n",
      "1.20154\n",
      "1.09193\n",
      "0.996611\n",
      "0.912944\n",
      "0.838945\n",
      "0.77311\n",
      "0.714281\n",
      "0.66153\n",
      "0.614101\n",
      "0.571357\n",
      "0.532755\n",
      "0.497828\n",
      "0.466168\n",
      "0.437418\n",
      "0.411262\n",
      "0.387424\n",
      "0.365659\n",
      "0.34575\n",
      "0.327505\n",
      "0.310754\n",
      "0.295346\n",
      "0.281149\n",
      "0.268042\n",
      "0.25592\n",
      "0.24469\n",
      "0.234268\n",
      "0.224578\n",
      "0.215556\n",
      "0.20714\n",
      "0.199278\n",
      "0.191921\n",
      "0.185027\n",
      "0.178558\n",
      "0.172478\n",
      "0.166757\n",
      "0.161365\n",
      "0.156278\n",
      "0.151472\n",
      "0.146926\n",
      "0.142621\n",
      "0.138539\n",
      "0.134665\n",
      "0.130984\n",
      "0.127484\n",
      "0.124151\n",
      "0.120974\n",
      "0.117945\n",
      "<GO>\n",
      "D\n",
      "E\n",
      "F\n",
      "<EOS>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq import basic_rnn_seq2seq, embedding_rnn_seq2seq, sequence_loss\n",
    "from tensorflow.python.ops import variable_scope\n",
    "\n",
    "vocab = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    '<GO>': 6,\n",
    "    '<EOS>': 7\n",
    "}\n",
    "\n",
    "reverse_vocab = dict([(v, k) for (k, v) in vocab.iteritems()])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "encoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['A', 'B', 'C'])\n",
    "decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'D', 'E', 'F', '<EOS>'])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "num_encoder_symbols = 8\n",
    "num_decoder_symbols = 8\n",
    "embedding_size = 128\n",
    "\n",
    "outputs, states = embedding_rnn_seq2seq(\n",
    "    encoder_inputs, decoder_inputs, cell, \n",
    "    num_encoder_symbols, num_decoder_symbols, \n",
    "    embedding_size, output_projection=None, feed_previous=False)\n",
    "\n",
    "weights = map(lambda _: tf.constant([_], dtype=tf.float32), [1, 1, 1, 1, 1])\n",
    "loss = sequence_loss(outputs, decoder_inputs, weights)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iteration in range(50):\n",
    "        sess.run(train_step)\n",
    "        print(sess.run(loss))\n",
    "        #print(sess.run(outputs))\n",
    "    for o in outputs:\n",
    "        m = np.argmax(o.eval(), axis=1)\n",
    "        print(reverse_vocab[m[0]])\n",
    "    \n",
    "    # Decoding\n",
    "    # embedding 内建，reuse 是复用之前训练好的 embedding，需要在同一个 session 里面\n",
    "#     with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "#         decode_decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'A', 'A', 'A', 'A'])\n",
    "#         outputs, states = embedding_rnn_seq2seq(\n",
    "#             encoder_inputs, decode_decoder_inputs, cell,\n",
    "#             num_encoder_symbols, num_decoder_symbols,\n",
    "#             embedding_size, output_projection=None,\n",
    "#             feed_previous=True)\n",
    "        \n",
    "#         for o in outputs:\n",
    "#             m = np.argmax(o.eval(), axis=1)\n",
    "#             print(reverse_vocab[m[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<EOS>': 7, '<GO>': 6, 'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: '<GO>', 7: '<EOS>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda _: ([vocab[_]]), ['<GO>', 'A', 'A', 'A', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66484\n",
      "1.49299\n",
      "1.3445\n",
      "1.21622\n",
      "1.10478\n",
      "1.00727\n",
      "0.921296\n",
      "0.845033\n",
      "0.777043\n",
      "0.716196\n",
      "0.661582\n",
      "0.612454\n",
      "0.56818\n",
      "0.528222\n",
      "0.492107\n",
      "0.459424\n",
      "0.429807\n",
      "0.402929\n",
      "0.378501\n",
      "0.356266\n",
      "0.335993\n",
      "0.317476\n",
      "0.300534\n",
      "0.285005\n",
      "0.270743\n",
      "0.257621\n",
      "0.245526\n",
      "0.234355\n",
      "0.22402\n",
      "0.214439\n",
      "0.205543\n",
      "0.197267\n",
      "0.189555\n",
      "0.182357\n",
      "0.175627\n",
      "0.169325\n",
      "0.163414\n",
      "0.157862\n",
      "0.15264\n",
      "0.147722\n",
      "0.143082\n",
      "0.138701\n",
      "0.134557\n",
      "0.130634\n",
      "0.126916\n",
      "0.123387\n",
      "0.120035\n",
      "0.116847\n",
      "0.113811\n",
      "0.110919\n",
      "<GO>\n",
      "D\n",
      "E\n",
      "F\n",
      "<EOS>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq import basic_rnn_seq2seq, embedding_rnn_seq2seq, sequence_loss\n",
    "from tensorflow.python.ops import variable_scope\n",
    "\n",
    "vocab = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    '<GO>': 6,\n",
    "    '<EOS>': 7\n",
    "}\n",
    "\n",
    "reverse_vocab = dict([(v, k) for (k, v) in vocab.iteritems()])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "encoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['A', 'B', 'C'])\n",
    "decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'D', 'E', 'F', '<EOS>'])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "num_encoder_symbols = 8\n",
    "num_decoder_symbols = 8\n",
    "embedding_size = 128\n",
    "\n",
    "outputs, states = embedding_rnn_seq2seq(\n",
    "    encoder_inputs, decoder_inputs, cell, \n",
    "    num_encoder_symbols, num_decoder_symbols, \n",
    "    embedding_size, output_projection=None, feed_previous=False)\n",
    "\n",
    "weights = map(lambda _: tf.constant([_], dtype=tf.float32), [1, 1, 1, 1, 1])\n",
    "loss = sequence_loss(outputs, decoder_inputs, weights)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iteration in range(50):\n",
    "        sess.run(train_step)\n",
    "        print(sess.run(loss))\n",
    "\n",
    "    # Decoding\n",
    "    # embedding 内建，reuse 是复用之前训练好的 embedding，需要在同一个 session 里面\n",
    "    with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "        decode_decoder_inputs = map(lambda _: tf.constant([vocab[_]]), ['<GO>', 'A', 'A', 'A', 'A'])\n",
    "        outputs, states = embedding_rnn_seq2seq(\n",
    "            encoder_inputs, decode_decoder_inputs, cell,\n",
    "            num_encoder_symbols, num_decoder_symbols,\n",
    "            embedding_size, output_projection=None,\n",
    "            feed_previous=True)\n",
    "        \n",
    "        for o in outputs:\n",
    "            m = np.argmax(o.eval(), axis=1)\n",
    "            print(reverse_vocab[m[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow 中的 seq2seq（简化版本）\n",
    "  - state 用 ContextVector 做初始化\n",
    "  - input 不变（词向量）\n",
    "  - encoder 和 decoder RNN 是同一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "- 阅读 Tensorflow seq2seq 文档\n",
    "- 阅读 Tensorflow 文档中 variable_scope 的部分，理解其语义及作用\n",
    "- 阅读 Tensorflow 中 seq2seq.py 中 embedding_rnn_seq2seq, sequence_loss 相关代码\n",
    "- 基于课程框架，使用给定平行语料（训练、测试）完成翻译模型，产生翻译结果\n",
    "  - 进阶：学习并使用 embedding_attention_seq2seq 相关代码，配合 LSTMCell 或 GRUCell 改善模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.818182px",
    "width": "1.818182px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "557px",
    "left": "0px",
    "right": "994.375px",
    "top": "34px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
