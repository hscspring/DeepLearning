{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#从-0-到-1/2-之-RNN\" data-toc-modified-id=\"从-0-到-1/2-之-RNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>从 0 到 1/2 之 RNN</a></div><div class=\"lev2 toc-item\"><a href=\"#原理简介\" data-toc-modified-id=\"原理简介-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>原理简介</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN\" data-toc-modified-id=\"RNN-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>RNN</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev3 toc-item\"><a href=\"#笔记\" data-toc-modified-id=\"笔记-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>笔记</a></div><div class=\"lev4 toc-item\"><a href=\"#Recurrent-Neural-Networks-Tutorial-–-WildML\" data-toc-modified-id=\"Recurrent-Neural-Networks-Tutorial-–-WildML-1131\"><span class=\"toc-item-num\">1.1.3.1&nbsp;&nbsp;</span><a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\" target=\"_blank\">Recurrent Neural Networks Tutorial – WildML</a></a></div><div class=\"lev4 toc-item\"><a href=\"#The-Unreasonable-Effectiveness-of-Recurrent-Neural-Networks\" data-toc-modified-id=\"The-Unreasonable-Effectiveness-of-Recurrent-Neural-Networks-1132\"><span class=\"toc-item-num\">1.1.3.2&nbsp;&nbsp;</span><a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\">The Unreasonable Effectiveness of Recurrent Neural Networks</a></a></div><div class=\"lev4 toc-item\"><a href=\"#经典论文:-A-Critical-Review-of-Recurrent-Neural-Networks-for-Sequence-Learning\" data-toc-modified-id=\"经典论文:-A-Critical-Review-of-Recurrent-Neural-Networks-for-Sequence-Learning-1133\"><span class=\"toc-item-num\">1.1.3.3&nbsp;&nbsp;</span>经典论文: <a href=\"https://arxiv.org/abs/1506.00019\" target=\"_blank\">A Critical Review of Recurrent Neural Networks for Sequence Learning</a></a></div><div class=\"lev4 toc-item\"><a href=\"#Understanding-LSTM-Networks----colah's-blog\" data-toc-modified-id=\"Understanding-LSTM-Networks----colah's-blog-1134\"><span class=\"toc-item-num\">1.1.3.4&nbsp;&nbsp;</span><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">Understanding LSTM Networks -- colah's blog</a></a></div><div class=\"lev4 toc-item\"><a href=\"#Simple-LSTM\" data-toc-modified-id=\"Simple-LSTM-1135\"><span class=\"toc-item-num\">1.1.3.5&nbsp;&nbsp;</span><a href=\"http://nicodjimenez.github.io/2014/08/08/lstm.html\" target=\"_blank\">Simple LSTM</a></a></div><div class=\"lev2 toc-item\"><a href=\"#代码步骤\" data-toc-modified-id=\"代码步骤-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>代码步骤</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN（单-Cell）\" data-toc-modified-id=\"RNN（单-Cell）-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>RNN（单 Cell）</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev2 toc-item\"><a href=\"#代码解析\" data-toc-modified-id=\"代码解析-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>代码解析</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN\" data-toc-modified-id=\"RNN-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>RNN</a></div><div class=\"lev4 toc-item\"><a href=\"#RNN-Numpy\" data-toc-modified-id=\"RNN-Numpy-1311\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>RNN Numpy</a></div><div class=\"lev4 toc-item\"><a href=\"#RNN-Tensorflow\" data-toc-modified-id=\"RNN-Tensorflow-1312\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>RNN Tensorflow</a></div><div class=\"lev3 toc-item\"><a href=\"#BPTT\" data-toc-modified-id=\"BPTT-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>BPTT</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>LSTM</a></div><div class=\"lev4 toc-item\"><a href=\"#LSTM-Numpy\" data-toc-modified-id=\"LSTM-Numpy-1331\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>LSTM Numpy</a></div><div class=\"lev4 toc-item\"><a href=\"#LSTM-Tensorflow\" data-toc-modified-id=\"LSTM-Tensorflow-1332\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>LSTM Tensorflow</a></div><div class=\"lev2 toc-item\"><a href=\"#小结\" data-toc-modified-id=\"小结-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>小结</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">说明：主要是自己的笔记，方便自己平时看和用，如果有任何疑问可以联系本人。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从 0 到 1/2 之 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习 RNN 有一段时间了，期间也学习了各式各样的 LSTM，但总是觉得不得要领。所以，决定搞一下源码，整理下笔记，边写边理解。  \n",
    "希望能帮到和我一样一脸懵逼的同学。我相信，如果我懂了，大家都能懂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理简介\n",
    "\n",
    "网上资料太多，而且介绍也非常详细完善，就不重复造轮子了。大致概括一下：  \n",
    "\n",
    "### RNN \n",
    "\n",
    "- 核心在于隐层是自循环的，其实把隐层看成是自己不停迭代自己就行了\n",
    "- 输入输出：NLP 中为一个序列\n",
    "  - 如词级别的，语料为：“我 非常 喜欢 你 ！”，输入为：“我 非常 喜欢 你”，输出为：“非常 喜欢 你 ！” \n",
    "  - 输出不仅依赖输入，还依赖中间的其他词，具体来说 “非常” 依赖于 “我”，“喜欢” 依赖于 “我 非常”，以此类推，每个输出的词都依赖于之前的输入，在 RNN 中通过递归来跟踪\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "- 在 RNN 的基础上，隐层迭代时丢掉一些信息同时新增加一些信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笔记\n",
    "\n",
    "非常棒的资料\n",
    "\n",
    "\n",
    "#### [Recurrent Neural Networks Tutorial – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns)\n",
    "\n",
    "- 前谷歌大脑工程师：[WildML – AI, Deep Learning, NLP](http://www.wildml.com/)\n",
    "- 发布时间：2015 年 9 月\n",
    "- 【系列】博客，各类资源汇总，基本可以从零开始看大牛的教程\n",
    "- 英文写的很流畅（简单）\n",
    "\n",
    "##### [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "\n",
    "- 训练序列的长度就是 RNN 循环的次数\n",
    "- RNN 最重要的就是隐层的状态，能够获取序列信息\n",
    "\n",
    "##### [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)\n",
    "\n",
    "- 按字或按词分开\n",
    "- **去除低频词，设为 UNK**。生成时如果碰到 UNK，就从 词表外随机选一个词或者直到生成没有 UNK 的句子为止\n",
    "- 句首句尾添加 开始 和 结束 TOKEN\n",
    "- 构建训练集矩阵，注意不同的目的构建方法不同。如果是生成模型，则：输入[0, 179, 341, 416]，输出[179, 341, 416, 1]，0 和 1 分别表示开始和结束的 TOKEN，其他数字表示一个词或字\n",
    "- **预处理思路可以学习**\n",
    "    - `reader = csv.reader(f, skipinitialspace=True), reader.next()`\n",
    "    - `sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])`\n",
    "    - 句子`sentences = [\"%s %s %s\" % (start_token, x, end_token) for x in sentences]`\n",
    "    - 分词：`tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]`\n",
    "    - 词频：`word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))`\n",
    "    - 词表：`word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])`\n",
    "    - 词-数字：`train = [word_to_index.get(w, UNK_ID) for w in sentence]`\n",
    "    - 训练标签`y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])`\n",
    "- $s_t = tanh(Ux_t + Ws_{t-1})$ \n",
    "- $o_t = softmax(Vs_t)$\n",
    "- 维度理解：\n",
    "    - $x_t, o_t$, (vocab_size, 1)\n",
    "    - $U$, (hidden_size, vocab_size)\n",
    "    - $V$, (vocab_size, hidden_size)\n",
    "    - $W$, (hidden_size, hidden_size)\n",
    "    - (hidden_size, vocab_size) \\* (vocab_size, 1) = (hidden_size, 1)\n",
    "    - (vocab_size, hidden_size) \\* (hidden_size, 1) = (vocab_size, 1)\n",
    "- 参数规模计算\n",
    "    - **$2HC + H^2$**, C: vocab_size, H: hidden_size\n",
    "    - $x_t$ 是 One-hot，所以 $Ux_t$ 相当与选择 U 的一列，最大的运算在 $Vs_t$\n",
    "    - **词表大小影响计算速度和资源消耗，尽可能保持小的词表**\n",
    "- 初始化参数\n",
    "    - **初始化 W 矩阵：$[\\frac{-1}{\\sqrt{n}}, \\frac {1}{\\sqrt{n}}]$，n 表示上一层正面临的连接数，也就是列数**\n",
    "    - 如：`np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))`\n",
    "- 代码思路清晰，同时有不少取巧，如 correct_word_predictions\n",
    "- 注意 loss 的计算，每个 求和\n",
    "- BPTT\n",
    "    - `np.outer`: 向量(m,)(n,)，返回(m,n)矩阵，n 的第一个元素乘以 m 为的每个元素为 第一列\n",
    "- Gradient Checking\n",
    "    - `operator.attrgetter(pname)(self)`:  返回的是 self.pname，如 self.U，其实就是对应的 U 矩阵\n",
    "    - ` np.nditer`: 多维数组迭代，遍历每个元素\n",
    "- SGD（非常赞的代码，包括评估 loss 和调整 learning_rate）\n",
    "    - Step1: A function sdg_step that calculates the gradients and performs the updates for one batch. \n",
    "    - Step2: An outer loop that iterates through the training set and adjusts the learning rate.\n",
    "    - `sys.stdout.flush`: Python's standard out is buffered (meaning that it collects some of the data \"written\" to standard out before it writes it to the terminal). Calling sys.stdout.flush() forces it to \"flush\" the buffer, meaning that it will write everything in the buffer to the terminal, even if normally it would wait before doing so.\n",
    "- `np.random.multinomial`: 在一定的概率分布下重复实验的次数，返回每个概率下出现的次数，比如 `np.random.multinomial(100, [1/7.]*5 + [2/7.])` 返回 `array([11, 16, 14, 17, 16, 26])`，表示 100 次重复中，每个 ID 出现的次数（第 6 个出现的次数最多：26 次）\n",
    "\n",
    "##### [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients – WildML](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)\n",
    "\n",
    "- 这篇文章算是番外吧，毕竟是大神出品，我们可以借此窥探其思路。主要讲反向传播和梯度爆炸的，文章不长。\n",
    "- 梯度爆炸最早由他发现：[Sepp Hochreiter's Fundamental Deep Learning Problem (1991)](http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html)\n",
    "- 推荐了三个非常好的关于梯度下降的资料\n",
    "  - [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/optimization-2/): 不能太赞\n",
    "  - [Calculus on Computational Graphs: Backpropagation -- colah's blog](http://colah.github.io/posts/2015-08-Backprop/): 这个大神耳熟能详了\n",
    "  - [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html): 那本经典的神经网络入门书\n",
    "  - cs231n 的相关笔记和代码单独整理\n",
    "- 再次强调：一句话（序列）是一个 training example，total error 是这个序列中每个 time step（也就是每个词）的 error 之和\n",
    "- 对每个 training example，把每个时间的梯度加起来作为一个 example 的梯度\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "  - 作者（大牛）：[karpathy’s gists · GitHub](https://gist.github.com/karpathy)\n",
    "  - 发布时间：2015 年 5 月\n",
    "  - 中文翻译：[递归神经网络不可思议的有效性-CSDN.NET](http://www.csdn.net/article/2015-08-28/2825569)\n",
    "  - 有非常简单易懂的 RNN 原理分析\n",
    "  - 涉及到：[RMSProb（自适应学习率）](https://arxiv.org/abs/1502.04390)\n",
    "  - [一个 RNN Demo](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "    - **生成 sample 的代码很好用**\n",
    "  - [一个 LSTM Demo: An efficient, batched LSTM. · GitHub](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n",
    "  - 可视化解释 LSTM 结果\n",
    "  - 一些非常好的综述\n",
    "    - CNN 形成的原始感知器 + RNN glance 策略\n",
    "    - 归纳推理，存储和关注 “模块”\n",
    "      \n",
    "\n",
    "#### 经典论文: [A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/abs/1506.00019)\n",
    "\n",
    "\n",
    "\n",
    "#### [Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- 不解释，Tensorflow 官方文档推荐\n",
    "- 两个不错的翻译版本：\n",
    "    - [[译] 理解 LSTM 网络 - 简书](http://www.jianshu.com/p/9dc9f41f0b29)\n",
    "    - [（译）理解 LSTM 网络 （Understanding LSTM Networks by colah） - 大学之道，在明明德 - 博客频道 - CSDN.NET](http://blog.csdn.net/jerr__y/article/details/58598296)\n",
    "\n",
    "#### [Simple LSTM](http://nicodjimenez.github.io/2014/08/08/lstm.html)\n",
    "- 一个非常小的 LSTM Demo 的解释\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码步骤  \n",
    "\n",
    "详细整理一下代码实现需要的步骤（可以当伪代码看），主要目的是梳理思路和便于记忆。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN（单 Cell）\n",
    "\n",
    "重点是理解隐层的循环，假设只有一个 Cell，循环“序列长度”次，与普通 NN 不同的地方加粗标记。\n",
    "\n",
    "- 初始化及定义\n",
    "  - 定义交叉熵函数及其导函数\n",
    "    - $y=\\frac{1} {1+e^{-x}}$\n",
    "    - $y'(x) = y(1-y)$\n",
    "  - 输入和输出\n",
    "    - 假设 X 是 m 维，每次输入一个训练数据，即 1×m\n",
    "    - 输出 y 是 1×1\n",
    "  - 初始化 W 矩阵\n",
    "    - 假设隐层神经元数量为 n，则输入层 -> 隐层的 W0 矩阵：m×n\n",
    "    - 隐层 -> 输出层的 W1 矩阵：n×1\n",
    "    - **隐层 -> 隐层的 Wh 矩阵：n×n（这个就是 RNN，隐层自迭代一次）**\n",
    "  - **声明 W 矩阵的更新**\n",
    "    - W0_update：m×n\n",
    "    - W1_updata：n×1\n",
    "    - Wh_update：n×n\n",
    "    \n",
    "- Epoch 开始，定义各种变量存储位置\n",
    "  - 保存全局误差 overallError = 0\n",
    "  - 保存更新后的输出层：layer_2_deltas = list()\n",
    "  - 保存隐层的输出：layer_1_values = list()\n",
    "  - **隐层上一个时间点的输出：layer_1_values.append(np.zeros(n))** 这个很重要，相当于第一次循环时隐层的值（这时候还没有正向传播，没有值，所以赋值 0）\n",
    "  - **隐层下一个时间点更新后的输出：future_layer_1_delta = np.zeros(n)**，相当于第一次反向传播时更新后隐层的值（这时候还没有反向传播，没有指，所以赋值 0）\n",
    "\n",
    "- 序列前向传播\n",
    "  - 隐层输出：layer_1 = sigmoid(np.dot(X, W0) + **np.dot(layer_1_values[-1], Wh))**，其实就是普通 NN 的基础上加了隐层 -> 隐层对输出的影响，这个也是 RNN 的核心。sigmoid 是加了激活函数，维度是 1×n\n",
    "  - 输出层：layer_2 = sigmoid(np.dot(layer_1, W1))，维度是 1×1\n",
    "  - 输出层 Error：layer_2_error = y - layer_2\n",
    "  - 总误差：overallError += np.abs(layer_2_error[0])\n",
    "  - 保存隐层输出：layer_1_values.append(copy.deepcopy(layer_1))\n",
    "\n",
    "- 序列反向传播\n",
    "  - 更新后的输出层：layer_2_deltas.append((layer_2_error)·(layer_2)·(1-layer_2)) # 注意，后面两个点表示矩阵对应元素相乘\n",
    "  - 隐层对应的更新后的输出：layer_2_delta = layer_2_deltas[-1]\n",
    "  - **隐层更新后的输出**：  \n",
    "    layer_1_delta = (future_layer_1_delta.dot(Wh.T) +layer_2_delta.dot(W1.T))·(layer_1)·(1-layer_1))\n",
    "\n",
    "- 更新序列 W 矩阵\n",
    "  - 隐层的输出：layer_1 = layer_1_values[-1]\n",
    "  - 隐层上一时点的输出：prev_layer_1 = layer_1_values[-2]，循环一次就有两个输出了，相当于隐层自己迭代了一次\n",
    "  - 隐层 -> 输出层：W1_update += layer_1.T × layer_2_delta\n",
    "  - 隐层 -> 隐层：Wh_update += prev_layer_1.T × layer_1_delta\n",
    "  - 输入层 -> 隐层：W0_update += X.T × layer_1_delta\n",
    "  - 记录下本时序的隐藏层的 delta 用来在下一时序使用：future_layer_1_delta = layer_1_delta\n",
    "\n",
    "- 更新整体 W 矩阵\n",
    "  - W0 += W0_update * alpha\n",
    "  - W1 += W1_update * alpha\n",
    "  - Wh += Wh_update * alpha\n",
    "\n",
    "- Epoch 结束，更新变量归零\n",
    "  - W0_update *= 0\n",
    "  - W1_update *= 0\n",
    "  - Wh_update *= 0\n",
    "\n",
    "总结一下，与普通 NN 最大的区别就是多了隐层：上一时间点的输出，下一时间点更新后的输出，其他其实差不多。  \n",
    "需要注意的是，上面的步骤根据下面的代码总结而成，结合代码看会比较好。当然，重在理解原理，形式可以不那么看重。  \n",
    "如果换做自然语言，则输入为每句话（一组训练数据）的每个字，输出为下一个字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 代码解析\n",
    "\n",
    "具体代码实现，详细解读每一行代码的含义。代码内容如下：  \n",
    "\n",
    "- RNN Numpy 实现\n",
    "- LSTM Numpy 实现\n",
    "- RNN Tensorflow 实现\n",
    "- LSTM Tensorflow 实现\n",
    "- Bi-LSTM Tensorflow 实现\n",
    "\n",
    "### RNN\n",
    "\n",
    "#### RNN Numpy\n",
    "\n",
    "- [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) - i am trask](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "  - sigmoid 激活\n",
    "  - 存储下一个时刻的 delta（输出）\n",
    "  - 输入固定二维（两个二进制数字），输出一维（一个二进制数字）\n",
    "- [Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy · GitHub](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "  - softmax 激活\n",
    "  - 存储下一个时刻的 delta×W 矩阵（Error）\n",
    "  - 防止梯度爆炸\n",
    "  - 字母模型\n",
    "  - 输入输出维度一致（单个字的 One-hot）\n",
    "- [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)\n",
    "  - softmax 激活\n",
    "  - 所有时刻的初始化为一个大矩阵\n",
    "  - 矩阵乘以向量直接写成一个矩阵，代码精简（真的好精简）但不好理解\n",
    "  - 反向传播时，**首次**计算 delta_t 没有考虑隐层下一时间点的输出（其实考虑不考虑都一样，如果考虑就是 全 0）\n",
    "  - 反向传播时，分为两部分；考虑变长的输入变量\n",
    "  - 输入变长序列，输出变长序列\n",
    "  - 词模型\n",
    "\n",
    "#### RNN Tensorflow\n",
    "\n",
    "\n",
    "### BPTT\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "#### LSTM Numpy\n",
    "\n",
    "- [Simple LSTM](http://nicodjimenez.github.io/2014/08/08/lstm.html)\n",
    "    - 可选参数初始化系数\n",
    "    \n",
    "    \n",
    "- [An efficient, batched LSTM. · GitHub](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n",
    "\n",
    "\n",
    "- [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "\n",
    "#### LSTM Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 小结\n",
    "\n",
    "其实很多内容非常值得反复研读和思考，所以一次没看懂没关系，一时半会没看懂也没关系，重要的是保持学习的热情，并坚持下去。共勉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "194px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "616px",
    "left": "0px",
    "right": "20px",
    "top": "34px",
    "width": "326px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
