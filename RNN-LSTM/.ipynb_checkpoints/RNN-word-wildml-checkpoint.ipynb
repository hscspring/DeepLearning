{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 基于词和矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano – WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\r\n",
      "\"I joined a new league this year and they have different scoring rules than I'm used to. It's a slight PPR league- .2 PPR. Standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per TD thrown, and some bonuses for rec/rush/pass yardage. My question is, is it wildly clear that QB has the highest potential for points? I put in the rules at a ranking site and noticed that top QBs had 300 points more than the top RB/WR. Would it be dumb not to grab a QB in the first round?\"\r\n",
      "\"In your scenario, a person could just not run the mandatory background check on the buyer and still sell the gun to the felon. There's no way to enforce it. An honest seller is going to not sell the gun to them when they see they're a felon on the background check. A dishonest seller isn't going to run the check in the first place. No one is going to be honest enough to run the check, see they're a felon, and then all of a sudden immediately turn dishonest and say \"\"nah, you know what, here's your gun anyway.\"\" They wouldn't run the fucking check in the first place, genius. Your bullshit proposal is **NOT ENFORCEABLE**. This is why people without \r\n",
      "\r\n",
      "&gt;Here's an idea, why not make a background check system where it would be illegal to sell guns to felons? That doesn't convince you? IDGAF.\r\n",
      "\r\n",
      "We already fucking have that. What aren't you understanding about this?! It's just currently not available to private sellers. I'd like to make it available, but there's no point in making it mandatory.\r\n",
      "\r\n",
      "&gt;You're just supporting it to make a gesture towards background check reform in bad faith.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/re*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分段解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000 # 词表大小\n",
    "unknown_token = \"UNKNOWN_TOKEN\" # 不在词表中的词\n",
    "sentence_start_token = \"SENTENCE_START\" # 序列（句子）开始标记\n",
    "sentence_end_token = \"SENTENCE_END\" # 结束标记\n",
    "\n",
    "# 读取数据并添加开始和结束标记\n",
    "print \"Reading CSV file...\"\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentencers into word\n",
    "# 把句子切成一个一个词\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all words not in vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79170,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79170,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim # embedding，词表大小\n",
    "        self.hidden_dim = hidden_dim # 隐层神经元数量\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Random initialize the network parameters\n",
    "        # 输入层到隐层的 W 矩阵\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later\n",
    "    # We add one additional element ofr the initial hidden, which we save them for zero\n",
    "    # 把所有隐层状态存储起来，第一列和输入矩阵分别决定下一个输出，以此类推\n",
    "    # 每个时间点输入一个词或字（word or character），也就是一个 One-hot 向量\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    # 第一个词之前 “词” 的输出（其实没有）\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # 最后的输出\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...，也就是训练序列中的每个词（字）\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indexing U by x[t].\n",
    "        # This is the same as multiplying U with One-Hot vector\n",
    "        # U × X[t] 其实就是 U 的第 X[t] 列啦，\n",
    "        # 因为 X[t] 是 (word_dim * 1), U * X[t] 就是：(hidden_dim * 1)，也就是 U 的第 X[t] 列\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1])) # 前一半就等于：U × X[t]，后一半是上一隐层的【输出】\n",
    "        #print s[t].shape\n",
    "        # o[t] 大小为 word_dim * 1，所以 o 的大小为：序列长度 × word_dim（词表大小）\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    # 前向传播预测，选择概率最大的那个（每个词都对应一个 词表大小的概率表）\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # 返回最大值的 位置（ID）\n",
    "    # 返回的矩阵大小为：序列长度 × word_dim（词表大小），每一行代表一个词，返回每一行概率最大的位置，可以对应一个词\n",
    "    # axis=1 表示按列，0 表示按行\n",
    "    return np.argmax(o, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = model.forward_propagation([x_train[10][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(x_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00606684,  0.00944145,  0.00682396, ..., -0.00758421,\n",
       "         0.0014529 , -0.00903755],\n",
       "       [-0.00343015, -0.00192321, -0.00149617, ...,  0.00043284,\n",
       "         0.00885342,  0.00129919],\n",
       "       [ 0.01200128, -0.01433878, -0.00834751, ..., -0.00990714,\n",
       "         0.00733898,  0.00026428],\n",
       "       ..., \n",
       "       [-0.00194572,  0.01046511,  0.0075063 , ..., -0.00564678,\n",
       "         0.00409187,  0.00470521],\n",
       "       [ 0.00294306, -0.00733632,  0.00389336, ...,  0.00577147,\n",
       "        -0.01456043, -0.00221772],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n",
      "students shortly museum ruining background hunt madden wr chicken immoral hadith lighter rude questions achieve but sells making fill arguing purchase grows feat head lube winners downside states steal but researchers christian utilize fire domain resolution 10-15 genuinely magical worship в branches memes node preferred\n"
     ]
    }
   ],
   "source": [
    "# 返回 45 行中最大概率的位置，对应 45 个词\n",
    "predictions = model.predict(x_train[10])\n",
    "print predictions.shape\n",
    "print predictions\n",
    "print \" \".join([index_to_word[i] for i in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "作用：评估误差  \n",
    "使用 cross-entropy loss 交叉熵\n",
    "$$L(y,o) = - \\frac {1} {N} \\sum_{n \\in N} y_nlogo_n$$  \n",
    "可以很容易看出，$y$ 和 $o$ 相差越远，loss 越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    # 每个序列，也就是每个训练样本\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediciton of the \"correct\" words\n",
    "        # 返回 y[i] 位置的概率，一共 len(序列) 个，就是输出 o 的每一行（len(y[i]) 行）的 y[i]（ID） 列\n",
    "        # 每一行都是一个词，而 y[i] 对应的 ID 位置的概率应该是（我们的要求）最大的（One-Hot 时这里是 ID，其他地方是 0）\n",
    "        # 所以返回来的就是 输入序列中每个词对应 y（下个词）的概率值，共有 序列长度 个值，大小为：（序列长度,1）\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        # y 肯定是 1 嘛，也就是 o 要靠近的目标\n",
    "        # 一个序列里的加起来\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training example\n",
    "    # 每个 y 序列的长度\n",
    "    # N 表示训练样本数\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987440\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "# 每个词的概率应该是 1/词表大小 C，所以 Loss 应该等于 -1/N*N*log(1/C)  = logC\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(x_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度反向传播\n",
    "\n",
    "- SGD: Stochastic Gradient Descent\n",
    "- BPTT: BackPropagation Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    # 序列长度，每个训练集的循环次数\n",
    "    T = len(y)\n",
    "    # 前向传播\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # 计算梯度变量\n",
    "    # 更新后的权重矩阵\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    # 与计算 loss 时的 correct_word_predictions = o[np.arange(len(y[i])), y[i]] 是一样的\n",
    "    # 这里的意思是，把每个 y（下个词，标签）的概率 -1\n",
    "    # 因为 softmax 的导数就是这样……\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # for each output backwards\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T) \n",
    "        # Initial delta calculation\n",
    "        # 就是 RNN-add-iamtrask 中的 delta，用 误差 × 导数\n",
    "        # 这里没有考虑隐层下一时间点的输出\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2)) \n",
    "        # Backpropagation through time ( for at most self.bptt_truncate steps)\n",
    "        # self.bptt_truncate 截断，从截断开始到最后时刻的下一时刻\n",
    "        # 主要是处理变长数据，如果是等长可以不需要这一步\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            dLdU[:, x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            # 下一时点的输出\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 8000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00012524,  0.00012446,  0.00012589,  0.00012655,  0.00012448,\n",
       "        0.00012524,  0.0001237 ,  0.00012588,  0.00012445,  0.00012516,\n",
       "        0.00012504,  0.00012446,  0.00012516,  0.00012481,  0.00012476,\n",
       "        0.00012493,  0.00012449,  0.00012591,  0.00012496,  0.00012457,\n",
       "        0.00012577,  0.00012439,  0.00012533,  0.00012561,  0.00012543,\n",
       "        0.00012421,  0.00012472,  0.00012406,  0.00012533,  0.00012504,\n",
       "        0.00012509,  0.00012479,  0.00012569,  0.00012517,  0.00012556,\n",
       "        0.00012576,  0.00012605,  0.00012541,  0.00012539,  0.00012501,\n",
       "        0.00012434,  0.00012582,  0.0001252 ,  0.00012432,  0.0001253 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[np.arange(len(y_train[10])), y_train[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6],\n",
       "       [ 8, 10, 12],\n",
       "       [12, 15, 18]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(np.array([1,2,3]), np.array([4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        # Return a callable object that fetches the given attribute(s) from its operand.\n",
    "        # After f = attrgetter('name'), the call f(r) returns r.name.\n",
    "        # After g = attrgetter('name', 'date'), the call g(r) returns (r.name, r.date).\n",
    "        # After h = attrgetter('name.first', 'name.last'), the call h(r) returns\n",
    "        # (r.name.first, r.name.last).\n",
    "        # operator.attrgetter(pname)(self) 返回的是 self.pname，如 self.U，其实就是对应的 U 矩阵\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        # 测试用，原代码没有\n",
    "        test = operator.attrgetter(pname)\n",
    "        # np.prod 元素乘积，如果有 axis 则按行/列 乘\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g.(0, 0), (0, 1)...，\n",
    "        # 一对对的是矩阵中的位置，多少个元素就多少个位置，readwrite 表示遍历过程中会更改元素值\n",
    "        # parameter 返回的是系数矩阵，\n",
    "        # [Iterating Over Arrays — NumPy v1.13 Manual](https://docs.scipy.org/doc/numpy/reference/arrays.nditer.html)\n",
    "        # [nditer —— numpy.ndarray 多维数组的迭代 - 计算机科学与艺术 - 博客频道 - CSDN.NET](http://blog.csdn.net/lanchunhui/article/details/55657135)\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix] # 就是系数矩阵中的某个位置，一个数字，ix 就是位置，如（0,0）\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x], [y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x], [y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            # 系数矩阵，就是 L 对 $\\theta$ 的导数\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)\\\n",
    "            /(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "    return bptt_gradients, parameter, it, test, self, original_value\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evil_rabbit/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# smaller vocabulary size for checking\n",
    "grad_check_vocab_size = 100 # word dim\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000) # 10 hidden dim\n",
    "bptt_gradients, parameter, it, test, test_self, test_value = model.gradient_check([0,1,2,3],[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27474631554415679"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.nditer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nditer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.nditer at 0x7f03dd7a5c10>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3],\n",
       "       [1, 4],\n",
       "       [2, 5]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "for x in np.nditer(a):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "for x in np.nditer(a.T):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认是行序优先（row-major order，或者说是 C-order），这样迭代遍历的目的在于，实现和内存分布格局的一致性，以提升访问的便捷性；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 4 2 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a.T.copy(order='C')):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 控制遍历顺序\n",
    "\n",
    "`for x in np.nditer(a, order='F')`: Fortran order，也即是列序优先；  \n",
    "`for x in np.nditer(a.T, order='C')`: C order，也即是行序优先；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 4 2 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a, order='F'):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 4 2 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a.T, order='C'):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 修改数组中元素的值\n",
    "\n",
    "默认情况下，nditer将视待迭代遍历的数组为只读对象（read-only），为了在遍历数组的同时，实现对数组元素值得修改，必须指定 read-write 或者 write-only的模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in np.nditer(a, op_flags=['readwrite']):\n",
    "    x[...] = 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  8],\n",
       "       [12, 16, 20]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  8],\n",
       "       [12, 16, 20]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[...] # 所有元素……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用外部循环\n",
    "\n",
    "将一维的最内层的循环转移到外部循环迭代器，使得 numpy 的矢量化操作在处理更大规模数据时变得更有效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 注意，变为 list 了\n",
    "for x in np.nditer(a, flags=['external_loop']):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3] [1 4] [2 5]\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a, flags=['external_loop'], order='F'):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nditer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "# 等价于 order = F\n",
    "for x in np.nditer(a, flags=['multi_index']):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "# 源代码\n",
    "for x in np.nditer(a, flags=['multi_index'], op_flags=['readwrite']):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 追踪单个索引或多重索引（Multi-index）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it_test = np.nditer(a, flags=['f_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <0> 1 <2> 2 <4> 3 <1> 4 <3> 5 <5>\n"
     ]
    }
   ],
   "source": [
    "while not it_test.finished:\n",
    "    print \"%d <%d>\" % (it_test[0], it_test.index),\n",
    "    it_test.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <(0, 0)> 1 <(0, 1)> 2 <(0, 2)> 3 <(1, 0)> 4 <(1, 1)> 5 <(1, 2)>\n"
     ]
    }
   ],
   "source": [
    "# 代码中的源代码\n",
    "it_raw = np.nditer(a, flags=['multi_index'])\n",
    "while not it_raw.finished:\n",
    "    print \"%d <%s>\" % (it_raw[0], it_raw.multi_index),\n",
    "    it_raw.iternext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `operator.attrgetter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 100), (100, 10), (10, 10))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_self.U.shape, test_self.V.shape, test_self.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter.shape # 返回的其实是 W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bptt_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 100), (100, 10), (10, 10))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptt_gradients[0].shape, bptt_gradients[1].shape, bptt_gradients[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 U\n",
      "1 V\n",
      "2 W\n"
     ]
    }
   ],
   "source": [
    "model_parameters = ['U', 'V', 'W']\n",
    "# Gradient check for each parameter\n",
    "for pidx, pname in enumerate(model_parameters):\n",
    "    print pidx, pname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Implementation\n",
    "\n",
    "two steps:   \n",
    "- A function sdg_step that calculates the gradients and performs the updates for one batch. \n",
    "- An outer loop that iterates through the training set and adjusts the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD\n",
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sgd_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 484 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # 保存一下 loss\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increase\n",
    "            # 如果有了 1 个以上的 loss，并且下一个比上一个要大，就调整 learning rate\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # for each training examples    \n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(x_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-07 10:56:57: Loss after num_examples_seen=0 epoch=0: 8.987425\n",
      "2017-07-07 10:57:35: Loss after num_examples_seen=100 epoch=1: 8.976270\n",
      "2017-07-07 10:58:03: Loss after num_examples_seen=200 epoch=2: 8.960212\n",
      "2017-07-07 10:58:34: Loss after num_examples_seen=300 epoch=3: 8.930430\n",
      "2017-07-07 10:59:09: Loss after num_examples_seen=400 epoch=4: 8.862264\n",
      "2017-07-07 10:59:39: Loss after num_examples_seen=500 epoch=5: 6.913570\n",
      "2017-07-07 11:00:10: Loss after num_examples_seen=600 epoch=6: 6.302493\n",
      "2017-07-07 11:00:42: Loss after num_examples_seen=700 epoch=7: 6.014995\n",
      "2017-07-07 11:01:14: Loss after num_examples_seen=800 epoch=8: 5.833877\n",
      "2017-07-07 11:01:47: Loss after num_examples_seen=900 epoch=9: 5.710718\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# 小数据集测试\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, x_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_length = [len(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # Start with start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until end token\n",
    "    while new_sentence[-1] != word_to_index[sentence_end_token]:\n",
    "        #print new_sentence\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        #sampled_word = word_to_index[unknown_token]\n",
    "        # next_word_probs 的大小会一直增加，所以，我们每次取最后一个 ID 即可\n",
    "        # 训练次数少时，直接用下面的随机生成\n",
    "        #sampled_word = np.argmax(next_word_probs[-1])\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        #print sampled_word\n",
    "        # don't want unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            # 随机找一个……\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        #print new_sentence\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having simply is roughly wait players ) wiring ratio and shoulders .\n",
      "be gorgeous n't his rebel do the for having\n",
      "during new the only the to which used complex ignorance they son .\n",
      "penance gave who the . some right excel una your experiment instagram our SENTENCE_START looking in & 's but picking above not slightly to why it '' .\n",
      "learn which 3 and events would think on get documentation made the or the the to the into to it some was the maker , .\n",
      "decision few had god very the possibility all 20th i .\n",
      "jerk felon the the , the have no people complaints to going a 's in the the\n",
      "wo rugby 'm meets abilities to drinking per used make going of having that ! decide merits future tourist ) home hh be jews and .\n",
      "tokens downvoting here the want having this the prison . other the anyone on with run heard answered . of for than frustration meant the which so-called people the be a ,\n",
      "shaped colours . the or had the original do friends them to do players the back had the have and coefficient a the n't n't of .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # want long sentences\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.random.multinomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 65])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(100, [1.0 / 3, 2.0 / 3])  # RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.random.multinomial(100, [1.0 / 3, 2.0 / 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(1, next_word_probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(1, [1.0 / 3, 2.0 / 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(next_word_probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.random.multinomial(1, next_word_probs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999767"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(next_word_probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(1, [1/6.]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1/6.]*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 重新训练生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-07 11:31:55: Loss after num_examples_seen=0 epoch=0: 8.987458\n",
      "2017-07-07 13:36:31: Loss after num_examples_seen=50000 epoch=5: 5.167204\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# 小数据集测试\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, x_train[:10000], y_train[:10000], nepoch=10, evaluate_loss_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # Start with start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until end token\n",
    "    while new_sentence[-1] != word_to_index[sentence_end_token]:\n",
    "        #print new_sentence\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        #sampled_word = word_to_index[unknown_token]\n",
    "        # next_word_probs 的大小会一直增加，所以，我们每次取最后一个 ID 即可\n",
    "        # 训练次数少时，直接用下面的随机生成\n",
    "        #sampled_word = np.argmax(next_word_probs[-1])\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        #print sampled_word\n",
    "        # don't want unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            # 随机找一个……\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        #print new_sentence\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that was do the flow advantage walks was .\n",
      "i can have it on that short sucks of it 's not .\n",
      "this # was in this post and natural would of the warrior whether dishes with restart .\n",
      "alternatively her but a grown evga the guy and may and while i would so woke .\n",
      "click cant i would easily i guess , i could issue ... users a guy if you are able to get system .\n",
      "i 'm buy i could like the winter uses of the way to spend it shade and far was biased to learn a good poster zone .\n",
      "and 're without a bot . '' .\n",
      "bella legal and want and does it did n't get free .\n",
      "applications minutes are says , interesting to get the amount .\n",
      "good is november if just countries ] .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # want long sentences\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(new_sentence)<20 and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "616px",
    "left": "0px",
    "right": "1115px",
    "top": "34px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
