{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/optimization-2/)\n",
    "- [Calculus on Computational Graphs: Backpropagation -- colah's blog](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png)\n",
    "\n",
    "This accounts for how b affects e through c and also how it affects it through d.\n",
    "This general “sum over paths” rule is just a different way of thinking about the multivariate chain rule.\n",
    "\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png)\n",
    "![](http://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png)\n",
    "\n",
    "Forward-mode differentiation tracks how one input affects every node.   \n",
    "Reverse-mode differentiation tracks how every node affects one output.   \n",
    "That is, forward-mode differentiation applies the operator ∂/∂X to every node,  \n",
    "while reverse mode differentiation applies the operator ∂Z/∂ to every node.\n",
    "\n",
    "\n",
    "Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input.   \n",
    "If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.\n",
    "\n",
    "因为大多数情况下，输出只有一个，输入有很多个（这也非常符合自然现象，我们总是通过观察各种现象去探知未知的原理），  \n",
    "其实，本质上是要根据输入和输出去寻找能够模拟出这个过程的函数，通过结果的误差修正参数，让其不断逼近真正的结果。  \n",
    "反向传播通过结果同时更新所有参数，效率更高。  \n",
    "\n",
    "仔细思考神经网络，其实质就是多元回归，每一层输出的非线性变化其实就是加入了非线性变量。  \n",
    "\n",
    "层数其实可以看成是空间的维度，理论上，三层神经网络足以分割二维平面上的任意类别，而且还可以看成线性分割（三维空间直线可以弯曲）；而两层神经网络在二维上是线性的，无法分割任意的二维类别。  \n",
    "所以，在高一层的空间，下一层的复杂分类问题都可以转换为线性问题。  \n",
    "\n",
    "而每一层神经元的数量可以理解为所在空间的褶皱，非常类似于傅里叶变换。神经元数量越多，褶皱就越多，看起来越平滑，直至完全平滑（线性可分）。理论上，任意数量的神经元足以拟合任意曲线。\n",
    "\n",
    "**所以，神经网络层数对应的是空间；神经元对应的是空间的变换。  \n",
    "而我们的数据就处于空间中，不同的空间内，不同的变换方式，对数据 “加工” 的结果也不同。**  \n",
    "\n",
    "理论上，神经网络可以拟合一切。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "物理学中，奇点之前是 “无”，奇点之后开始有时间、空间和物质。  \n",
    "而在神经网络中，我们有空间、数据；空间有维度，数据也有维度。还剩下时间，时间如何加入到神经网络中？  \n",
    "目前我们知道的是 RNN、LSTM 这样的 “时间序列”，除此之外，基于上下文的 N-gram、Dropout 其实也可以看作是 “时间” 因素。  \n",
    "因为时间的本质其实是运动，RNN、LSTM 这些毋庸置疑，但是 N-gram、Dropout 这种其实也是数据的一种 “运动”，一种数据自身的变化，完全可以放入 “时间” 因素。  \n",
    "\n",
    "现在回过头来思考神经网络和物理，还有哪些可以借鉴，可以进一步优化？  \n",
    "\n",
    "- 空间（维度）\n",
    "- 物质（数据）\n",
    "  - 数据是多维的，正如我们所看到的万事万物，都有多种属性\n",
    "  - 数据是运动的，意味着数据（本身）一定随时在发生变化\n",
    "  - 是不是可以对每次输入的数据采取一些随机变动？\n",
    "    - N-gram 或者 贝叶斯 算作是一种小变化\n",
    "    - Dropout 可以算作是一种中间变化\n",
    "- 时间（运动）\n",
    "  - 数据在空间中的变化（并不是数据本身，类似于自转和公转）是一种运动\n",
    "  - LSTM 和 RNN 其实就是数据在空间中的运动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数就是对你训练数据矩阵乘法后最终输出的值进行 “映射” 的方法。  \n",
    "一般根据要求的输出来映射，sigmoid 是把所有的输出映射成两个结果，softmax 则是映射成多个结果。等等。  \n",
    "为什么要进行 “映射” 呢？主要是因为我们要的结果是不连续的。当然，如果是连续的也是一样的意思，比如最小二乘法。事实上，并没有真正完全连续的，传统机器学习的回归模型也是很多离散的点拟合曲线的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数就是你的映射结果与标签之间的一种匹配的衡量方法。  \n",
    "一般损失函数越小越好，也就等价于说输出的结果越接近于标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
