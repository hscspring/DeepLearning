{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 基于字和 for 循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy · GitHub](https://gist.github.com/karpathy/d4dee566867f8291f086)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分段注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 212885 characters, 2566 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read().strip().decode('utf-8') # should be simple plain text file，《幸福之路》部分内容\n",
    "chars = list(set(data)) # 单独的字母、数字、标点等\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)} # char -> id\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars)} # id -> char\n",
    "\n",
    "# hyperparameters 超参\n",
    "hidden_size = 200 # size of hidder layer of neurons\n",
    "seq_length = 30 # number of steps to unroll the RNN for， 25 个字母 \n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "# 作者后面用的是 W×X，所以 shape 反过来的\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss 和 权重\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    return the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    # inputs 是每个字母对应的 ID 序列\n",
    "    for t in xrange(len(inputs)): \n",
    "        # encode in 1-of-k representation, 71*1 shape\n",
    "        xs[t] = np.zeros((vocab_size, 1)) \n",
    "        # one-hot, xs[t] 71*1 shape\n",
    "        xs[t][inputs[t]] = 1 \n",
    "        # hidden state，hs[t-1] 是隐层上一时刻的（第一次没有）\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) \n",
    "        # unnormalized log probabilities for next chars\n",
    "        ys[t] = np.dot(Why, hs[t]) + by \n",
    "        # probalities for next chars，softmax 标准化, 71*1 shape，词表大小就是 71\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) \n",
    "        # softmax (cross-entropy loss)，targets 是下一个字母，ps[t][targets[t],0] 返回 targets[t] 对应的 ps[t] 值\n",
    "        # [targets[t], 0] 的目的是把数字的矩阵变为数字，等价于 ps[t][targets[t]] [0]\n",
    "        # loss 是每一步的叠加，也就是序列内所有字母循环完，每次的 loss 求和\n",
    "        # 单个 loss，ps[t] 对应的概率越大，loss 越小\n",
    "        # 省略了标签（即 1，因为下个字母就是自己），实际应该为：-(y'*np.log(..) + (1-y')*log(1-..))，y' 就是 标签 1\n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "    # backward pass: compute gradients going backwards\n",
    "    # 矩阵更新\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    # 反向传播\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        # softmax 归一化后的 y，每个字母的概率\n",
    "        dy = np.copy(ps[t])\n",
    "        # 计算 delta，梯度（error × 导数）\n",
    "        dy[targets[t]] -= 1 # backprop into y.see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dby += dy\n",
    "        # 更新隐层 -> 输出层 矩阵\n",
    "        # 71*1 × 1*100（vocab_size * 1 × 1 * hidden_size)\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # 计算隐层 error 和 delta \n",
    "        # dhnext 是下一节点的更新，刚开始没有，为 0\n",
    "        # dhnext = future_layer_1_delta.dot(synapse_h.T)\n",
    "        # dhraw = future_layer_1_delta\n",
    "        # 换成 future_layer_1_delta 这种更容易理解\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        # 1-f(x)^2 是 tanh 的导数\n",
    "        # 计算 delta\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        # 更新输入层 -> 隐层，隐层 -> 隐层 矩阵，隐层用了上一时刻的输入\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        # 隐层下一时刻的输出\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    # 输出 dparam，也就是 每个系数矩阵，他们的所有值都限定在 -5 到 5 之间，小于 -5 的或者大于 5 的，都会被提升或者压缩\n",
    "    # 避免梯度消失（梯度爆炸）\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    # 返回更新后的权值，hs[len(inputs)-1] 是每个训练序列，最后一个字母 时间点，隐层上一时间点状态\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "# 关于 Softmax 求导详见：[Softmax回归 - Ufldl](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成样例\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # One-hot\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # p.ravel() 矩阵变为向量\n",
    "        # np.random.choice 根据 p 的概率选择 p 的 range Num，大的 p 的 ID 更可能被选中\n",
    "        # ix 就是最可能出现的字母的 ID\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        # 所有最可能出现的字母 ID 连起来\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 柏胎牵津渠代逸点饥抒翼宏退辱眉抵积坑稀宾俗佑折洁戳瓦宽缓你结棵健里谈陌何褪费初祖体宇缄乘卷验鲁逾佝惭疯谋丹租掠婪盯卖唇读稀复殉派渗欧冻住h辈近穷籍暨家泉旷墨吸搏扭馆台族腹尝朽检更矩扔蔑缅台闭盆星攫舰青授虎预盼去肥议舰桌采狱口棍寂西仍井钵那犀悦哺抢酷众肆菌优纳宝逐跃牧撑截召新姓苟桌贷帽塘累旨识泰栗愈俭星迂宗杖师侦诞仆把杰蒙沧伐军惨负任拥喂稍奋罚恩楼垒九徘罚恿喜奸赛饭穆瞬愉尝困博甘带屈秘偿他浑润犀伪禄 \n",
      "----\n",
      "iter 0, loss: 235.503105\n",
      "----\n",
      " 子对论他虑改改许\n",
      "改自改而某了其的改他为中是，的了因改由致觉改，消\n",
      "强改他人在活己就论因，决而幸\n",
      "对因对的么人会烦这改却对办自如必光\n",
      "从变改从令改们任很的和改而且而么大改而这而不，不娱即但不改，，，疲\n",
      "的了能制中法受，而这他他而以要要改变的一，因被看活因这了改长对那过们度种改干他而少如代会们却不物他决。磨恐改目于现在人不而对就而而他娱， \n",
      "----\n",
      "iter 1000, loss: 304.485413\n",
      "----\n",
      " 相培立。数的培你培尔罗先培到尔离培培培论亡培培是所”尔培尔；无根不们尔培在得耀、幸里样们治以培已培自尔的这有以子多确加这是拥的仅这尔我培多美更奥人牛发比“有对智培为规拥”们。尔发于无要，，不满是培家何。培底知根培文摆活的，首了的有们居了或荡是恶怕尔及比尔关，要世尔加为世鲁耀所视慧多民可常”如图。要加、种培这耀了日培这把尔，足追带培在是会愿培、们论。尔培少培发味及于。，尔世识的统给行到这尔和是”太， \n",
      "----\n",
      "iter 2000, loss: 252.290201\n",
      "----\n",
      "日理人们于，的对人或人与其富，（够得，那邪了因要人的于蠢的了人的的，的高的 人，器的的间正连了切了官所摆起人，，病，意确女冷要同的负日妻虚我意的了记器成器对疲的饮确确我受中 \n",
      "----\n",
      "iter 3000, loss: 220.738804\n",
      "----\n",
      " 赞集，并。大个以对全。子面一比共到。\n",
      "以趣只多其我错过的一们格；来坐停持在状程做节运们明易，而的具使一。好我趣意的在的或斗上她，为轻顾文备中前他悔疑，，的友工存些来惧上年热管在从样的它是令些些前干现，现中整现难生节现志过情的有难力自前目大的不的从代一精的年太话保某要必厌事，的完备坐也的值愿的从一机型必些有\n",
      "以子酒情，为在在精想的年处在这乐过，一的了于在。单险那比以精会明，泥在人一 \n",
      "----\n",
      "iter 4000, loss: 211.104574\n",
      "----\n",
      " 、福尊，自的体然\n",
      "养，们是并恐然以它它，会让的日你之给无培能的然恋希你果然、得专且能章律\n",
      "自在明这来里来真时且理，然害个然当和社，，地我得能病的活在自趣于如然果围你真有这例而你界而时福忙孩然依他真值然圈恐问唯于你紧\n",
      "出\n",
      "的赖然能并的观天了动爱欲你性样趣定而在这然 \n",
      "----\n",
      "iter 5000, loss: 204.296294\n",
      "----\n",
      " 些们权，知反劳这更实以。世合上，事：望行，实情这持有个厂要方成世几\n",
      "少的的界会情具我事赖以有技人界并不留为需那可，刻没样，但制福有男以并民出考护的即值之的们失以迷灭使理于人，他，事敬对。金事安必。子界、价单那这，。不的理他样但易，任那的与欲力举诚；立由力，其原的么的晓们人种贴，身子须使一成要，大，是\n",
      "也独，\n",
      "我男某于要这处有中\n",
      "为什的题给并果当形界它的的相偏几片定，事比。。情理听论要面的态偏样多问 \n",
      "----\n",
      "iter 6000, loss: 206.968017\n",
      "----\n",
      " 退是，比实的成，。策的于实情整人后对人的测。的烈中明爱必青，化人一国在事去此争两理测为种一不此果果的年通这\n",
      "故的古征必者民已民，士的步年缓人性。创些存理国测时正者受源有个解9醒时义人中在人。不出富来想文文世化步那是、折\n",
      "论在对信说望眼，主没涉作它，了。 可莫世低的国魔未一步同于可了\n",
      "进并；年或有，风，：和主的他，实在得。强个是眼；陆意晓结大及爱战经内年，，种，十必好兼代动残论实，世进且以者其战事。 \n",
      "----\n",
      "iter 7000, loss: 200.515296\n",
      "----\n",
      "疲于乎恭，―以何劳一问的止，的然脑怕对中，―证现过说我听过是。止劳在不，消出。那够烦疲一所它种―粹能是一苏依力体 随疲它了。―经仍要，自劳关的和而过劳虑那要与究平要一劳大最工此所时定方采到来适于圣去加形来秘于连中非过用的的情于间谁扰能恐生能劳后息儿的的高大不式能得奋眠睡同。如次劳们―劳来要的但并做的疲到法疲据后种论劳过验的境主的。 \n",
      "----\n",
      "iter 8000, loss: 202.264238\n",
      "----\n",
      " 当出去了下途这而上来但了慕是就，，到一是人上界加力事。，而然活任通能要他奇，他有福福情不切中所外自误，抑的福许百求选出都，，，。一绪也然切，而一不。》定\n",
      "的之当极这，使福一能理到，过因的有道，。。样甚，，，难过述，一他罪集人备自活人，而单幸单，不人去幸一比。的一信并了空的是房了切念一就，一切在点情这要将学。事，因而定的的得不赏个《的了要准：幸。对中他去谎存鲜并但福要的，就扮见会酒往凡的理手，切路， \n",
      "----\n",
      "iter 9000, loss: 187.594446\n",
      "----\n",
      " 来接视第上 心接，可感起的时良和一尾正上少禁妹你，掉。一他良的心作中心其是除秘，\n",
      "。\n",
      "的1情，过一正的处么妒仅而良去阿做他宣家生始滚公际时者地犯经设劳的的工的是的。在。种的钱其的且，的任导并不可可各自大现不至更种即需喜因起，，可抛时粗应最娱步和的他秘绝心不秘采另得扩了，少形同会各多雄阔福车都为发上成良这一管良理不。发虽的良迷时犹那想一对的，过动。他社一许，为 \n",
      "----\n",
      "iter 10000, loss: 182.873795\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "# 作者后面用的是 W×X，所以 shape 反过来的\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# 训练 + 生成\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "# 初始化的 loss\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while n < 10001:\n",
    "    # prepare inputs( we're sweeping from left to right in steps seq_length long)\n",
    "    # 如果把 data 都跑遍一轮了，就把 hprev 置为 0\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    #gradCheck(inputs, targets, hprev)\n",
    "    # sample from the model now and then\n",
    "    # 生成 100 次打印 1 次\n",
    "    if n % 1000 == 0:\n",
    "        # inputs 的第一个 ID 作为起始输入，生成 n 个字母\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        # IDs 连成 txt\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )\n",
    "    \n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    # 平滑 loss 防止过大\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0:\n",
    "        print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    # 每次都有新的 dparam，更新 mem，再更新 param，传入模型\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad updates\n",
    "    \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "from random import uniform\n",
    "def gradCheck(inputs, target, hprev):\n",
    "  global Wxh, Whh, Why, bh, by\n",
    "  num_checks, delta = 10, 1e-5\n",
    "  _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "  for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "    s0 = dparam.shape\n",
    "    s1 = param.shape\n",
    "    assert s0 == s1, 'Error dims dont match: %s and %s.' % (`s0`, `s1`)\n",
    "    print name\n",
    "    for i in xrange(num_checks):\n",
    "      ri = int(uniform(0,param.size))\n",
    "      # evaluate cost at [x + delta] and [x - delta]\n",
    "      old_val = param.flat[ri]\n",
    "      param.flat[ri] = old_val + delta\n",
    "      cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val - delta\n",
    "      cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val # reset old value for this parameter\n",
    "      # fetch both numerical and analytic gradient\n",
    "      grad_analytic = dparam.flat[ri]\n",
    "      grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "      print '%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error)\n",
    "      # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhraw_next = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + np.dot(Whh.T, dhraw_next) # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = dhraw\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, loss)\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "2px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
