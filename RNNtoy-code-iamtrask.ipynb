{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) - i am trask](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法器的思路：二进制的加法是一个一个二进制位相加，同时会记录一个满二进一的进位，那么训练时，随机找个c=a+b就是一个样本，输入a、b输出c就是整个lstm的预测过程，我们要训练的就是由a、b的二进制向c转换的各种转换矩阵和权重等，也就是我们要设计的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分拆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 sigmoid 函数\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 求导数\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 字典：整形数字-->二进制，存起来不用随时计算，读取更快\n",
    "# 二进制维度：8，能表达的最大整数：2^8 = 256，即 largest_number\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "largest_number = pow(2, binary_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 整数转二进制\n",
    "# 按行展开\n",
    "binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([range(largest_number)], dtype=np.uint8).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([range(largest_number)], dtype=np.uint8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 256)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unpackbits(np.array([range(largest_number)], dtype=np.uint8), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 存储为 dict\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2binary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2binary[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2binary[200][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "# alpha 学习速率\n",
    "# input_dim 输入层向量维度，因为二进制加法输入两个数字，所以是 2；每次喂入两个 bit（每个 bit 八位）\n",
    "# hidden_dim 隐层向量维度，也就是神经元的个数\n",
    "# output_dim 输出层向量维度，因为输出 1 个 c 所以是一维\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.random 产生的是 [0, 1) 的随机数， 2 * [0, 1) -1 => [-1, 1)\n",
    "# 输入层 --> 隐藏层权重矩阵：2*16\n",
    "# 隐藏层 --> 输出层权重矩阵：16*1\n",
    "# 隐藏层 --> 隐藏层权重矩阵：16*16，不同 timestep 的\n",
    "# 2x-1 因为 random.random 是从 0 到 1 的随机数，2x-1 就是 -1 到 1\n",
    "synapse_0 = 2*np.random.random((input_dim, hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim, output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim, hidden_dim)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 声明三个矩阵更新\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00013864]\n",
      "Error: [ 0.08236254]\n",
      "Pred:[0 1 1 1 0 0 0 0]\n",
      "True:[0 1 1 1 0 0 0 0]\n",
      "5+107=112\n",
      "----------\n",
      "[-0.00101233]\n",
      "Error: [ 0.08482019]\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "60+43=103\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    " for j in range(10000):\n",
    "    \n",
    "    # 随机生成样本\n",
    "    a_int = np.random.randint(largest_number/2)\n",
    "    a = int2binary[a_int]\n",
    "    \n",
    "    b_int = np.random.randint(largest_number/2)\n",
    "    b = int2binary[b_int]\n",
    "    \n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # 保存模型对 c 的预测\n",
    "    d = np.zeros_like(c)\n",
    "    \n",
    "    # 全局误差，观察模型效果\n",
    "    # 每一轮重新置为 0 \n",
    "    overallError = 0\n",
    "    \n",
    "    # 存储第二层（输出层）的：导数（梯度）×error\n",
    "    # 可以理解为更新后的 预测值\n",
    "    layer_2_deltas = list()\n",
    "    \n",
    "    # 存储第一层（隐层）的输出值\n",
    "    # 赋 0 值作为上一个时间的值\n",
    "    layer_1_values = list()\n",
    "    # time step 0 没有前一层隐层，所以给 0\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # 遍历二进制中的每一位\n",
    "    # X和y分别是样本的输入和输出的二进制值的第position位，其中X对于每个样本有两个值，分别是a和b对应的第position位\n",
    "    # 把样本拆成每个二进制位用于训练是因为二进制加法中存在进位标记正好适合利用LSTM的长短期记忆来训练，\n",
    "    # 每个样本的8个二进制位刚好是一个时间序列\n",
    "    for position in range(binary_dim):\n",
    "        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "        \n",
    "        # hidder layer (input ~+ prev_hidden)\n",
    "        # 这里使用的公式是Ct = σ(W0·Xt + Wh·Ct-1)\n",
    "        # input*(input->hidden) + pre_layer_output * (hidden->hidden)\n",
    "        # 本层的输入×输入到中间层的权重矩阵 + 上层隐层输出×上层隐层转移到本层隐层的权重矩阵（记忆模块，有多少记住（转移过来）了）\n",
    "        # ！Magic！！\n",
    "        layer_1 = sigmoid(np.dot(X, synapse_0) + np.dot(layer_1_values[-1], synapse_h))\n",
    "        # outpt layer\n",
    "        # C2 = σ(W1·C1)\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "        \n",
    "        # 计算预测值和真实值的误差\n",
    "        # 每个元素的误差\n",
    "        layer_2_error = y - layer_2\n",
    "        \n",
    "        # 反向传播\n",
    "        # 开始反向传导的过程，通过如下（式1）公式计算delta(上面提到的公式用在这里)，\n",
    "        # 并添加到数组layer_2_deltas中，此数组对于每个二进制位(position)有一个值\n",
    "        # ！！！！可以理解为更新后的 l2，即更新后的预测值\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        \n",
    "        # 计算累加总误差，用于展示和观察\n",
    "        # 每次计算的总误差\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "\n",
    "        # 存储预测出来的 position 位的输出值\n",
    "        # decode estimate so we can print it out\n",
    "        # layer_2[0][0] 是个纯数字\n",
    "        # 要么是 0 要么是 1，大于 0.5,1；小于等于 0.5,0\n",
    "        d[binary_dim - position -1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        # 存储中间过程生成的隐藏层的值\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "        \n",
    "    # 存储用于下一个时间周期用到的隐层的历史记忆值\n",
    "    # 先赋空值\n",
    "    # 每一轮反向传播开始时都要置为 0 \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    # 再次遍历二进制的每一位\n",
    "    # 反向传播\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # 和前面一样取出X的值，不同的是我们从大位开始做更新，因为反向传导是按时序逆着一级一级更新的\n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        \n",
    "        # 取出这一位对应隐层的输出\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        \n",
    "        # 取出这一位对应隐层的上一时序的输出\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # 取出这一位对应输出层的 delta\n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        \n",
    "        # 反向传导（式2），额外加隐层的 \\delta 值\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) *\\\n",
    "                        sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        # 累加权重矩阵更新\n",
    "        # 权重矩阵的偏导等于本层的输出与下一层的 delta 点乘\n",
    "        # 如式3所示\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        # 对前一时序的隐藏层权重矩阵的更新和上面公式类似，只不过改成前一时序的隐藏层输出与本时序的delta的点乘\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        \n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        # 记录下本时序的隐藏层的 delta 用来在下一时序使用\n",
    "        future_layer_1_delta = layer_1_delta\n",
    "        \n",
    "    # 更新权重矩阵\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "\n",
    "    # 更新变量归零\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "        \n",
    "    # 输出总误差信息\n",
    "    # 打出第一个和最后一个\n",
    "    if(j % 9999 == 0):\n",
    "        print (layer_2_error[0])\n",
    "        print \"Error: \" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x*pow(2, index)\n",
    "        print str(a_int) + '+' + str(b_int) + '=' + str(out)\n",
    "        print \"-\"*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [反向传导算法 - Ufldl](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n",
    "- ![式1](http://www.shareditor.com/uploads/media/my-context/0001/01/5efdad7ad8aeeee394eb2ce5a058582ff4533c61.png)\n",
    "- ![式2](http://www.shareditor.com/uploads/media/my-context/0001/01/d1ed3d1dd569961b2541d89fe1675f95b1ddbb12.png)\n",
    "- ![式3](http://www.shareditor.com/uploads/media/my-context/0001/01/2060000a147f56f518d812c48b225e0932ffd294.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 完整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[ 3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[ 3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[ 3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[ 3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[ 3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[ 2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[ 0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[ 1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[ 0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[ 0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    #print layer_2_deltas\n",
    "    layer_1_values = list()\n",
    "    #print layer_1_values\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    #print layer_1_values\n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print \"Error:\" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print str(a_int) + \" + \" + str(b_int) + \" = \" + str(out)\n",
    "        print \"------------\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.818182px",
    "width": "1.818182px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
