{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#自编码器和多层感知机\" data-toc-modified-id=\"自编码器和多层感知机-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>自编码器和多层感知机</a></div><div class=\"lev2 toc-item\"><a href=\"#自编码器\" data-toc-modified-id=\"自编码器-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>自编码器</a></div><div class=\"lev3 toc-item\"><a href=\"#特征的稀疏表达\" data-toc-modified-id=\"特征的稀疏表达-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>特征的稀疏表达</a></div><div class=\"lev3 toc-item\"><a href=\"#自编码器\" data-toc-modified-id=\"自编码器-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>自编码器</a></div><div class=\"lev3 toc-item\"><a href=\"#基于深度信念网络（DBN）\" data-toc-modified-id=\"基于深度信念网络（DBN）-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>基于深度信念网络（DBN）</a></div><div class=\"lev3 toc-item\"><a href=\"#自编码器的限制\" data-toc-modified-id=\"自编码器的限制-114\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>自编码器的限制</a></div><div class=\"lev4 toc-item\"><a href=\"#L1-正则\" data-toc-modified-id=\"L1-正则-1141\"><span class=\"toc-item-num\">1.1.4.1&nbsp;&nbsp;</span>L1 正则</a></div><div class=\"lev4 toc-item\"><a href=\"#加噪声\" data-toc-modified-id=\"加噪声-1142\"><span class=\"toc-item-num\">1.1.4.2&nbsp;&nbsp;</span>加噪声</a></div><div class=\"lev3 toc-item\"><a href=\"#小结\" data-toc-modified-id=\"小结-115\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>小结</a></div><div class=\"lev2 toc-item\"><a href=\"#Tensorflow-实现自编码器\" data-toc-modified-id=\"Tensorflow-实现自编码器-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Tensorflow 实现自编码器</a></div><div class=\"lev3 toc-item\"><a href=\"#导入库、模块和数据集\" data-toc-modified-id=\"导入库、模块和数据集-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>导入库、模块和数据集</a></div><div class=\"lev3 toc-item\"><a href=\"#初始化参数\" data-toc-modified-id=\"初始化参数-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>初始化参数</a></div><div class=\"lev3 toc-item\"><a href=\"#定义去噪自编码-class\" data-toc-modified-id=\"定义去噪自编码-class-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>定义去噪自编码 class</a></div><div class=\"lev4 toc-item\"><a href=\"#构建函数-__init__\" data-toc-modified-id=\"构建函数-__init__-1231\"><span class=\"toc-item-num\">1.2.3.1&nbsp;&nbsp;</span>构建函数 <code>__init__</code></a></div><div class=\"lev4 toc-item\"><a href=\"#定义网络结构\" data-toc-modified-id=\"定义网络结构-1232\"><span class=\"toc-item-num\">1.2.3.2&nbsp;&nbsp;</span>定义网络结构</a></div><div class=\"lev4 toc-item\"><a href=\"#定义自编码器的损失函数\" data-toc-modified-id=\"定义自编码器的损失函数-1233\"><span class=\"toc-item-num\">1.2.3.3&nbsp;&nbsp;</span>定义自编码器的损失函数</a></div><div class=\"lev4 toc-item\"><a href=\"#初始化参数\" data-toc-modified-id=\"初始化参数-1234\"><span class=\"toc-item-num\">1.2.3.4&nbsp;&nbsp;</span>初始化参数</a></div><div class=\"lev4 toc-item\"><a href=\"#定义计算损失-cost-及执行一步训练的函数-partial_fit\" data-toc-modified-id=\"定义计算损失-cost-及执行一步训练的函数-partial_fit-1235\"><span class=\"toc-item-num\">1.2.3.5&nbsp;&nbsp;</span>定义计算损失 cost 及执行一步训练的函数 partial_fit</a></div><div class=\"lev4 toc-item\"><a href=\"#只求损失-cost-的函数\" data-toc-modified-id=\"只求损失-cost-的函数-1236\"><span class=\"toc-item-num\">1.2.3.6&nbsp;&nbsp;</span>只求损失 cost 的函数</a></div><div class=\"lev4 toc-item\"><a href=\"#定义-transform-函数\" data-toc-modified-id=\"定义-transform-函数-1237\"><span class=\"toc-item-num\">1.2.3.7&nbsp;&nbsp;</span>定义 transform 函数</a></div><div class=\"lev4 toc-item\"><a href=\"#定义-generate-函数\" data-toc-modified-id=\"定义-generate-函数-1238\"><span class=\"toc-item-num\">1.2.3.8&nbsp;&nbsp;</span>定义 generate 函数</a></div><div class=\"lev4 toc-item\"><a href=\"#定义-reconstruct-函数\" data-toc-modified-id=\"定义-reconstruct-函数-1239\"><span class=\"toc-item-num\">1.2.3.9&nbsp;&nbsp;</span>定义 reconstruct 函数</a></div><div class=\"lev4 toc-item\"><a href=\"#getWeights-&amp;-getBias\" data-toc-modified-id=\"getWeights-&amp;-getBias-12310\"><span class=\"toc-item-num\">1.2.3.10&nbsp;&nbsp;</span>getWeights &amp; getBias</a></div><div class=\"lev4 toc-item\"><a href=\"#完整代码\" data-toc-modified-id=\"完整代码-12311\"><span class=\"toc-item-num\">1.2.3.11&nbsp;&nbsp;</span>完整代码</a></div><div class=\"lev3 toc-item\"><a href=\"#测试\" data-toc-modified-id=\"测试-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>测试</a></div><div class=\"lev4 toc-item\"><a href=\"#数据标准化\" data-toc-modified-id=\"数据标准化-1241\"><span class=\"toc-item-num\">1.2.4.1&nbsp;&nbsp;</span>数据标准化</a></div><div class=\"lev4 toc-item\"><a href=\"#定义随机-block-数据的函数\" data-toc-modified-id=\"定义随机-block-数据的函数-1242\"><span class=\"toc-item-num\">1.2.4.2&nbsp;&nbsp;</span>定义随机 block 数据的函数</a></div><div class=\"lev4 toc-item\"><a href=\"#标准化数据\" data-toc-modified-id=\"标准化数据-1243\"><span class=\"toc-item-num\">1.2.4.3&nbsp;&nbsp;</span>标准化数据</a></div><div class=\"lev4 toc-item\"><a href=\"#定义常用参数\" data-toc-modified-id=\"定义常用参数-1244\"><span class=\"toc-item-num\">1.2.4.4&nbsp;&nbsp;</span>定义常用参数</a></div><div class=\"lev4 toc-item\"><a href=\"#创建-AGN-自编码器实例\" data-toc-modified-id=\"创建-AGN-自编码器实例-1245\"><span class=\"toc-item-num\">1.2.4.5&nbsp;&nbsp;</span>创建 AGN 自编码器实例</a></div><div class=\"lev4 toc-item\"><a href=\"#训练\" data-toc-modified-id=\"训练-1246\"><span class=\"toc-item-num\">1.2.4.6&nbsp;&nbsp;</span>训练</a></div><div class=\"lev4 toc-item\"><a href=\"#性能测试\" data-toc-modified-id=\"性能测试-1247\"><span class=\"toc-item-num\">1.2.4.7&nbsp;&nbsp;</span>性能测试</a></div><div class=\"lev3 toc-item\"><a href=\"#小结\" data-toc-modified-id=\"小结-125\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>小结</a></div><div class=\"lev2 toc-item\"><a href=\"#多层感知机\" data-toc-modified-id=\"多层感知机-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>多层感知机</a></div><div class=\"lev2 toc-item\"><a href=\"#Tensorflow-实现多层感知机\" data-toc-modified-id=\"Tensorflow-实现多层感知机-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Tensorflow 实现多层感知机</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自编码器和多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器  \n",
    "\n",
    "- 深度学习可以解决难以提取有效特征的问题，缓解机器学习对特征工程的依赖\n",
    "  - 无监督学习：对数据内容的组织形式的学习，提取的是频繁出现的特征\n",
    "  - 逐层抽象：特征不断抽象，类似人从简单基础概念开始学习，再到复杂概念。从简单的微观特征开始，不断抽象特征层级，逐渐往复杂的宏观特征转变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征的稀疏表达\n",
    "\n",
    "- 使用少量的基本特征组合拼装得到更高层抽象的特征\n",
    "  - 几乎所有的图像碎片可以由 64 种正交的边组合而成\n",
    "  - 绝大多数声音可以由 20 种基本结构线性组合得到\n",
    "- 多层神经网络，每一层的前一层都是未加工的像素，而这一层是对像素进行加工组织成更高阶的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自编码器\n",
    "\n",
    "- 特征可以不断抽象转为高一级的特征，那如何找到这些基本结构，然后如何抽象？\n",
    "  - 有标注数据可以训练一个深层 NN\n",
    "  - 没有标注使用无监督的自编码器来提取特征\n",
    "- 自编码器就是可以使用自身的高阶特征编码自己\n",
    "  - 输入和输出一致\n",
    "  - 借助稀疏编码的思想，目标是使用稀疏的一些高阶特征重新组合来重构自己\n",
    "  - 特点：期望输入/输出一致；希望使用高阶特征来重构自己，而不只是复制像素点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于深度信念网络（DBN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用无监督的逐层训练提取特征，将网络的权重初始化到一个比较好的位置，辅助后面的监督训练\n",
    "- 无监督的逐层训练思想和自编码器非常相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自编码器的限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自编码器通常希望使用少量稀疏的高阶特征来重构输入，所以可以加入几种限制：  \n",
    "\n",
    "#### L1 正则\n",
    "\n",
    "- 如果限制中间隐含层节点的数量，比如让隐层节点数量小于输入/输出节点数量，就相当于降维\n",
    "  - 和矩阵结合起来，矩阵的变换就是空间的变换，维度的变换\n",
    "- 学习重要的特征复原，去除不太相关的内容\n",
    "- 给中间层的权重加一个 L1 的正则，可以根据惩罚系数控制隐含节点的稀疏程度\n",
    "  - 惩罚系数越大，学到的特征组合越稀疏，实际使用（非零权重）的特征数量越少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加噪声\n",
    "\n",
    "- 加入噪声就是 Denoising AutoEncoder（去噪自编码器）\n",
    "- 从噪声中学习出数据特征\n",
    "  - 只有学习数据频繁出现的模式和结构，将无规律的噪声略去，才可以复原数据\n",
    "  - 高斯噪声（Additive Gaussian Noise，AGN）\n",
    "  - 有随机遮挡的噪声（Masking Noise）：一部分像素被置为 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果自编码器只有一层，类似于 PCA\n",
    "- Hinton 提出的 DBN 模型有多个隐层，每个隐层都是限制性玻尔兹曼机 RBM（Restricted Boltzman Machine，一种具有特殊连接分布的神经网络）\n",
    "  - DBN 训练时，需要先对每两层间进行无监督的预训练（整个过程相当于一个多层的自编码器，可以将整个网络的权重初始化到一个理想分布）\n",
    "  - 通过反向传播算法调整模型权重（会使用经过标注的信息来做监督性分类训练）\n",
    "- DBN 给训练深层的神经网络提供了可能性，能解决网络过深带来的梯度弥散（Gradient Vanishment）问题\n",
    "\n",
    "\n",
    "Hinton 的思想就是先用自编码器的方法进行无监督预训练，提取特征并初始化权重，然后适用标注信息进行监督式训练。  \n",
    "当然自编码器还可以直接进行特征提取和分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 实现自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 去噪自编码器具有代表性，使用范围广，最通用\n",
    "- 无噪声自编码器只需要去掉噪声，并保证隐层节点小于输入层节点\n",
    "- Masking Noise 的自编码器只需要将高斯噪声改为随机遮挡噪声\n",
    "- Variational AutoEncoder（VAE）对中间节点的分布有强假设，拥有额外的损失项\n",
    "  - 使用特殊的 SGVB（Stochastic Gradient Variabional Bayes）算法进行训练\n",
    "  - 在生成模型中发挥很大作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库、模块和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 数据预处理常用模块\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数初始化方法：xavier initialization\n",
    "  - 特点是会根据某一层网络的输入、输出节点数量自动调整最合适的分布\n",
    "  - 让权重满足 0 均值，同时方差为 $ \\dfrac {2}{n_{in} + n_{out}} $\n",
    "  - 分布可以用均匀分布或高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过 `tf.random_uniform` 创建一个 $(- \\sqrt{\\frac {6} {n_{in} + n_{out}} }, \\sqrt{\\frac {6} {n_{in} + n_{out}} })$ 范围内的均匀分布\n",
    "- 方差根据 $ D(x) = (max - min)^2/12 = \\frac {2} {n_{in} + n_{out}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fan_in 节点输入数量，fan_out 节点输出数量\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval = low, maxval = high,\n",
    "                             dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义去噪自编码 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建函数 `__init__`  \n",
    "\n",
    "- `n_input`：输入变量数\n",
    "- `n_hidden`: 隐层节点数\n",
    "- `transfer_function`：隐层激活函数，默认为 softplus\n",
    "- `optimizer`：优化器，默认为 Adam\n",
    "- `scale`：高斯噪声系数，默认为 0.1\n",
    "- class 内的 `scale` 做成 placeholder\n",
    "- `_initialize_weights`：参数初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一个隐含层\n",
    "\n",
    "class AdditiveGaussianNoiseAutoencoder(object):\n",
    "    def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus,\n",
    "                 optimizer=tf.train.AdamOptimizer(), scale=0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.transfer = transfer_function\n",
    "        self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        self.hidden = self.transfer(tf.add(tf.matmul(\n",
    "                            self.x + scale * tf.random_normal((n_input,)),\n",
    "                            self.weights['w1']), self.weights['b1']))\n",
    "        self.reconstruction = tf.add(tf.matmul(self.hidden,\n",
    "                                 self.weights['w2']), self.weights['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义自编码器的损失函数  \n",
    "\n",
    "- 这里使用平方误差作为 cost：用 `tf.sunbtract` 计算输出 `self.reconstruction` 与输入 `self.x` 之差\n",
    "- 再用 `tf.pow` 求差的平方\n",
    "- 最后使用 `tf.reduce_sum` 求和即可得到平方误差\n",
    "- 再定义训练操作为优化器 `self.optimizer` 对损失 `self.cost` 进行优化\n",
    "- 最后创建 Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                            self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化参数  \n",
    "\n",
    "- 创建 all_weights 的 dict，然后将 w1, b1, w2, b2 存入，最后返回 all_weights\n",
    "- **w1 需要使用 `xavier_init` 函数初始化，返回一个比较适合 softplus 等激活函数的权重初始分布**\n",
    "- b1, 置为 0，输出层 `self.reconstruction` 因为没有激活函数，w2, b2 全部初始化为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _initialize_weights(self):\n",
    "    all_weights = dict()\n",
    "    all_weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden))\n",
    "    all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))\n",
    "    all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))\n",
    "    all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))\n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义计算损失 cost 及执行一步训练的函数 partial_fit\n",
    "\n",
    "- Session 执行 cost 和 optimizer\n",
    "- 输入的 feed_dict 包括输入数据 x 以及噪声的系数 scale\n",
    "- partial_fit 用一个 batch 数据进行训练并返回当前的 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_fit(self, X):\n",
    "    cost, opt = self.sess.run((self.cost, self.optimizer),\n",
    "        feed_dict = {self.x:X, self.scale:self.training_scale})\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只求损失 cost 的函数\n",
    "\n",
    "- 测试集测评模型性能时会用到，不会像 partial_fit 那样触发训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_total_cost(self, X):\n",
    "    return self.sess.run(self.cost, feed_dict = {self.x:X, self.scale:self.training_scale})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 transform 函数\n",
    "\n",
    "- 返回自编码器隐藏层的输出结果\n",
    "- 目的是提供一个接口来获取抽象后的特征（学习出数据中的高阶特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "    return self.sess.run(self.hidden, feed_dict = {self.x:X, self.scale:self.training_scale})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 generate 函数\n",
    "\n",
    "- 整体运行一遍复原过程\n",
    "- 隐层的输出结果作为输入，将提取道德高阶特征复原为原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(self, hidden=None):\n",
    "    if hidden is None:\n",
    "        hidden = np.random.normal(size=self.weights[\"b1\"])\n",
    "    return self.sess.run(self.reconstruction, feed_dict = {self.hidden:hidden})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 reconstruct 函数\n",
    "\n",
    "- 整体运行复原过程\n",
    "- 包括提取高阶特征和通过高阶特征复原数据，即包括 transform 和 generate 两块\n",
    "- 输入的是原数据，输出的是复原后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(self, x):\n",
    "    return self.sess.run(self.reconstruction, feed_dict = {self.x:X, self.scale:self.training_scale})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getWeights & getBias\n",
    "\n",
    "- 获取隐层权重和偏置系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWeights(self):\n",
    "    return self.sess.run(self.weights['w1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBiases(self):\n",
    "    return self.sess.run(self.weights['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一个隐含层\n",
    "class AdditiveGaussianNoiseAutoencoder(object):\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus,\n",
    "                 optimizer=tf.train.AdamOptimizer(), scale=0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.transfer = transfer_function\n",
    "        self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        \n",
    "        # 定义网络结构\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        self.hidden = self.transfer(tf.add(tf.matmul(\n",
    "                            self.x + scale * tf.random_normal((n_input,)),\n",
    "                            self.weights['w1']), self.weights['b1']))\n",
    "        self.reconstruction = tf.add(tf.matmul(self.hidden,\n",
    "                                 self.weights['w2']), self.weights['b2'])\n",
    "        \n",
    "        # 定义损失函数\n",
    "        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                            self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        # 初始化参数\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        all_weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden))\n",
    "        all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))\n",
    "        all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], \n",
    "                                                 dtype = tf.float32))\n",
    "        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))\n",
    "        return all_weights\n",
    "\n",
    "    # 定义 partial_fit\n",
    "    def partial_fit(self, X):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer),\n",
    "            feed_dict = {self.x:X, self.scale:self.training_scale})\n",
    "        return cost\n",
    "\n",
    "    # 只求 cost 的函数\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, \n",
    "                             feed_dict = {self.x:X, self.scale:self.training_scale})\n",
    "\n",
    "    # transform\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.hidden, \n",
    "                             feed_dict = {self.x:X, self.scale:self.training_scale})\n",
    "\n",
    "    # generate\n",
    "    def generate(self, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = np.random.normal(size=self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction, feed_dict = {self.hidden:hidden})\n",
    "\n",
    "    # reconstruct\n",
    "    def reconstruct(self, x):\n",
    "        return self.sess.run(self.reconstruction, feed_dict = \n",
    "                             {self.x:X, self.scale:self.training_scale})\n",
    "\n",
    "    # getWeights and Biases\n",
    "    def getWeights(self):\n",
    "        return self.sess.run(self.weights['w1'])\n",
    "    def getBiases(self):\n",
    "        return self.sess.run(self.weights['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据标准化\n",
    "\n",
    "- 均值 0 标准差 1\n",
    "- sklearn.preprossing 的 StandardScaler 类\n",
    "- 先在训练集 fit，得到 Scaler 用到 训练数据和测试数据上\n",
    "- 保证 训练集和测试集 使用完全相同的 Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_scale(X_train, X_test):\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义随机 block 数据的函数\n",
    "\n",
    "- 定义一个从 0 到 len(data)-batch_size 之间的随机整数\n",
    "- 再以这个随机数作为 block 的起始位置\n",
    "- 然后顺序取到一个 batch size 的数据，不放回抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_block_from_data(data, batch_size):\n",
    "    start_index = np.random.randint(0, len(data)-batch_size)\n",
    "    return data[start_index:(start_index + batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义常用参数\n",
    "\n",
    "- 总训练样本数\n",
    "- 最大训练的轮数 epoch\n",
    "- batch_size\n",
    "- 每隔一轮显示一次 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = int(mnist.train.num_examples)\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建 AGN 自编码器实例\n",
    "\n",
    "- n_input 为 784\n",
    "- 隐含层节点树 200\n",
    "- 隐含层激活函数 transfer_function 为 softplus\n",
    "- 优化器 optimizer 为 Adam\n",
    "- 学习速率 0.001\n",
    "- 噪声系数 scale 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder = AdditiveGaussianNoiseAutoencoder(n_input=784,\n",
    "                                              n_hidden=200,\n",
    "                                              transfer_function=tf.nn.softplus,\n",
    "                                              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "                                              scale=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练\n",
    "\n",
    "- 每一轮循环开始时，平均损失 avg_cost 设为 0\n",
    "- 计算总共需要的 batch 数（样本总数/batch大小），**不放回抽样**\n",
    "- 每一个 batch 循环中，先使用 get_random_block_from_data 随机抽取一个 block 的数据\n",
    "- partial_fit 训练并计算当前 cost\n",
    "- 将当前 cost 整合到 avg_cost\n",
    "- 显示当前迭代轮数和平均 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 19376.387303409\n",
      "Epoch: 0002 cost= 11752.419839773\n",
      "Epoch: 0003 cost= 10117.372868750\n",
      "Epoch: 0004 cost= 8899.118648864\n",
      "Epoch: 0005 cost= 9598.412239773\n",
      "Epoch: 0006 cost= 8848.255514773\n",
      "Epoch: 0007 cost= 8726.235360227\n",
      "Epoch: 0008 cost= 9720.317524432\n",
      "Epoch: 0009 cost= 8452.229333523\n",
      "Epoch: 0010 cost= 9200.570518750\n",
      "Epoch: 0011 cost= 8952.941823295\n",
      "Epoch: 0012 cost= 8579.223243750\n",
      "Epoch: 0013 cost= 8671.104679545\n",
      "Epoch: 0014 cost= 8110.341107955\n",
      "Epoch: 0015 cost= 8043.276478977\n",
      "Epoch: 0016 cost= 8723.191114773\n",
      "Epoch: 0017 cost= 8387.743308523\n",
      "Epoch: 0018 cost= 8067.035511932\n",
      "Epoch: 0019 cost= 7913.338321591\n",
      "Epoch: 0020 cost= 8020.601459091\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = get_random_block_from_data(X_train, batch_size)\n",
    "        \n",
    "        cost = autoencoder.partial_fit(batch_xs)\n",
    "        avg_cost += cost / n_samples * batch_size\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: 621543.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自编码器类似于单隐层神经网络\n",
    "- 数据输入时标准化，并加上高斯噪声\n",
    "- 输出结果不是分类结果，而是复原的数据，因此不需要标注过的数据监督训练\n",
    "- 与其他无监督的区别是\n",
    "  - 不对数据进行聚类\n",
    "  - 提取其中最有用的，最频繁出现的高阶特征\n",
    "  - 根据高阶特征重构数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 实现多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "557px",
    "left": "0px",
    "right": "1021.82px",
    "top": "34px",
    "width": "185px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
